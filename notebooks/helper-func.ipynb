{"cells":[{"metadata":{},"cell_type":"markdown","source":"# All functions about building the data and building the knowledge base here are taken from Medlinker github\nhttps://github.com/danlou/MedLinker/blob/master/scripts/"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting scispacy\n  Downloading scispacy-0.4.0-py3-none-any.whl (44 kB)\n\u001b[K     |████████████████████████████████| 44 kB 119 kB/s eta 0:00:01\n\u001b[?25hCollecting conllu\n  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from scispacy) (1.19.5)\nCollecting pysbd\n  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n\u001b[K     |████████████████████████████████| 71 kB 831 kB/s eta 0:00:01\n\u001b[?25hCollecting nmslib>=1.7.3.6\n  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n\u001b[K     |████████████████████████████████| 13.5 MB 3.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from scispacy) (1.0.0)\nRequirement already satisfied: scikit-learn>=0.20.3 in /opt/conda/lib/python3.7/site-packages (from scispacy) (0.24.1)\nCollecting spacy<3.1.0,>=3.0.0\n  Downloading spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8 MB)\n\u001b[K     |████████████████████████████████| 12.8 MB 7.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scispacy) (2.25.1)\nCollecting pybind11<2.6.2\n  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n\u001b[K     |████████████████████████████████| 188 kB 11.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from nmslib>=1.7.3.6->scispacy) (5.8.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (1.26.2)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.3->scispacy) (1.5.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.3->scispacy) (2.1.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.8.2)\nCollecting catalogue<2.1.0,>=2.0.1\n  Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB)\nCollecting srsly<3.0.0,>=2.4.0\n  Downloading srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n\u001b[K     |████████████████████████████████| 456 kB 11.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.3.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (1.0.5)\nRequirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.3.2)\nRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.7.4.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.0.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (20.8)\nRequirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (1.7.3)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.7.4)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (4.55.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.0.5)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.4.0)\nCollecting thinc<8.1.0,>=8.0.2\n  Downloading thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 12.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.11.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.0.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (49.6.0.post20201009)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->scispacy) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->scispacy) (2.4.7)\nRequirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->scispacy) (3.0.0)\nRequirement already satisfied: click<7.2.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->scispacy) (7.1.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->scispacy) (1.1.1)\nInstalling collected packages: catalogue, srsly, thinc, pybind11, spacy, pysbd, nmslib, conllu, scispacy\n  Attempting uninstall: catalogue\n    Found existing installation: catalogue 1.0.0\n    Uninstalling catalogue-1.0.0:\n      Successfully uninstalled catalogue-1.0.0\n  Attempting uninstall: srsly\n    Found existing installation: srsly 1.0.5\n    Uninstalling srsly-1.0.5:\n      Successfully uninstalled srsly-1.0.5\n  Attempting uninstall: thinc\n    Found existing installation: thinc 7.4.5\n    Uninstalling thinc-7.4.5:\n      Successfully uninstalled thinc-7.4.5\n  Attempting uninstall: pybind11\n    Found existing installation: pybind11 2.6.2\n    Uninstalling pybind11-2.6.2:\n      Successfully uninstalled pybind11-2.6.2\n  Attempting uninstall: spacy\n    Found existing installation: spacy 2.3.5\n    Uninstalling spacy-2.3.5:\n      Successfully uninstalled spacy-2.3.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.0.5 which is incompatible.\nen-core-web-lg 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.0.5 which is incompatible.\nallennlp 2.0.1 requires spacy<2.4,>=2.1.0, but you have spacy 3.0.5 which is incompatible.\u001b[0m\nSuccessfully installed catalogue-2.0.1 conllu-4.4 nmslib-2.1.1 pybind11-2.6.2 pysbd-0.3.4 scispacy-0.4.0 spacy-3.0.5 srsly-2.4.0 thinc-8.0.2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThis script expects you've followed the instructions in https://github.com/chb/py-umls to install UMLS.\n\"\"\"\n\nimport json\nfrom collections import Counter\nimport sqlite3\n\nfrom scispacy.umls_semantic_type_tree import construct_umls_tree_from_tsv\n\numls_tree = construct_umls_tree_from_tsv('../input/preprocessing/umls_semantic_type_tree.tsv')  # change to your location\n\numls_db_path = '../input/umls-database/umls.db'  # change to your location\nconn = sqlite3.connect(umls_db_path)\nc = conn.cursor()\n\ncui_data = {}\nsource_counter = Counter()\ndef_mismatches = set()\n\n\nst21pv_sources = set(['CPT', 'FMA', 'GO', 'HGNC', 'HPO', 'ICD10',\n                      'ICD10CM', 'ICD9CM', 'MDR', 'MSH', 'MTH',\n                      'NCBI', 'NCI', 'NDDF', 'NDFRT', 'OMIM',\n                      'RXNORM', 'SNOMEDCT_US'])\n\n\nst21pv_types = set(['T005', 'T007', 'T017', 'T022', 'T031', 'T033', 'T037', \n                    'T038', 'T058', 'T062', 'T074', 'T082', 'T091', 'T092', \n                    'T097', 'T098', 'T103', 'T168', 'T170', 'T201', 'T204'])\n\n\nst21pv_types_children = {}\nfor st in st21pv_types:\n    st_node = umls_tree.get_node_from_id(st)\n    st_children = set([ch.type_id for ch in umls_tree.get_children(st_node)])\n    st21pv_types_children[st] = st_children\n\n\nRESTRICT_ST21PV = False\nNO_DEFS = False\n\nprint('Collecting info from \\'descriptions\\' table ...')\nfor row_idx, row in enumerate(c.execute('SELECT * FROM descriptions')):\n    \n    CUI, LAT, SAB, TTY, STR, STY = row\n\n    source_counter[SAB] += 1\n\n\n    STY = STY.split('|')\n    if LAT != 'ENG':\n        continue\n\n    if RESTRICT_ST21PV:\n        if SAB not in st21pv_sources:\n            continue\n\n        valid_row_sts = []\n        for row_st in STY:\n            if row_st in st21pv_types:\n                valid_row_sts.append(row_st)\n            \n            else:\n                for st in st21pv_types:\n                    if row_st in st21pv_types_children[st]:\n                        valid_row_sts.append(st)  # not row_st !\n                        break\n\n        if len(valid_row_sts) == 0:\n            continue\n        else:\n            STY = valid_row_sts\n\n        if len(st21pv_types.intersection(set(STY))) == 0:\n            continue\n\n    if CUI not in cui_data:\n        CUI_info = {}\n        CUI_info['SAB'] = SAB\n        # CUI_info['TTY'] = TTY\n        CUI_info['STY'] = STY\n\n        if NO_DEFS is False:\n            CUI_info['DEF'] = []\n        \n        CUI_info['STR'] = [STR]\n        CUI_info['Name'] = '' # custom\n\n        cui_data[CUI] = CUI_info\n    \n    else:\n        cui_data[CUI]['STR'].append(STR)\n\n    # source_counter[SAB] += 1\n\nprint('# CUIs:', len(cui_data))\n\nif NO_DEFS is False:\n    print('Collecting info from \\'MRDEF\\' table ...')\n    for row_idx, row in enumerate(c.execute('SELECT * FROM MRDEF')):    \n        CUI, AUI, ATUI, SATUI, SAB, DEF, SUPPRESS, CVF = row\n        \n        if CUI in cui_data:\n            cui_data[CUI]['DEF'].append(DEF)\n        else:\n            def_mismatches.add(CUI)\n\n\nprint('Preprocessing data ...')\nfor cui in cui_data.keys():\n    cui_data[cui]['Name'] = cui_data[cui]['STR'][0]\n    cui_data[cui]['STR'] = list(set(cui_data[cui]['STR'][1:]))\n\n\nprint('Storing data as JSON ...')\n\nfn = 'umls.2020AA.active'\nif RESTRICT_ST21PV:\n    fn += '.st21pv'\nelse:\n    fn += '.full'\n\nif NO_DEFS:\n    fn += '.no_defs'\n\nfn += '.json'\n\nwith open(fn, 'w') as json_f:\n    json.dump(cui_data, json_f)","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting info from 'descriptions' table ...\n# CUIs: 4412440\nCollecting info from 'MRDEF' table ...\nPreprocessing data ...\nStoring data as JSON ...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nutils for reading MedMentions original format\nadapted from scispacy: https://github.com/allenai/scispacy\n\"\"\"\n\nfrom typing import NamedTuple, List, Iterator, Dict, Tuple\nimport tarfile\nimport atexit\nimport os\nimport shutil\nimport tempfile\n\nfrom scispacy.file_cache import cached_path\n\nfrom scispacy.umls_semantic_type_tree import construct_umls_tree_from_tsv\numls_tree = construct_umls_tree_from_tsv(\"../input/preprocessing/umls_semantic_type_tree.tsv\")\n\n\nclass MedMentionEntity(NamedTuple):\n    start: int\n    end: int\n    mention_text: str\n    mention_type: str\n    umls_id: str\n\nclass MedMentionExample(NamedTuple):\n    title: str\n    abstract: str\n    text: str\n    pubmed_id: str\n    entities: List[MedMentionEntity]\n\n\ndef process_example(lines: List[str]) -> MedMentionExample:\n    \"\"\"\n    Processes the text lines of a file corresponding to a single MedMention abstract,\n    extracts the title, abstract, pubmed id and entities. The lines of the file should\n    have the following format:\n    PMID | t | Title text\n    PMID | a | Abstract text\n    PMID TAB StartIndex TAB EndIndex TAB MentionTextSegment TAB SemanticTypeID TAB EntityID\n    ...\n    \"\"\"\n    pubmed_id, _, title = [x.strip() for x in lines[0].split(\"|\", maxsplit=2)]\n    _, _, abstract = [x.strip() for x in lines[1].split(\"|\", maxsplit=2)]\n\n    entities = []\n    for entity_line in lines[2:]:\n        _, start, end, mention, mention_type, umls_id = entity_line.split(\"\\t\")\n        # mention_type = mention_type.split(\",\")[0]\n        mention_type = max(mention_type.split(\",\"), key=lambda x: umls_tree.get_node_from_id(x).level)\n        entities.append(MedMentionEntity(int(start), int(end),\n                                         mention, mention_type, umls_id))\n\n    # compose text from title and abstract\n    text = title + ' ' + abstract\n\n    return MedMentionExample(title, abstract, text, pubmed_id, entities)\n\ndef med_mentions_example_iterator(filename: str) -> Iterator[MedMentionExample]:\n    \"\"\"\n    Iterates over a MedMentions file, yielding examples.\n    \"\"\"\n    with open(filename, \"r\") as med_mentions_file:\n        lines = []\n        for line in med_mentions_file:\n            line = line.strip()\n            if line:\n                lines.append(line)\n            else:\n                yield process_example(lines)\n                lines = []\n        # Pick up stragglers\n        if lines:\n            yield process_example(lines)\n\n# def read_med_mentions(filename: str):\n#     \"\"\"\n#     Reads in the MedMentions dataset into Spacy's\n#     NER format.\n#     \"\"\"\n#     examples = []\n#     for example in med_mentions_example_iterator(filename):\n#         # spacy_format_entities = [(x.start, x.end, x.mention_type) for x in example.entities]\n#         spacy_format_entities = [(x.start, x.end, x.mention_text, x.mention_type, x.umls_id) for x in example.entities]\n#         examples.append((example.text, {\"entities\": spacy_format_entities}))\n\n#     return examples\n\n\ndef read_full_med_mentions(directory_path: str,\n                           label_mapping: Dict[str, str] = None,\n                           span_only: bool = False):\n\n    def _cleanup_dir(dir_path: str):\n        if os.path.exists(dir_path):\n            shutil.rmtree(dir_path)\n\n    resolved_directory_path = cached_path(directory_path)\n    if \"tar.gz\" in directory_path:\n        # Extract dataset to temp dir\n        tempdir = tempfile.mkdtemp()\n        print(f\"extracting dataset directory {resolved_directory_path} to temp dir {tempdir}\")\n        with tarfile.open(resolved_directory_path, 'r:gz') as archive:\n            archive.extractall(tempdir)\n        # Postpone cleanup until exit in case the unarchived\n        # contents are needed outside this function.\n        atexit.register(_cleanup_dir, tempdir)\n\n        resolved_directory_path = tempdir\n\n    expected_names = [\"corpus_pubtator.txt\",\n                      \"corpus_pubtator_pmids_all.txt\",\n                      \"corpus_pubtator_pmids_dev.txt\",\n                      \"corpus_pubtator_pmids_test.txt\",\n                      \"corpus_pubtator_pmids_trng.txt\"]\n\n    corpus = os.path.join(resolved_directory_path, expected_names[0])\n    examples = med_mentions_example_iterator(corpus)\n\n    train_ids = {x.strip() for x in open(os.path.join(resolved_directory_path, expected_names[4]))}\n    dev_ids = {x.strip() for x in open(os.path.join(resolved_directory_path, expected_names[2]))}\n    test_ids = {x.strip() for x in open(os.path.join(resolved_directory_path, expected_names[3]))}\n\n    train_examples = []\n    dev_examples = []\n    test_examples = []\n\n    for example in examples:\n        if example.pubmed_id in train_ids:\n            train_examples.append(example)\n\n        elif example.pubmed_id in dev_ids:\n            dev_examples.append(example)\n\n        elif example.pubmed_id in test_ids:\n            test_examples.append(example)\n\n    return train_examples, dev_examples, test_examples\n\n\n############################################\n\nimport itertools\nimport json\n\n\nclass MedMentionSentenceEntity(NamedTuple):\n    cui: str\n    st: str\n    tokens: List[str]\n    start: int\n    end: int\n\n\ndef iterate_annotations(sci_nlp, dataset_examples):\n\n    for ex in dataset_examples:\n\n        # get sentence positions to delimit annotations to sentences\n        sent_span_idxs = []\n        text = sci_nlp(ex.text)\n        sents = list(text.sents)\n\n        ch_idx = 0\n        # first sent will include title (due to composition expected by start/end ent indices)\n        # need to handle first sent differently\n        sent = sents.pop(0)\n        \n        # start by adding title as first sentence\n        sent_span_idxs.append((0, len(ex.title)))\n\n        # add remaining as another sentence (if any left)\n        if len(sent.text) > len(ex.title) + 1:\n            sent_span_idxs.append((len(ex.title) + 1, len(sent.text)))\n\n        ch_idx += len(sent.text) + 1\n\n        for sent in sents:\n            start_idx = ch_idx\n            end_idx = ch_idx + len(sent.text)\n            sent_span_idxs.append((start_idx, end_idx))\n\n            if text[end_idx] != ' ':\n                ch_idx = end_idx + 1  # ws separating sentences\n        # ch_idx -= 1  # fix last added ws\n\n        for ent in ex.entities:\n\n            # sanity check 1 - mentions match in text\n            text_mention_extraction = ex.text[ent.start:ent.end]\n            assert ent.mention_text == text_mention_extraction\n\n            for sent_start, sent_end in sent_span_idxs:\n                if (ent.start >= sent_start) and (ent.end <= sent_end):\n                    sent = ex.text[sent_start:sent_end]\n\n                    # adjust start and end positions\n                    ent = MedMentionEntity(ent.start - sent_start,\n                                           ent.end - sent_start,\n                                           ent.mention_text,\n                                           ent.mention_type,\n                                           ent.umls_id)\n\n                    # sanity check 2 - mentions match in sentence\n                    sent_mention_extraction = sent[ent.start:ent.end]\n                    assert ent.mention_text == sent_mention_extraction\n\n                    yield (ent, sent)\n\n\n# def locate_tokens(all_tokens, subset_tokens):\n#     \"\"\"\n#     Returns a list of indices (LoL) for all mention tokens within a list of tokens (i.e. sentence tokens).\n#     \"\"\"\n#     # tests all combinations, very slow and fails for long spans\n#     # gets the job done for now, to be improved later\n\n#     def get_idxs(elems, e):  # assumes must occurr\n#         return [i for i, e_ in enumerate(elems) if e == e_]\n\n#     def is_linear(elems):\n#         # return elems == [elems[0] + i for i in range(len(elems))]\n#         return all(e1 == e2 - 1 for e1, e2 in zip(elems, elems[1:]))\n\n#     # method isn't tractable for very long lists (also very rare)\n#     if len(subset_tokens) > 10:\n#         return [-1]\n\n#     all_possible_idxs = []  # indices for overlaps between all_tokens and subset\n#     for token in subset_tokens:\n#         if token in all_tokens:\n#             all_possible_idxs.append(get_idxs(all_tokens, token))\n    \n#     if len(all_possible_idxs) > 0:\n#         for combination in itertools.product(*all_possible_idxs):\n#             combination = list(combination)\n#             if is_linear(combination):  # only want indices increasing by +1\n#                 return combination\n    \n#     return [-1]\n\n\ndef locate_tokens(all_tokens, subset_tokens, reserved_spans=set()):\n\n    def get_idxs(elems, e):\n        return [i for i, e_ in enumerate(elems) if e == e_]\n\n    for t0_idx in get_idxs(all_tokens, subset_tokens[0]):\n        shift_idx = t0_idx + len(subset_tokens)\n        if all_tokens[t0_idx:shift_idx] == subset_tokens:\n            start = t0_idx\n            end = shift_idx - 1\n\n            if (start, end) not in reserved_spans:\n                return [start, end]\n    \n    return [-1]\n\n\ndef get_sent_boundaries(sci_nlp, text, title):\n    \"\"\"\n    Returns char indices for start and end of sentences from the full text.\n    The title is concatenated with the text, needs to processed as first sentence.\n    \"\"\"\n\n    # start with scispacy's sentence splitting\n    sents = [sent.text for sent in sci_nlp(text).sents]\n\n    sent_span_idxs = []\n\n    ch_idx = 0\n    # first sent will include title (due to composition expected by start/end ent indices)\n    # need to handle first sent differently\n    sent = sents.pop(0)\n    \n    # start by adding title as first sentence\n    sent_span_idxs.append((0, len(title) - 1))\n\n    # add remaining as another sentence (if any left)\n    if len(sent) > len(title) + 1:\n        sent_span_idxs.append((len(title) + 1, len(sent) - 1))\n\n    ch_idx += (len(sent) - 1) + 2  # skip over ws to next char, len gives +1\n\n    for sent in sents:\n        start_idx = ch_idx\n        end_idx = ch_idx + (len(sent) - 1)\n\n        # move to next char, skips ws\n        try:\n            if text[end_idx + 1] == ' ':\n                ch_idx = end_idx + 2\n            else:  # happens when sentence splitting fails\n                ch_idx = end_idx + 1\n        except IndexError:  # end of text\n            ch_idx = end_idx\n        \n        sent_span_idxs.append((start_idx, end_idx))\n\n    return sent_span_idxs\n\n\ndef get_sent_ents(sci_nlp, sent_tokens, sent_start, sent_end, doc_entities):\n\n    sent_ents = []\n    reserved_spans = set()\n    skipped_mentions = 0  # failed locating mention\n    for ent in doc_entities:\n        # only interested in entities located within sentence boundaries\n        if (ent.start >= sent_start) and (ent.end <= sent_end):\n\n            mention_tokens = [tok.text for tok in sci_nlp(ent.mention_text)]\n            mention_tokens_idxs = locate_tokens(sent_tokens, mention_tokens, reserved_spans)\n\n            if -1 in mention_tokens_idxs:\n                skipped_mentions += 1  # something may have gone wrong with splitting\n                continue\n            \n            mention_token_start = mention_tokens_idxs[0]\n            mention_token_end = mention_tokens_idxs[-1] + 1  # +1 for easier slicing... not sure about this choice\n\n            if (mention_token_start, mention_token_end) not in reserved_spans:  # no overlapping/duplicate spans\n\n                sent_ent = MedMentionSentenceEntity(cui=ent.umls_id,\n                                                    st=ent.mention_type,\n                                                    tokens=mention_tokens,\n                                                    start=mention_token_start,\n                                                    end=mention_token_end)\n\n                sent_ents.append(sent_ent)\n                reserved_spans.add((mention_token_start, mention_token_end))\n\n    return sent_ents, skipped_mentions\n\n\n# def iterate_docs_converted(split_path):\n\n#     # load json dataset\n#     with open(split_path, 'r') as json_f:\n#         dataset = json.load(json_f)\n\n#     for doc in dataset['docs']:\n#         yield doc","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport logging\nfrom time import time\n\nimport spacy\n\nsci_nlp = spacy.load('en_core_sci_md')\n\n\nlogging.basicConfig(level=logging.DEBUG,\n                    format='%(asctime)s - %(levelname)s - %(message)s',\n                    datefmt='%d-%b-%y %H:%M:%S')\n\n\nmm_splits = {'train':[], 'dev': [], 'test': []}\n# mm_splits['train'], mm_splits['dev'], mm_splits['test'] = read_full_med_mentions('data/MedMentions/full/data/')\nmm_splits['train'], mm_splits['dev'], mm_splits['test'] = read_full_med_mentions('../input/thesis/')\n\nlogging.info('Processing Instances ...')\n\nfor split_label in ['dev', 'test', 'train']:\n    split_data = {'split': split_label, 'timestamp': int(time()), 'n_unlocated_mentions': 0, 'n_located_mentions': 0, 'docs': []}\n    instances = mm_splits[split_label]\n    for doc_idx, ex in enumerate(instances):\n\n        if doc_idx % 100 == 0:\n            logging.info('[%s] Converted %d/%d instances.' % (split_label, doc_idx, len(instances)))\n\n        doc = {}\n        doc['idx'] = doc_idx\n        doc['title'] = ex.title\n        doc['abstract'] = ex.abstract\n        doc['text'] = ex.text\n        doc['pubmed_id'] = ex.pubmed_id\n        doc['sentences'] = []\n\n        # get sentence positions to delimit annotations to sentences\n        sent_span_idxs = get_sent_boundaries(sci_nlp, ex.text, ex.title)\n\n        for sent_start, sent_end in sent_span_idxs:\n            sent = {}\n\n            sent_text = ex.text[sent_start:sent_end + 1]\n            sent_tokens = [tok.text.strip() for tok in sci_nlp(sent_text)]\n            sent_tokens = [tok for tok in sent_tokens if tok != '']  # ensure no ws\n\n            sent['text'] = sent_text\n            sent['start'] = sent_start\n            sent['end'] = sent_end\n            sent['tokens'] = sent_tokens\n\n            # get gold ents\n            gold_ents, n_sent_skipped_mentions = get_sent_ents(sci_nlp, sent_tokens, sent_start, sent_end, ex.entities)\n\n            sent['n_unlocated_mentions'] = n_sent_skipped_mentions\n            split_data['n_unlocated_mentions'] += n_sent_skipped_mentions\n\n            sent['spans'] = []\n            for mm_entity in gold_ents:\n                ent = {}\n                ent['cui'] = mm_entity.cui\n                ent['st'] = mm_entity.st\n                ent['tokens'] = mm_entity.tokens\n                ent['start'] = mm_entity.start\n                ent['end'] = mm_entity.end\n                sent['spans'].append(ent)\n\n            split_data['n_located_mentions'] += len(sent['spans'])\n            doc['sentences'].append(sent)\n\n        split_data['docs'].append(doc)\n\n    logging.info('[%s] Writing converted MedMentions ...' % split_label)\n    with open('mm_converted.%s.json' % split_label, 'w') as json_f:\n        json.dump(split_data, json_f, sort_keys=True, indent=4)","execution_count":5,"outputs":[{"output_type":"stream","text":"Here\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport logging\nimport json\n\nlogging.basicConfig(level=logging.DEBUG,\n                    format='%(asctime)s - %(levelname)s - %(message)s',\n                    datefmt='%d-%b-%y %H:%M:%S')\n\n\ndef iterate_docs_converted(split_path):\n\n    # load json dataset\n    with open(split_path, 'r') as json_f:\n        dataset = json.load(json_f)\n\n    for doc in dataset['docs']:\n        yield doc\n\n\nif __name__ == '__main__':\n\n    \"\"\"Change those here\"\"\"\n    specify_st = True\n    split_label = 'dev'\n    \n    mm_path = './mm_converted.%s.json' % split_label\n\n    logging.info('Loading MedMentions - %s ...' % mm_path)\n    mm_docs = list(iterate_docs_converted(mm_path))\n\n    conll_lines = []\n\n    logging.info('Processing Instances ...')\n    for doc_idx, doc in enumerate(mm_docs):\n\n        conll_lines.append('-DOCSTART- (%s)' % doc['pubmed_id'])\n        conll_lines.append('')\n\n        for sent in doc['sentences']:\n\n            tokens = sent['tokens']\n            tags = ['O' for t in tokens]\n\n            for ent in sent['spans']:\n                \n                if specify_st:\n                    tag = ent['st']\n                else:\n                    tag = 'Entity'\n\n                if len(ent['tokens']) == 1:\n                    marker = 'B'\n                    tags[ent['start']] = '%s-%s' % (marker, tag)\n                \n                else:\n                    B_added = False\n                    for tag_idx in range(ent['start'], ent['end']):\n                        if not B_added:\n                            marker = 'B'\n                            B_added = True\n                        else:\n                            marker = 'I'\n\n                        tags[tag_idx] = '%s-%s' % (marker, tag)\n            \n            for token, tag in zip(tokens, tags):\n                conll_lines.append('%s\\tO\\tO\\t%s' % (token, tag))\n            conll_lines.append('')\n\n\n    if specify_st:\n        filepath = 'mm_ner_sts.%s.conll' % split_label\n    else:\n        filepath = 'mm_ner_ent.%s.conll' % split_label\n\n    logging.info('Writing CONLL - %s ...' % filepath)\n    with open(filepath, 'w') as f:\n        for line in conll_lines:\n            f.write('%s\\n' % line)\n    ","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text = \" \".join(train_sentences[0])\n#nlp = en_core_sci_sm.load()\n#nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n#doc = nlp(text)\n#entity = doc.ents[3]\n\n#print(\"Name: \", entity)\n\n#linker = nlp.get_pipe(\"scispacy_linker\")\n\n#for umls_ent in entity._.kb_ents:\n#    print(umls_ent)\n#    print(linker.kb.cui_to_entity['C0010674'])\n\n#db = linker.kb.cui_to_entity\n#db['C0857937']\n\n\n\n\n#import nltk\n#for a in b:\n#    tokens = nltk.word_tokenize(a)\n#    tokens = [token.lower() for token in tokens if len(token) > 1]\n#    if len(tokens) == 1:\n#        print(tokens, \" That's unigram\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}