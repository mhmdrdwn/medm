{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Searching nearest neighbour in UMLS knowledge base\n\n\nThe goal of this notebook is to search for nearest neighbour of each predicted mentions (from mention detection) in the UMLS knowledge base. And given that UMLS concepts are mapped to STY labels, we take the label of the concept/alias as the label of the predicted mention.","metadata":{"id":"HvnDSXa_Zcnu"}},{"cell_type":"code","source":"%%capture\n\nimport json\nimport numpy as np\nfrom numpy import save\nimport pandas as pd\n\n!pip install sentence_transformers\n!pip install faiss\n!apt-get install libomp-dev --yes\n!pip install seqeval\n\nimport faiss\nfrom sentence_transformers import SentenceTransformer\n\nfrom transformers import BertModel, BertTokenizer, BertConfig\n\nfrom tqdm.auto import tqdm\n\nimport tensorflow as tf\n\nfrom seqeval.metrics import classification_report, f1_score, accuracy_score\nfrom seqeval.scheme import IOB2\n\nimport gc\nimport csv\nfrom IPython.display import FileLink","metadata":{"id":"uz6UaMNvZco-","execution":{"iopub.status.busy":"2021-05-30T13:46:36.599149Z","iopub.execute_input":"2021-05-30T13:46:36.599524Z","iopub.status.idle":"2021-05-30T13:47:15.211217Z","shell.execute_reply.started":"2021-05-30T13:46:36.599446Z","shell.execute_reply":"2021-05-30T13:47:15.210323Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def build_concepts_arr(kb):\n    umls_concepts = []\n    umls_cui = []\n    umls_sty = []\n    \n    for k, v in kb.items():\n        umls_concepts.append(kb[k]['Name'].lower())\n        umls_cui.append(k)\n        umls_sty.append(kb[k]['STY'][0])\n        for STR in kb[k]['STR']:\n            umls_concepts.append(STR.lower())\n            umls_cui.append(k)\n            umls_sty.append(kb[k]['STY'][0])\n            \n    return umls_concepts, umls_cui, umls_sty","metadata":{"id":"6WDSzi3ZZcp2","execution":{"iopub.status.busy":"2021-05-30T13:47:15.212737Z","iopub.execute_input":"2021-05-30T13:47:15.213132Z","iopub.status.idle":"2021-05-30T13:47:15.220457Z","shell.execute_reply.started":"2021-05-30T13:47:15.213091Z","shell.execute_reply":"2021-05-30T13:47:15.219207Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We mapped all the concepts that was just stored in the dict database to a cui code for each concept, so it becomes fast to extract cui codes later","metadata":{"id":"MsFoSF00ZcqS"}},{"cell_type":"code","source":"def create_maps():\n    alias2cui = {}\n    alias2sty = {}\n    cui2sty = {}\n    cui2alias = {}\n    sty2interpretation = {}\n    \n    with open('../input/umls-kb/umls.2017AA.active.st21pv.json') as f:\n        kb = json.load(f)\n        \n    umls_concepts, umls_cui, umls_sty = build_concepts_arr(kb)\n\n    for concept, cui, sty in zip(umls_concepts, umls_cui, umls_sty):\n        alias2cui[concept] = cui\n        alias2sty[concept] = sty\n        cui2sty[cui] = sty\n\n    for alias, cui in alias2cui.items():\n        if cui in cui2alias.keys():\n            if alias not in cui2alias[cui]:\n                cui2alias[cui].append(alias)\n        else:\n            cui2alias[cui] = [alias]\n            \n    with open('../input/thesis/SemanticTypes_2018AB.txt', 'r') as file:\n        for line in file.readlines():\n            line = line.split(\"|\")\n            interpretation = (line[2][:-1]).lower().replace(\" \", \"_\")\n            sty2interpretation[line[1]] = interpretation\n\n    interpretation2sty = {v: k for k, v in sty2interpretation.items()}\n\n    del kb\n    del umls_concepts\n    del umls_cui\n    del umls_sty\n    gc.collect()\n    \n    return alias2cui, alias2sty, cui2sty, cui2alias, sty2interpretation, interpretation2sty\n\nalias2cui, alias2sty, cui2sty, cui2alias, sty2interpretation, interpretation2sty = create_maps()","metadata":{"id":"wW6QznIOZcqZ","execution":{"iopub.status.busy":"2021-05-30T13:47:15.222657Z","iopub.execute_input":"2021-05-30T13:47:15.223172Z","iopub.status.idle":"2021-05-30T13:47:40.194269Z","shell.execute_reply.started":"2021-05-30T13:47:15.223133Z","shell.execute_reply":"2021-05-30T13:47:40.193436Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(\"Semantic Type of the CUI C0086930 are: \", cui2sty['C0086930'], \" or \", sty2interpretation[cui2sty['C0086930']])\nprint(\"Aliases of the CUI C0086930 are: \", cui2alias['C0086930'])","metadata":{"id":"1tqNF-aBZcqk","outputId":"d478db0b-6b86-497d-c6b4-941fd75d58fe","execution":{"iopub.status.busy":"2021-05-30T13:47:40.195794Z","iopub.execute_input":"2021-05-30T13:47:40.196134Z","iopub.status.idle":"2021-05-30T13:47:40.201426Z","shell.execute_reply.started":"2021-05-30T13:47:40.196096Z","shell.execute_reply":"2021-05-30T13:47:40.200615Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Semantic Type of the CUI C0086930 are:  T058  or  health_care_activity\nAliases of the CUI C0086930 are:  ['risk assessment', 'assessments, risk', 'risk assessments', 'assessment, risk']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Predicted Mentions","metadata":{"id":"Z1jCI99sZcra"}},{"cell_type":"markdown","source":"This csv file is the predictions of mentions from the mentions detections","metadata":{"id":"jhy2clLoZcrd"}},{"cell_type":"code","source":"test_predictions = pd.read_csv('../input/predictions/test_prediction.csv')\n\ndef build_sentences():\n    single_sent = []\n    sentences = []\n    rep_sentences = []\n    \n    for row in test_predictions.iterrows():\n        if row[1]['Tokens'] == '[SEP]':\n            single_sent.append(row[1]['Tokens'])\n            sentences.append(single_sent[1:-1])\n            single_sent = []\n        else:\n            single_sent.append(row[1]['Tokens'])\n\n    for sent in sentences:\n        for i in range(len(sent)+2):\n            rep_sentences.append(sent)\n    \n    return rep_sentences\n\ndf_sentences = build_sentences()\n\ntest_predictions['Sentences'] = df_sentences","metadata":{"id":"DqT8XBiIZcrg","execution":{"iopub.status.busy":"2021-05-30T13:47:40.202621Z","iopub.execute_input":"2021-05-30T13:47:40.203131Z","iopub.status.idle":"2021-05-30T13:48:01.295110Z","shell.execute_reply.started":"2021-05-30T13:47:40.203095Z","shell.execute_reply":"2021-05-30T13:48:01.294222Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_predictions = pd.read_csv('../input/predictions/train_prediction.csv')\n\ndef build_sentences():\n    single_sent = []\n    sentences = []\n    rep_sentences = []\n    \n    for row in train_predictions.iterrows():\n        if row[1]['Tokens'] == '[SEP]':\n            single_sent.append(row[1]['Tokens'])\n            sentences.append(single_sent[1:-1])\n            single_sent = []\n        else:\n            single_sent.append(row[1]['Tokens'])\n\n    for sent in sentences:\n        for i in range(len(sent)+2):\n            rep_sentences.append(sent)\n    \n    return rep_sentences\n\ndf_sentences = build_sentences()\n\ntrain_predictions['Sentences'] = df_sentences","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:48:01.296429Z","iopub.execute_input":"2021-05-30T13:48:01.296812Z","iopub.status.idle":"2021-05-30T13:48:08.352844Z","shell.execute_reply.started":"2021-05-30T13:48:01.296758Z","shell.execute_reply":"2021-05-30T13:48:08.351982Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_mentions_candidates = np.unique(test_predictions[test_predictions['Detection Prediction'] != \"O\"]['Span'])\ntrain_mentions_candidates = np.unique(train_predictions[train_predictions['Detection Prediction'] != \"O\"]['Span'])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:48:08.354047Z","iopub.execute_input":"2021-05-30T13:48:08.354367Z","iopub.status.idle":"2021-05-30T13:48:08.529434Z","shell.execute_reply.started":"2021-05-30T13:48:08.354335Z","shell.execute_reply":"2021-05-30T13:48:08.528571Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Extract Embedding of candidates and mentions using the UMLSBert model\n\nThe next function is used to weight combination the context embeddings and mentions embeddigs","metadata":{"id":"-RSdbUEFZcs8"}},{"cell_type":"code","source":"def combine_mention_context_test(query_emb_dict, sent_emb_dict):\n    idx = -1\n    combined_emb = {}\n    \n    for mention, sent, detect_label in zip(test_predictions['Span'], \n                                           test_predictions['Sentences'], \n                                           test_predictions['Detection Prediction']):\n        idx += 1\n        \n        if type(mention) != str:\n            mention = str(mention)\n        \n        s = []\n        \n        for tok in sent:\n            if type(tok) != str:\n                s.append(str(tok))\n            else:\n                s.append(tok)\n        s = \" \".join(s)\n        \n        if detect_label != 'O':\n            if query_emb_dict.get(mention) is not None:\n                combined = query_emb_dict[mention] * 0.7 + sent_emb_dict[s] * 0.3\n                combined_norm = np.linalg.norm(combined)\n                combined = combined/combined_norm\n                combined_emb[idx] = combined\n    \n    return combined_emb","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:48:08.532175Z","iopub.execute_input":"2021-05-30T13:48:08.532540Z","iopub.status.idle":"2021-05-30T13:48:08.540781Z","shell.execute_reply.started":"2021-05-30T13:48:08.532502Z","shell.execute_reply":"2021-05-30T13:48:08.539653Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def combine_mention_context_train(query_emb_dict, sent_emb_dict):\n    idx = -1\n    combined_emb = {}\n    \n    for mention, sent, detect_label in zip(train_predictions['Span'], \n                                           train_predictions['Sentences'], \n                                           train_predictions['Detection Prediction']):\n        idx += 1\n        \n        if type(mention) != str:\n            mention = str(mention)\n        \n        s = []\n        \n        for tok in sent:\n            if type(tok) != str:\n                s.append(str(tok))\n            else:\n                s.append(tok)\n        s = \" \".join(s)\n        \n        if detect_label != 'O':\n            if query_emb_dict.get(mention) is not None:\n                combined = query_emb_dict[mention] * 0.7 + sent_emb_dict[s] * 0.3\n                combined_norm = np.linalg.norm(combined)\n                combined = combined/combined_norm\n                combined_emb[idx] = combined\n    \n    return combined_emb","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:48:08.542527Z","iopub.execute_input":"2021-05-30T13:48:08.542959Z","iopub.status.idle":"2021-05-30T13:48:08.556704Z","shell.execute_reply.started":"2021-05-30T13:48:08.542919Z","shell.execute_reply":"2021-05-30T13:48:08.555843Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We use this function to save the embeddings of the mentions, context and UMLS aliases in a binary files, The embeddings are normalized in order to calculate the cosine similarity later ","metadata":{}},{"cell_type":"code","source":"def save_emb(calculate_kb=False):\n    query_emb = {}\n    sent_emb = {}\n    unique_queries = {}\n    unique_mentions = {}\n    kb_concepts_emb = []\n    sent_emb_dict = {}\n    query_emb_dict = {}\n    kb_concepts_count = {}\n    \n    coder_embedder = SentenceTransformer(\"GanjinZero/UMLSBert_ENG\")\n\n    for q in test_mentions_candidates:\n        q = str(q)\n        unique_queries[q] = None\n    \n    unique_queries = list(unique_queries.keys())\n\n    query_emb = coder_embedder.encode(unique_queries, batch_size=500)\n    \n    for q, emb in zip(unique_queries, query_emb):\n        query_emb_dict[q] = emb\n    \n    del query_emb\n    del sent_emb\n    gc.collect()\n    \n    sentences = test_predictions['Sentences']\n    \n    merged_sents = []\n    for sent in np.unique(sentences):\n        s = []\n        for tok in sent:\n            if type(tok) != str:\n                s.append(str(tok))\n            else:\n                s.append(tok)\n        s = \" \".join(s)\n        merged_sents.append(s)\n    \n    sent_emb = coder_embedder.encode(merged_sents, batch_size=500)\n    \n    for sent, emb in zip(merged_sents, sent_emb):\n        sent_emb_dict[sent] = emb\n    \n    combined_mention_context = combine_mention_context_test(query_emb_dict, sent_emb_dict)\n\n    np.save('test_queries.npy', list(combined_mention_context.values()))\n\n    del query_emb_dict\n    del sent_emb_dict\n    gc.collect()\n    \n    if calculate_kb:\n        kb_unique_concepts = list(alias2cui.keys())\n\n        chunks = (len(kb_unique_concepts) - 1) // 100000 + 1\n    \n        for i in tqdm(range(chunks)):\n            batch = kb_unique_concepts[i*100000:(i+1)*100000]\n            kb_concepts_emb = coder_embedder.encode(batch)\n            kb_concepts_emb /= np.linalg.norm(kb_concepts_emb, axis=1, keepdims=True)\n            np.save('./kb'+str(i)+'.npy', kb_concepts_emb)    \n            del kb_concepts_emb\n            gc.collect()\n\n    del sentences\n    del merged_sents\n    gc.collect()\n\n    np.save('test_query_indices.npy', list(combined_mention_context.keys())) ","metadata":{"id":"kUGdsSjhOaIA","outputId":"16ba496e-e1e1-4f55-f49b-4b03b4241f65","execution":{"iopub.status.busy":"2021-05-30T13:48:08.558069Z","iopub.execute_input":"2021-05-30T13:48:08.558512Z","iopub.status.idle":"2021-05-30T13:48:08.573335Z","shell.execute_reply.started":"2021-05-30T13:48:08.558475Z","shell.execute_reply":"2021-05-30T13:48:08.572532Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#save_emb()\n#FileLink(r'./test_queries.npy')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:48:08.574522Z","iopub.execute_input":"2021-05-30T13:48:08.574982Z","iopub.status.idle":"2021-05-30T13:48:08.587743Z","shell.execute_reply.started":"2021-05-30T13:48:08.574943Z","shell.execute_reply":"2021-05-30T13:48:08.586872Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"queries_emb = np.load('../input/d/mhmdrdwn/saved-embeddings/test_queries.npy', mmap_mode='r')\nqueries_indices = np.load('../input/d/mhmdrdwn/saved-embeddings/test_query_indices.npy', mmap_mode='r') \nkb_concepts_emb = np.load('../input/d/mhmdrdwn/saved-embeddings/kb'+str(0)+'.npy', mmap_mode='r')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:49:48.426186Z","iopub.execute_input":"2021-05-30T13:49:48.426543Z","iopub.status.idle":"2021-05-30T13:49:48.510889Z","shell.execute_reply.started":"2021-05-30T13:49:48.426504Z","shell.execute_reply":"2021-05-30T13:49:48.510069Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"kb_concepts_emb.shape, queries_emb.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:49:50.795423Z","iopub.execute_input":"2021-05-30T13:49:50.795762Z","iopub.status.idle":"2021-05-30T13:49:50.806306Z","shell.execute_reply.started":"2021-05-30T13:49:50.795731Z","shell.execute_reply":"2021-05-30T13:49:50.805440Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"((100000, 768), (63567, 768))"},"metadata":{}}]},{"cell_type":"code","source":"dimension = kb_concepts_emb.shape[1]\nk = 1\nnlist = 21 #number of clusters\nindex = faiss.IndexFlatIP(dimension)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:49:54.905809Z","iopub.execute_input":"2021-05-30T13:49:54.906135Z","iopub.status.idle":"2021-05-30T13:49:54.910148Z","shell.execute_reply.started":"2021-05-30T13:49:54.906106Z","shell.execute_reply":"2021-05-30T13:49:54.909160Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"We build a search index using Faiss, we train the index on all the embeddigs of the UMLS aliases","metadata":{}},{"cell_type":"code","source":"# add all UMLS embeddings to the index\nfor i in tqdm(range(26)):\n    kb_concepts_emb = np.load('../input/d/mhmdrdwn/saved-embeddings/kb'+str(i)+'.npy', mmap_mode='r')\n    index.train(kb_concepts_emb)\n    index.add(kb_concepts_emb)\n    kb_concepts_emb = None","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:50:09.462113Z","iopub.execute_input":"2021-05-30T13:50:09.462438Z","iopub.status.idle":"2021-05-30T13:51:46.359660Z","shell.execute_reply.started":"2021-05-30T13:50:09.462408Z","shell.execute_reply":"2021-05-30T13:51:46.358786Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4445611383134f999ac34c2a663a6fa3"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now we search all the detected embeddings in the UMLS, We set nprobe as 21 which equals the number of clusters meaning we are doing exhaustive search","metadata":{}},{"cell_type":"code","source":"def search():\n    index.nprobe = 21  \n    chunks = (len(queries_emb) - 1) // 1000 + 1\n    nearest_neighbors = []\n    distances = []\n    kb_unique_concepts = list(alias2cui.keys())\n    for i in tqdm(range(chunks)):\n        batch = queries_emb[i*1000:(i+1)*1000]\n        distance, nearest_neighbor = index.search(batch, k)\n        distances.extend(distance)\n        nearest_neighbors.extend(nearest_neighbor)\n        \n    idx2nn = {}\n    idx2dis = {}\n    for nn, dis, q_idx in zip(nearest_neighbors, distances ,queries_indices):\n        idx2nn[q_idx] = sty2interpretation[alias2sty[kb_unique_concepts[nn[0]]]]\n        idx2dis[q_idx] = dis[0]\n        \n    return idx2nn, idx2dis\n\nidx2nn, idx2dis = search()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:52:12.370633Z","iopub.execute_input":"2021-05-30T13:52:12.371004Z","iopub.status.idle":"2021-05-30T14:45:06.007884Z","shell.execute_reply.started":"2021-05-30T13:52:12.370968Z","shell.execute_reply":"2021-05-30T14:45:06.007027Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/64 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e4557ecb5f4306834a688b305e8ea3"}},"metadata":{}}]},{"cell_type":"code","source":"def build_prediction_list():\n    extracted_sty = []\n    sty_sim = []\n    idx = -1\n    \n    for mention, pred, sent in zip(tqdm(test_predictions['Span']), \n                                   test_predictions['Detection Prediction'], \n                                   test_predictions['Sentences']):\n        \n        idx += 1\n        if pred != 'O':\n            if idx2nn.get(idx) is not None:\n                extracted_sty.append(pred[:2]+idx2nn.get(idx))\n                sty_sim.append(idx2dis.get(idx))\n            else:\n                extracted_sty.append('O')\n                sty_sim.append(None)\n        else:\n            extracted_sty.append('O')\n            sty_sim.append(None)\n        \n    return extracted_sty, sty_sim","metadata":{"id":"T3phCxDbZctk","execution":{"iopub.status.busy":"2021-05-30T14:45:16.538895Z","iopub.execute_input":"2021-05-30T14:45:16.539211Z","iopub.status.idle":"2021-05-30T14:45:16.545097Z","shell.execute_reply.started":"2021-05-30T14:45:16.539182Z","shell.execute_reply":"2021-05-30T14:45:16.544262Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"extracted_sty, sty_sim = build_prediction_list()\n#train_predictions['Predicted Label'] = extracted_sty\n#train_predictions['Cosine Similarity'] = sty_sim\ntest_predictions['Predicted Label'] = extracted_sty\ntest_predictions['Cosine Similarity'] = sty_sim","metadata":{"id":"P__8U1TmZctp","outputId":"fe853952-8915-4b71-be25-23b79f5502ca","execution":{"iopub.status.busy":"2021-05-30T14:45:20.727575Z","iopub.execute_input":"2021-05-30T14:45:20.727918Z","iopub.status.idle":"2021-05-30T14:45:21.254232Z","shell.execute_reply.started":"2021-05-30T14:45:20.727885Z","shell.execute_reply":"2021-05-30T14:45:21.253466Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/270657 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a552a38a8b7a4ac2ae1101b7f7654e4d"}},"metadata":{}}]},{"cell_type":"code","source":"test_predictions.to_csv('test_nn_prediction.csv')\n#train_predictions.to_csv('train_nn_prediction.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T14:45:25.147496Z","iopub.execute_input":"2021-05-30T14:45:25.147838Z","iopub.status.idle":"2021-05-30T14:45:30.424650Z","shell.execute_reply.started":"2021-05-30T14:45:25.147804Z","shell.execute_reply":"2021-05-30T14:45:30.423840Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"FileLink(r'./test_nn_prediction.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T14:45:32.978029Z","iopub.execute_input":"2021-05-30T14:45:32.978368Z","iopub.status.idle":"2021-05-30T14:45:32.983718Z","shell.execute_reply.started":"2021-05-30T14:45:32.978339Z","shell.execute_reply":"2021-05-30T14:45:32.982650Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/test_nn_prediction.csv","text/html":"<a href='./test_nn_prediction.csv' target='_blank'>./test_nn_prediction.csv</a><br>"},"metadata":{}}]}]}