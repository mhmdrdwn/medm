{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import islice\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nfrom keras.preprocessing import text, sequence\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import backend as K\n\n\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, Dense, LSTM, TimeDistributed, Lambda,Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\nfrom tensorflow.keras.models import Model\nfrom keras.optimizers import SGD, Adam, RMSprop\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"semgroups = \"/kaggle/input/thesis/SemGroups_2018.txt\"\nsemtypes = \"/kaggle/input/thesis/SemanticTypes_2018AB.txt\"\nmedmentions = \"/kaggle/input/thesis/corpus_pubtator.txt\"\numls_concept = \"/kaggle/input/thesis/MRCONSO.RRF\"\ntrain_docs_ids = \"../input/doc-ids/corpus_pubtator_pmids_trng.txt\"\nval_docs_ids = \"../input/doc-ids/corpus_pubtator_pmids_dev.txt\"\ntest_docs_ids = \"../input/doc-ids/corpus_pubtator_pmids_test.txt\"\numls_embedding_file = \"../input/umls-embeddings/embeddings.csv\"","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_in_chunks(file_object, n=10000):\n    \"\"\"Lazy function (generator) to read a file piece by piece.\n    Default chunk size: 1k.\"\"\"\n    while True:\n        data = list(islice(file_object, n))\n        if not data:\n            break\n        yield data\n\n        \ndef umls_concepts():\n    umls_concepts = {}\n    with open(umls_concept) as f:\n        for piece in read_in_chunks(f):\n            for line in piece:\n                if line != \"\":\n                    line_list = line.split(\"|\")\n                    if line_list[1] == 'ENG':\n                        umls_concepts[line_list[0]] = line_list[14]\n    return umls_concepts\n\n\ndef umls_semtype():\n    umls_semtype = {}\n    with open(semgroups) as f:\n        f = f.read().split(\"\\n\")\n        for line in f:\n            if line != \"\":\n                line_list = line.split(\"|\")\n                umls_semtype[line_list[2]] = [line_list[1], line_list[3]]\n    return umls_semtype","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"umls_concepts = umls_concepts()\numls_semtype = umls_semtype()\n\numls_semtype['T131']","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"['Chemicals & Drugs', 'Hazardous or Poisonous Substance']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(umls_concepts), len(umls_semtype)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(4412440, 127)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Build Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/ArshSekhon/pubtator_loader.git\n\nfrom pubtator_loader import PubTatorCorpusReader\ndataset_reader = PubTatorCorpusReader(medmentions)\n\ncorpus = dataset_reader.load_corpus() \n# corpus will be a List[PubtatorDocuments]","execution_count":6,"outputs":[{"output_type":"stream","text":"Collecting git+https://github.com/ArshSekhon/pubtator_loader.git\n  Cloning https://github.com/ArshSekhon/pubtator_loader.git to /tmp/pip-req-build-c5m2otci\n  Running command git clone -q https://github.com/ArshSekhon/pubtator_loader.git /tmp/pip-req-build-c5m2otci\nRequirement already satisfied: spacy==3.0.5 in /opt/conda/lib/python3.7/site-packages (from pubtator-loader==0.1.4) (3.0.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (2.0.5)\nRequirement already satisfied: thinc<8.1.0,>=8.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (8.0.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (2.11.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (20.8)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (2.25.1)\nRequirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (1.7.3)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (1.0.5)\nRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (3.7.4.3)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (0.7.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (49.6.0.post20201009)\nRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (2.4.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (0.8.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (3.0.1)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (0.4.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (1.19.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (4.55.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (3.0.5)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (2.0.1)\nRequirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (0.3.2)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from spacy==3.0.5->pubtator-loader==0.1.4) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->spacy==3.0.5->pubtator-loader==0.1.4) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy==3.0.5->pubtator-loader==0.1.4) (2.4.7)\nRequirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy==3.0.5->pubtator-loader==0.1.4) (3.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5->pubtator-loader==0.1.4) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5->pubtator-loader==0.1.4) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5->pubtator-loader==0.1.4) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5->pubtator-loader==0.1.4) (2.10)\nRequirement already satisfied: click<7.2.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.5->pubtator-loader==0.1.4) (7.1.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy==3.0.5->pubtator-loader==0.1.4) (1.1.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_data():\n    sequneces = []\n    labels = []\n    articles_ids = []\n    for doc in corpus:\n        row = []\n        full_text = doc.get_space_separated_title_and_abstract()\n        full_text_replaced = full_text\n        for ent in doc.entities:\n            start = ent.start_index\n            end = ent.end_index\n            umls_concept_code = ent.entity_id\n            text_segment = ent.text_segment\n            full_text_replaced = full_text_replaced.replace(text_segment, \" \"+umls_concept_code+\" \", 1)\n        full_text_list = full_text_replaced.split(\" \")\n        \n        entities_list = [ent.entity_id for ent in doc.entities]\n        articles_ids.append(doc.id)\n        \n        for word in full_text_list:\n            if word != '':\n                idx = -1\n                indices = [i for i, x in enumerate(entities_list) if x == word]\n                if word in entities_list:\n                    idx = entities_list.index(word)   \n                if idx > -1:\n                    start = doc.entities[idx].start_index\n                    end = doc.entities[idx].end_index\n                    if \",\" in doc.entities[idx].semantic_type_id:\n                        sem_ent = doc.entities[idx].semantic_type_id.split(\",\")[0]\n                    else:\n                        sem_ent = doc.entities[idx].semantic_type_id\n                    row.append([word, sem_ent])\n                else:\n                    row.append([word, 'o'])\n                    \n        sequneces.append(list(np.array(row)[:,0]))\n        labels.append(list(np.array(row)[:,1]))\n    return sequneces, labels, articles_ids\n    \nsequences, labels, articles_ids = build_data()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert sequences[0][0] == \"C4308010\"\nassert labels[0][0] == \"T116\"\nassert sequences[0][-5] == \"C0854135\"\nassert labels[0][-5] == \"T047\"\n\nassert sequences[5][1] == \"C0870811\"\nassert labels[5][1] == \"T054\"\nassert sequences[5][-2] == \"C0243095\"\nassert labels[5][-2] == \"T033\"","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokens_tags(data_X, data_y):\n    vocab = []\n    tags = []\n    for seq in data_X:\n        for word in np.unique(seq):\n            if word.lower() not in vocab:\n                vocab.append(word.lower())\n    for seq_ent in data_y:\n        for ent in seq_ent:\n            if ent not in tags:\n                tags.append(ent)\n                \n    return len(vocab), len(tags), max([len(s) for s in sequences])\n\nnum_tokens, num_tags, maxlen = tokens_tags(sequences, labels)\nprint('Number of unique tokens ', num_tokens)\nprint('Max length of sequence ', maxlen)\nprint('Number of unique tags ', num_tags)","execution_count":9,"outputs":[{"output_type":"stream","text":"Number of unique tokens  66317\nMax length of sequence  685\nNumber of unique tags  127\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minority_classes(entities):\n    class_count = {}\n    for seq_tags in entities:\n        for tag in seq_tags:\n            if tag not in class_count.keys():\n                class_count[tag] = 1\n            else:\n                class_count[tag] += 1\n    \n    threeshold = 10\n\n    less_represented_classes = {}\n    \n    for k, v in class_count.items():\n        if v < threeshold:\n            less_represented_classes[k] = v\n\n    return less_represented_classes\n\nminority_classes(labels)            ","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"{'T021': 2, 'T127': 2}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Val split"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_val_split():\n    train_sequences = []\n    test_sequences = []\n    val_sequences = []\n    train_labels = []\n    test_labels = []\n    val_labels = []\n    \n    with open(train_docs_ids) as train_ids:\n        train_id_list = []\n        train_ids = train_ids.read().split(\"\\n\")\n        for line in train_ids:\n            train_id_list.append(line)\n    \n    with open(val_docs_ids) as val_ids:\n        val_id_list = []\n        val_ids = val_ids.read().split(\"\\n\")\n        for line in val_ids:\n            val_id_list.append(line)\n\n    with open(test_docs_ids) as test_ids:\n        test_id_list = []\n        test_ids = test_ids.read().split(\"\\n\")\n        for line in test_ids:\n            test_id_list.append(line)\n\n\n    for x, y, idx in zip(sequences, labels, articles_ids):\n        idx = str(idx)\n        if idx in train_id_list:\n            train_sequences.append(x)\n            train_labels.append(y)\n        elif idx in val_id_list:\n            val_sequences.append(x)\n            val_labels.append(y)\n        elif idx in test_id_list:\n            test_sequences.append(x)\n            test_labels.append(y)\n            \n    return train_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels\n\n\ntrain_sequences, train_labels, val_sequences, val_labels, test_sequences, test_labels = train_test_val_split()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_sequences), len(train_labels), len(val_sequences), len(val_labels), len(test_sequences), len(test_labels)","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"(2635, 2635, 878, 878, 879, 879)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_tokens_train, num_tags_train = tokens_tags(train_sequences, train_labels)[:2]\nprint('Number of unique tokens in train data ', num_tokens_train)\nprint('Number of unique tags in train data ', num_tags_train)\nnum_tokens_val, num_tags_val = tokens_tags(val_sequences, val_labels)[:2]\nprint('Number of unique tokens in val data ', num_tokens_val)\nprint('Number of unique tags in val data ', num_tags_val)\nnum_tokens_test, num_tags_test = tokens_tags(test_sequences, test_labels)[:2]\nprint('Number of unique tokens in test data ', num_tokens_test)\nprint('Number of unique tags in test data ', num_tags_test)","execution_count":46,"outputs":[{"output_type":"stream","text":"Number of unique tokens in train data  48187\nNumber of unique tags in train data  127\nNumber of unique tokens in val data  23181\nNumber of unique tags in val data  125\nNumber of unique tokens in test data  23252\nNumber of unique tags in test data  124\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_pad_data():\n    tokenizer = text.Tokenizer(num_tokens+1, lower=True)\n    label_tokenizer = text.Tokenizer(num_tags+1, lower=True)\n    tokenizer.fit_on_texts(train_sequences)\n    \"\"\"Train the tokenizer on the test and valdiation sequences, \n    otherwise, not all tokens will be tokized and will cause clashes\"\"\"\n    tokenizer.fit_on_texts(val_sequences)\n    tokenizer.fit_on_texts(test_sequences) \n    label_tokenizer.fit_on_texts(train_labels)\n    \n    encoded_train_sequences = tokenizer.texts_to_sequences(train_sequences)\n    encoded_train_labels = label_tokenizer.texts_to_sequences(train_labels)    \n    encoded_test_sequences = tokenizer.texts_to_sequences(test_sequences)\n    encoded_test_labels = label_tokenizer.texts_to_sequences(test_labels)  \n    encoded_val_sequences = tokenizer.texts_to_sequences(val_sequences)\n    encoded_val_labels = label_tokenizer.texts_to_sequences(val_labels)  \n    \n    #encoded_labels_ohe = [to_categorical(i, num_classes=num_tags+1) for i in encoded_labels]\n\n    padded_train_sequences = sequence.pad_sequences(encoded_train_sequences, dtype='int32', maxlen=maxlen, padding='post')\n    padded_train_labels = sequence.pad_sequences(encoded_train_labels, maxlen=maxlen, dtype='int32', padding='post')\n    padded_test_sequences = sequence.pad_sequences(encoded_test_sequences, dtype='int32', maxlen=maxlen, padding='post')\n    padded_test_labels = sequence.pad_sequences(encoded_test_labels, maxlen=maxlen, dtype='int32', padding='post')\n    padded_val_sequences = sequence.pad_sequences(encoded_val_sequences, dtype='int32', maxlen=maxlen, padding='post') \n    padded_val_labels = sequence.pad_sequences(encoded_val_labels, maxlen=maxlen, dtype='int32', padding='post')\n    \n    return padded_train_sequences, padded_train_labels, padded_test_sequences, padded_test_labels , padded_val_sequences, padded_val_labels, tokenizer ,label_tokenizer\n\ntrain_seqs, train_labels, test_seqs, test_labels, val_seqs, val_labels, tokenizer ,label_tokenizer = encode_pad_data()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_label_map = {v: k for k, v in label_tokenizer.word_index.items()}\ninv_token_map = {v: k for k, v in tokenizer.word_index.items()}","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(inv_token_map)","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"66317"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.array(train_seqs).shape)\nprint(np.array(train_labels).shape)\nprint(np.array(test_seqs).shape)\nprint(np.array(test_labels).shape)\nprint(np.array(val_seqs).shape)\nprint(np.array(val_labels).shape)","execution_count":11,"outputs":[{"output_type":"stream","text":"(2635, 685)\n(2635, 685)\n(879, 685)\n(879, 685)\n(878, 685)\n(878, 685)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### F1 score excluding certain entities like 'O' and \"PAD\""},{"metadata":{"trusted":true},"cell_type":"code","source":"def exclude_from_f1(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    f1 = f1_score(ytrue, yhat, average='weighted')\n    return f1","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BiLSTM with attentions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask(m, q):\n    # Assumes m is 2D\n    mask = tf.math.reduce_any(tf.not_equal(m, q), axis=-1)\n    #return tf.boolean_mask(m, mask)\n    return mask\n\ndef recall(y_true, y_pred):\n    pad = tf.constant([0 for i in range(num_tags+1)], dtype=tf.float32)\n    mask_ = mask(y_true, pad)\n    masked_y_data = tf.boolean_mask(y_true, mask_)\n    masked_y_pred = tf.boolean_mask(y_pred, mask_)\n    true_positives = K.sum(K.round(K.clip(masked_y_data * masked_y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(masked_y_data, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision(y_true, y_pred):\n    pad = tf.constant([0 for i in range(num_tags+1)], dtype=tf.float32)\n    mask_ = mask(y_true, pad)\n    masked_y_data = tf.boolean_mask(y_true, mask_)\n    masked_y_pred = tf.boolean_mask(y_pred, mask_)\n    true_positives = K.sum(K.round(K.clip(masked_y_data * masked_y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(masked_y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1(y_true, y_pred):\n    precision_ = precision(y_true, y_pred)\n    recall_ = recall(y_true, y_pred)\n    return 2*((precision_*recall_)/(precision_+recall_+K.epsilon()))","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_dim = 100\ntf.random.set_seed(42)\nopt = Adam(0.01)\n\nsequence_input = Input(shape=(maxlen,), dtype=tf.int32, name='sequence_input')\nsequence_mask = Lambda(lambda x: tf.greater(x, 0))(sequence_input)\noutputs = Embedding(input_dim=num_tokens+1, output_dim=output_dim, trainable=True, mask_zero=True)(sequence_input)\noutputs = Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat')(outputs)\noutputs = (TimeDistributed(Dense(output_dim, activation=\"relu\")))(outputs)\n\noutputs = Dense(num_tags+1, activation=\"softmax\")(outputs)\n\nmodel = Model(inputs=sequence_input, outputs=outputs)\nmodel.compile(loss = 'SparseCategoricalCrossentropy', optimizer=opt, metrics=['accuracy'])\nmodel.summary()","execution_count":25,"outputs":[{"output_type":"stream","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsequence_input (InputLayer)  [(None, 685)]             0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 685, 100)          6631800   \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 685, 200)          160800    \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 685, 100)          20100     \n_________________________________________________________________\ndense_3 (Dense)              (None, 685, 128)          12928     \n=================================================================\nTotal params: 6,825,628\nTrainable params: 6,825,628\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_weights(y_classes):    \n    total = len(y_classes)\n    class_dict = {}\n    for tag in y_classes:\n        if tag not in class_dict.keys():\n            class_dict[tag] = 1\n        else:\n            class_dict[tag] += 1\n\n    class_weight = {}\n\n    for key, value in class_dict.items():\n        class_weight[key] = (1 / value * total / num_tags)\n    \n    for i in range(127):\n        if i not in class_weight.keys():\n            class_weight[i] = 0\n    class_weight[0] = 0\n    return class_weight","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(20):\n    print('epoch ', epoch)\n    for x, y in zip(train_seqs, train_labels):\n        weights = get_weights(y)\n        model.train_on_batch(x, y, class_weight=weights)\n\n    val_f1 = []\n    for x, y in zip(val_seqs[:100], val_labels[:100]):\n        y_pred = np.argmax(model.predict(x), axis=-1)\n        val_f1.append(exclude_from_f1(y, y_pred, [0, 1]))\n    \n    print(np.mean(val_f1))\n                #sample_weight=class_weights_arr)#, callbacks=[accuracy_reached])","execution_count":26,"outputs":[{"output_type":"stream","text":"epoch  0\n0.775173251649654\nepoch  1\n0.8234216969425718\nepoch  2\n0.8335770025633539\nepoch  3\n0.8410674965074513\nepoch  4\n0.8464045860677775\nepoch  5\n0.8465307949579468\nepoch  6\n0.8490874167194016\nepoch  7\n0.8498744802113234\nepoch  8\n0.8500002116002235\nepoch  9\n0.8524950397519704\nepoch  10\n0.8515114480391359\nepoch  11\n0.8490540579284545\nepoch  12\n0.8522782980947268\nepoch  13\n0.8522411155052165\nepoch  14\n0.8492012639213445\nepoch  15\n0.8511595434411251\nepoch  16\n0.8502245950491596\nepoch  17\n0.8477115334304046\nepoch  18\n0.8473053002188435\nepoch  19\n0.8478006689706451\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(model.predict(test_seqs[400]), axis=-1)\nfor x, yhat, y in zip(test_seqs[400],y_pred, test_labels[400]):\n    if x != 0:\n        print(inv_token_map[x], inv_label_map[y], inv_label_map[yhat[0]])","execution_count":27,"outputs":[{"output_type":"stream","text":"c2350277 t063 t063\nof o o\nc0017393 t045 t045\nin o o\nc0008051 t012 t012\nthrough o o\nc0752046 t086 t086\nc0008051 t012 t012\nis o o\nrecognized o o\nas o o\nan o o\nexcellent o o\nc0026339 t075 t075\nfor o o\nstudies o o\nof o o\nc0314603 t169 t169\nc0441712 t169 t169\nof o o\nc0031437 t032 t032\nand o o\nc0017428 t028 t028\nc0015219 t045 t045\n, o o\nwith o o\nlarge o o\neffective o o\nc0032683 t081 t081\nand o o\nstrong o o\nc0086418 t016 t016\n-driven o o\nc1707391 t052 t052\n. o o\nin o o\nthe o o\npresent o o\nstudy, o o\nwe o o\nperformed o o\nc0545278 unknowntype t005\n( o o\nc0545278 unknowntype t005\n) o o\ntests o o\nto o o\nidentify o o\nsignificant o o\nc0004793 t086 t086\nemploying o o\n600k o t005\nc0752046 t086 t086\nc0008051 t012 t012\nc3897601 t063 t005\nin o o\nan o o\nc0441833 t078 t078\nof o o\n1,534 o t005\nc0005595 t012 t012\n, o o\nwhich o o\nwas o o\nderived o o\nfrom o o\nc0010364 t059 t005\nbetween o o\nc0008051 t012 t012\nand o o\nc0008051 t012 t012\n. o o\nresults o o\nindicated o o\nthat o o\na o o\ntotal o o\nof o o\n49,151 o t005\nc0004793 t086 t086\nwith o o\nan o o\naverage o o\nc1444754 t081 t081\nof o o\n9.79 o t005\nkb o o\nwere o o\nc0205396 t080 t080\n, o o\nwhich o o\noccupied o t005\napproximately o o\n52.15% o t005\nof o o\nc0017428 t028 t028\nacross o o\nall o o\nc0596142 t026 t026\n, o o\nand o o\n806 o t005\nsignificant o o\nc0004793 t086 t086\nattracted o o\nus o o\nmostly. o t005\nc0017337 t028 t028\nin o o\nc0004793 t086 t086\nmay o o\nexperience o o\nc2347644 t059 t005\nand o o\nwere o o\nconsidered o o\nto o o\nhave o o\npossible o o\ninfluence o o\non o o\nbeneficial o o\nc0205556 t080 t080\n. o o\na o o\nc0441833 t078 t078\nof o o\nc0017337 t028 t028\nincluding o o\nc1422497 t028 t005\n, o o\nc1539559 t028 t005\n, o o\nc1414507 t028 t005\n, o o\nc1424266 t028 t005\n, o o\nc1415193 t028 t005\nand o o\na o o\nc0037080 t044 t044\nof o o\nc0812228 t028 t005\nwere o o\nc0442726 t033 t033\nwith o o\nthe o o\nmost o o\nextreme o o\nc1709380 t081 t081\n. o o\nfurther o o\nc0936012 t062 t062\nindicated o o\nthat o o\nthese o o\nc0017337 t028 t028\nwere o o\nc0332281 t080 t080\nc1817756 t042 t042\n, o o\nc1160370 t042 t005\nand o o\nc0814002 t040 t040\n, o o\nand o o\nmay o o\nhave o o\nexperienced o o\nc2347644 t059 t005\nin o o\nc0008051 t012 t012\n. o o\nmoreover, o o\nsome o o\nof o o\nc0004793 t086 t086\nexactly o o\noverlapped o o\nwith o o\nc0017337 t028 t028\nexcavated o t005\nin o o\nour o o\nprevious o o\nc2350277 t063 t063\n, o o\nsuggesting o o\nthat o o\nthese o o\nc0017337 t028 t028\nhave o o\nundergone o o\nc2347644 t059 t005\nmay o o\naffect o o\nc0013710 t168 t168\nc0441655 t052 t052\n. o o\nc2607943 t169 t169\nin o o\nour o o\nstudy o o\ncould o o\ndraw o o\na o o\ncomparatively o o\nintegrate o o\nc0079435 t062 t062\nof o o\nc0017393 t045 t045\nin o o\nthe o o\nc0008051 t012 t012\nc0017428 t028 t028\n, o o\nand o o\nwould o o\nbe o o\nworthy o o\nfor o o\nexplicating o t005\nthe o o\nc0314603 t169 t169\nc0441712 t169 t169\nof o o\nc0031437 t032 t032\nc1880371 t080 t080\nin o o\nc0032850 t012 t012\nc0006159 t040 t040\n. o o\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_scores_no_O, f1_scores_with_O = [], []\ny_pred = []\n\nfor x, y in zip(test_seqs, test_labels):\n    ypred = model.predict(x)\n    ypred = np.argmax(ypred, axis=-1)\n    y_pred.extend(ypred)\n    f1_scores_no_O.append(exclude_from_f1(y, ypred, [0, 1]))\n    f1_scores_with_O.append(exclude_from_f1(y, ypred, [0]))\n\nprint('f1 score on test data including \"Other\" tag', np.mean(f1_scores_with_O))\nprint('f1 score on test data without \"Other\" tag', np.mean(f1_scores_no_O))","execution_count":28,"outputs":[{"output_type":"stream","text":"f1 score on test data including \"Other\" tag 0.9327400148537758\nf1 score on test data without \"Other\" tag 0.8515278887145905\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"},{"metadata":{},"cell_type":"markdown","source":"### LSTM with UMLS customed Embedding"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def concept_dict():\n    umls_embeddings = pd.read_csv(umls_embedding_file, header=None)\n    umls_embeddings_arr = np.array(umls_embeddings)\n    dict_ = {} \n    for concept in umls_embeddings_arr:\n        dict_[concept[0]] = concept[1:]\n    \n    del umls_embeddings\n    del umls_embeddings_arr\n    \n    return dict_\n\nconcepts = concept_dict()","execution_count":14,"outputs":[{"output_type":"stream","text":"here\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def get_embeddings():\n    embed_size=50\n\n    word_index = tokenizer.word_index\n\n    embedding_matrix = np.zeros((num_tokens+1, embed_size))\n\n    for word, i in word_index.items():\n        if i%1000 == 0:\n            print('1000 words done')\n        embedding_vector = concepts.get(word.upper()) #umls_embeddings.loc[umls_embeddings[0].values() == word.upper()]\n        #embedding_vector = np.array(embedding_vector)[:, 1:]\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix, embed_size\n\nembedding_matrix, embed_size = get_embeddings()\ndel concepts","execution_count":15,"outputs":[{"output_type":"stream","text":"1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n1000 words done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras_self_attention import SeqSelfAttention\n\noutput_dim = 32\ntf.random.set_seed(42)\nopt = Adam(0.001)\n\nsequence_input = Input(shape=(maxlen,), dtype=tf.int32, name='sequence_input')\nsequence_mask = Lambda(lambda x: tf.greater(x, 0))(sequence_input)\noutputs = Embedding(input_dim=num_tokens+1, output_dim=embed_size, weights=[embedding_matrix], trainable=True, mask_zero=True)(sequence_input)\noutputs = Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat')(outputs)\noutputs = (TimeDistributed(Dense(output_dim, activation=\"relu\")))(outputs)\n\n#outputs = Conv1D(64, kernel_size=3, padding='same', kernel_initializer='glorot_uniform')(outputs)\n\n#avg_pool = GlobalAveragePooling1D()(outputs)\n#max_pool = GlobalAveragePooling1D()(outputs)\n#outputs = concatenate([avg_pool, max_pool])\noutputs = Dense(num_tags+1, activation=\"softmax\")(outputs)\n\nmodel = Model(inputs=sequence_input, outputs=outputs)\nmodel.compile(loss = 'SparseCategoricalCrossentropy', optimizer=opt, metrics=['accuracy'])\nmodel.summary()","execution_count":17,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsequence_input (InputLayer)  [(None, 685)]             0         \n_________________________________________________________________\nembedding (Embedding)        (None, 685, 50)           3315900   \n_________________________________________________________________\nbidirectional (Bidirectional (None, 685, 64)           21248     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 685, 32)           2080      \n_________________________________________________________________\ndense_1 (Dense)              (None, 685, 128)          4224      \n=================================================================\nTotal params: 3,343,452\nTrainable params: 3,343,452\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(20):\n    print('epoch ', epoch)\n    for x, y in zip(train_seqs, train_labels):\n        weights = get_weights(y)\n        model.train_on_batch(x, y, class_weight=weights)\n\n    val_f1 = []\n    for x, y in zip(val_seqs[:100], val_labels[:100]):\n        y_pred = np.argmax(model.predict(x), axis=-1)\n        val_f1.append(exclude_from_f1(y, y_pred, [0, 1]))\n    \n    print(np.mean(val_f1))\n                #sample_weight=class_weights_arr)#, callbacks=[accuracy_reached])","execution_count":18,"outputs":[{"output_type":"stream","text":"epoch  0\n0.7181724454134879\nepoch  1\n0.8073648844690546\nepoch  2\n0.8276048702985144\nepoch  3\n0.8544529420642106\nepoch  4\n0.8590060742729123\nepoch  5\n0.8610952470975347\nepoch  6\n0.8614899307170539\nepoch  7\n0.8613326378830167\nepoch  8\n0.8587416430405108\nepoch  9\n0.8619065507288421\nepoch  10\n0.8622863866606998\nepoch  11\n0.861752374239942\nepoch  12\n0.8622863866606998\nepoch  13\n0.8622863866606998\nepoch  14\n0.8622354816833241\nepoch  15\n0.8621831874862932\nepoch  16\n0.862062506063685\nepoch  17\n0.8618390088295879\nepoch  18\n0.8622863866606998\nepoch  19\n0.8622863866606998\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(model.predict(test_seqs[400]), axis=-1)\nfor x, yhat, y in zip(test_seqs[400],y_pred, test_labels[400]):\n    if x != 0:\n        print(inv_token_map[x], inv_label_map[y], inv_label_map[yhat[0]])","execution_count":22,"outputs":[{"output_type":"stream","text":"c2350277 t063 t063\nof o o\nc0017393 t045 t045\nin o o\nc0008051 t012 t012\nthrough o o\nc0752046 t086 t086\nc0008051 t012 t012\nis o o\nrecognized o o\nas o o\nan o o\nexcellent o o\nc0026339 t075 t075\nfor o o\nstudies o o\nof o o\nc0314603 t169 t169\nc0441712 t169 t169\nof o o\nc0031437 t032 t032\nand o o\nc0017428 t028 t028\nc0015219 t045 t045\n, o o\nwith o o\nlarge o o\neffective o o\nc0032683 t081 t081\nand o o\nstrong o o\nc0086418 t016 t016\n-driven o o\nc1707391 t052 t052\n. o o\nin o o\nthe o o\npresent o o\nstudy, o o\nwe o o\nperformed o o\nc0545278 unknowntype t190\n( o o\nc0545278 unknowntype t190\n) o o\ntests o o\nto o o\nidentify o o\nsignificant o o\nc0004793 t086 t086\nemploying o o\n600k o t190\nc0752046 t086 t086\nc0008051 t012 t012\nc3897601 t063 t190\nin o o\nan o o\nc0441833 t078 t078\nof o o\n1,534 o t190\nc0005595 t012 t012\n, o o\nwhich o o\nwas o o\nderived o o\nfrom o o\nc0010364 t059 t190\nbetween o o\nc0008051 t012 t012\nand o o\nc0008051 t012 t012\n. o o\nresults o o\nindicated o o\nthat o o\na o o\ntotal o o\nof o o\n49,151 o t190\nc0004793 t086 t086\nwith o o\nan o o\naverage o o\nc1444754 t081 t081\nof o o\n9.79 o t190\nkb o o\nwere o o\nc0205396 t080 t080\n, o o\nwhich o o\noccupied o t190\napproximately o o\n52.15% o t190\nof o o\nc0017428 t028 t028\nacross o o\nall o o\nc0596142 t026 t026\n, o o\nand o o\n806 o t190\nsignificant o o\nc0004793 t086 t086\nattracted o o\nus o o\nmostly. o t190\nc0017337 t028 t028\nin o o\nc0004793 t086 t086\nmay o o\nexperience o o\nc2347644 t059 t190\nand o o\nwere o o\nconsidered o o\nto o o\nhave o o\npossible o o\ninfluence o o\non o o\nbeneficial o o\nc0205556 t080 t080\n. o o\na o o\nc0441833 t078 t078\nof o o\nc0017337 t028 t028\nincluding o o\nc1422497 t028 t190\n, o o\nc1539559 t028 t190\n, o o\nc1414507 t028 t190\n, o o\nc1424266 t028 t190\n, o o\nc1415193 t028 t190\nand o o\na o o\nc0037080 t044 t044\nof o o\nc0812228 t028 t190\nwere o o\nc0442726 t033 t033\nwith o o\nthe o o\nmost o o\nextreme o o\nc1709380 t081 t081\n. o o\nfurther o o\nc0936012 t062 t062\nindicated o o\nthat o o\nthese o o\nc0017337 t028 t028\nwere o o\nc0332281 t080 t080\nc1817756 t042 t042\n, o o\nc1160370 t042 t190\nand o o\nc0814002 t040 t040\n, o o\nand o o\nmay o o\nhave o o\nexperienced o o\nc2347644 t059 t190\nin o o\nc0008051 t012 t012\n. o o\nmoreover, o o\nsome o o\nof o o\nc0004793 t086 t086\nexactly o o\noverlapped o o\nwith o o\nc0017337 t028 t028\nexcavated o t190\nin o o\nour o o\nprevious o o\nc2350277 t063 t063\n, o o\nsuggesting o o\nthat o o\nthese o o\nc0017337 t028 t028\nhave o o\nundergone o o\nc2347644 t059 t190\nmay o o\naffect o o\nc0013710 t168 t168\nc0441655 t052 t052\n. o o\nc2607943 t169 t169\nin o o\nour o o\nstudy o o\ncould o o\ndraw o o\na o o\ncomparatively o o\nintegrate o o\nc0079435 t062 t062\nof o o\nc0017393 t045 t045\nin o o\nthe o o\nc0008051 t012 t012\nc0017428 t028 t028\n, o o\nand o o\nwould o o\nbe o o\nworthy o o\nfor o o\nexplicating o t190\nthe o o\nc0314603 t169 t169\nc0441712 t169 t169\nof o o\nc0031437 t032 t032\nc1880371 t080 t080\nin o o\nc0032850 t012 t012\nc0006159 t040 t040\n. o o\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_scores_no_O, f1_scores_with_O = [], []\ny_pred = []\n\nfor x, y in zip(test_seqs, test_labels):\n    ypred = model.predict(x)\n    ypred = np.argmax(ypred, axis=-1)\n    y_pred.extend(ypred)\n    f1_scores_no_O.append(exclude_from_f1(y, ypred, [0, 1]))\n    f1_scores_with_O.append(exclude_from_f1(y, ypred, [0]))\n\nprint('f1 score on test data including \"Other\" tag', np.mean(f1_scores_with_O))\nprint('f1 score on test data without \"Other\" tag', np.mean(f1_scores_no_O))","execution_count":23,"outputs":[{"output_type":"stream","text":"f1 score on test data including \"Other\" tag 0.9371161541520516\nf1 score on test data without \"Other\" tag 0.8626894803513677\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}