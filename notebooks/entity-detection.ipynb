{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"bb0c5777-c7dc-49cc-8ba7-4b564087d331","_cell_guid":"55a376bc-981e-41d5-a628-ce0c0c5a7e3a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport json\n\nimport pandas as pd\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, Dense, LSTM, TimeDistributed, Lambda, SpatialDropout1D, Layer\n#from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop\n\nimport tensorflow_addons as tfa\n\nfrom keras import backend as K\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\nfrom transformers import BertTokenizer, BertConfig, TFBertForTokenClassification, TFBertModel\n\nimport warnings","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reformat_data(data_file):\n    with open(data_file, 'r') as file:\n        article_sentences, article_labels = [], []\n        sentence_tokens, sentence_labels = [], []\n        \n        for line in file.readlines():\n            if \"-DOCSTART-\" in line:\n                if sentence_labels != []:\n                    article_sentences.append(sentence_tokens)\n                    article_labels.append(sentence_labels)\n                    sentence_tokens, sentence_labels = [], []\n            else:\n                try:\n                    token = line.split(\"\\t\")[0]\n                    label = line.split(\"\\t\")[3][:-1]\n                    sentence_tokens.append(token)\n                    sentence_labels.append(label)\n                except:\n                    if sentence_labels != []:\n                        article_sentences.append(sentence_tokens)\n                        article_labels.append(sentence_labels)\n                        sentence_tokens, sentence_labels = [], []\n                \n    return article_sentences, article_labels\n    \ntrain_sentences, train_detect_labels = reformat_data(\"../input/medlinker-data/mm_ner_ent.train.conll\")\ntest_sentences, test_detect_labels = reformat_data(\"../input/medlinker-data/mm_ner_ent.test.conll\")\ndev_sentences, dev_detect_labels = reformat_data(\"../input/medlinker-data/mm_ner_ent.dev.conll\")\n_, train_recog_labels = reformat_data(\"../input/medlinker-data/mm_ner_sts.train.conll\")\n_, test_recog_labels = reformat_data(\"../input/medlinker-data/mm_ner_sts.test.conll\")\n_, dev_recog_labels = reformat_data(\"../input/medlinker-data/mm_ner_sts.dev.conll\")","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"b430f3cd-4160-430b-a60a-27e085fc451f","_cell_guid":"adfafa15-232f-4bbb-bd9e-638b3d5e4b88","trusted":true},"cell_type":"code","source":"def unique_words():\n    dict_ = {}\n    lengths = []\n    sent = []\n    i = 0\n    for txt in [train_sentences, test_sentences, dev_sentences]:\n        for sentence in txt:\n            lengths.append(len(sentence))\n            sent.append(sentence)\n            for word in np.unique(sentence):\n                if word.lower() not in dict_.keys():\n                    i+=1\n                    dict_[word.lower()] = i\n                    \n    return dict_, np.max(lengths), sent\n            \ntokens_dict, maxlen, sent = unique_words()\nmaxlen","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"178"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokens_dict)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"54563"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict = {}\ni = 0\nfor sent_labels in train_detect_labels:\n    for label in sent_labels:\n        if label not in label_dict.keys():\n            i+=1\n            label_dict[label] = i ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict['[PAD]'] = 0\nlabel_dict","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"{'B-Entity': 1, 'O': 2, 'I-Entity': 3, '[PAD]': 0}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\"\"\"This ignored warning because precision and recall give warnings\nthat not all the true labels are represented in the predictions\"\"\"\n\ndef exclude_from_f1(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    f1 = f1_score(ytrue, yhat, average='micro')\n    return f1\n\ndef exclude_from_precision(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    precision = precision_score(ytrue, yhat, average='micro')\n    return precision\n\ndef exclude_from_recall(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    recall = recall_score(ytrue, yhat, average='micro')\n    return recall","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask(m, q):\n    # Assumes m is 2D\n    mask = tf.math.reduce_any(tf.not_equal(m, q), axis=-1)\n    #return tf.boolean_mask(m, mask)\n    return mask\n\ndef recall(y_true, y_pred):\n    pad = tf.constant([0 for i in range(4)], dtype=tf.float32)\n    mask_ = mask(y_true, pad)\n    masked_y_data = tf.boolean_mask(y_true, mask_)\n    masked_y_pred = tf.boolean_mask(y_pred, mask_)\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision(y_true, y_pred):\n    pad = tf.constant([0 for i in range(4)], dtype=tf.float32)\n    mask_ = mask(y_true, pad)\n    masked_y_data = tf.boolean_mask(y_true, mask_)\n    masked_y_pred = tf.boolean_mask(y_pred, mask_)\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n\ndef f1(y_true, y_pred):\n    precision_ = precision(y_true, y_pred)\n    recall_ = recall(y_true, y_pred)\n    return 2*((precision_*recall_)/(precision_+recall_+K.epsilon()))","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n#!wget \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\"\n!wget -O umlsbert.tar.xz https://www.dropbox.com/s/qaoq5gfen69xdcc/umlsbert.tar.xz?dl=0\n#!tar -xf scibert_scivocab_uncased.tar\n!tar -xvf umlsbert.tar.xz\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('./umlsbert', do_lower_case=True)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"PreTrainedTokenizer(name_or_path='./umlsbert', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = sequence.pad_sequences(train_sentences, dtype=object, maxlen=maxlen, padding='post', value='[PAD]')\ndev_seq = sequence.pad_sequences(dev_sentences, dtype=object, maxlen=maxlen, padding='post', value='[PAD]') \ntest_seq = sequence.pad_sequences(test_sentences, dtype=object, maxlen=maxlen, padding='post', value='[PAD]') \ntrain_seq_tokenized = [tokenizer.convert_tokens_to_ids(s) for s in train_seq]\ndev_seq_tokenized = [tokenizer.convert_tokens_to_ids(s) for s in dev_seq]\ntest_seq_tokenized = [tokenizer.convert_tokens_to_ids(s) for s in test_seq]","execution_count":64,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lebel tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels, dev_labels, test_labels = train_detect_labels, dev_detect_labels, test_detect_labels\n\nfor i, labels in enumerate(train_detect_labels):\n    for j, label in enumerate(labels):\n        train_labels[i][j] = label_dict[label]\nfor i, labels in enumerate(dev_detect_labels):\n    for j, label in enumerate(labels):\n        dev_labels[i][j] = label_dict[label]\nfor i, labels in enumerate(test_detect_labels):\n    for j, label in enumerate(labels):\n        test_labels[i][j] = label_dict[label]\n        \ntrain_labels_ohe = [to_categorical(i, num_classes=4) for i in train_labels]\ndev_labels_ohe = [to_categorical(i, num_classes=4) for i in dev_labels]\ntest_labels_ohe = [to_categorical(i, num_classes=4) for i in test_labels]\n\ntrain_labels = sequence.pad_sequences(train_labels_ohe, maxlen=maxlen, dtype='int32', padding='post')\ndev_labels = sequence.pad_sequences(dev_labels_ohe, maxlen=maxlen, dtype='int32', padding='post')\ntest_labels = sequence.pad_sequences(test_labels_ohe, maxlen=maxlen, dtype='int32', padding='post')\n\ntrain_labels = np.array(train_labels)\ndev_labels = np.array(dev_labels)\ntest_labels = np.array(test_labels)","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build mask to ignore padded values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mask = [[1]*len(sent)+[0]*(maxlen - len(sent)) for sent in train_sentences]\ntrain_mask = tf.cast(train_mask,tf.int32)\ndev_mask = [[1]*len(sent)+[0]*(maxlen - len(sent)) for sent in dev_sentences]\ndev_mask = tf.cast(dev_mask,tf.int32)\ntest_mask = [[1]*len(sent)+[0]*(maxlen - len(sent)) for sent in test_sentences]\ntest_mask = tf.cast(test_mask,tf.int32)","execution_count":61,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cast sequences into tensors"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = train_seq_tokenized\ntrain_seq = tf.cast(train_seq, tf.int32)\ndev_seq = dev_seq_tokenized\ndev_seq = tf.cast(dev_seq, tf.int32)\ntest_seq = test_seq_tokenized\ntest_seq = tf.cast(test_seq, tf.int32)\ntrain_labels = tf.cast(train_labels, tf.int32)\ndev_labels = tf.cast(dev_labels, tf.int32)\ntest_labels = tf.cast(test_labels, tf.int32)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_seq.shape)\nprint(train_mask.shape)\nprint(train_labels.shape)\nprint(dev_seq.shape)\nprint(dev_mask.shape)\nprint(dev_labels.shape)\nprint(test_seq.shape)\nprint(test_mask.shape)\nprint(test_labels.shape)","execution_count":67,"outputs":[{"output_type":"stream","text":"(27892, 178)\n(27892, 178)\n(27892, 178, 4)\n(9219, 178)\n(9219, 178)\n(9219, 178, 4)\n(9283, 178)\n(9283, 178)\n(9283, 178, 4)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Scibert LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./umlsbert/","execution_count":18,"outputs":[{"output_type":"stream","text":"config.json  pytorch_model.bin\tvocab.txt\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_bert_lstm_model():\n    config = BertConfig.from_json_file('./umlsbert/config.json')\n    #config.return_dict=True\n    config.output_hidden_states=True\n    encoder = TFBertModel.from_pretrained(\"./umlsbert/\", from_pt=True, name='scibert', config=config)\n    encoder.bert.trainable = False\n\n    input_ids = Input(shape=(maxlen,), dtype=tf.int32)\n\n    attention_mask = Input(shape=(maxlen,), dtype=tf.int32)\n    outputs = encoder(input_ids, attention_mask=attention_mask)\n    last_hidden_state = outputs.hidden_states[0]\n        \n    outputs = Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.5, recurrent_dropout=0.5), \n                            merge_mode = 'concat')(last_hidden_state)\n    outputs = LSTM(units=128, dropout=0.5,  return_sequences=True, recurrent_dropout=0.5)(outputs)\n\n    outputs = Dense(len(label_dict), activation='softmax', name='output')(outputs)\n    \n    bert_lstm_model = Model([input_ids, attention_mask], outputs)\n\n    bert_lstm_model.summary()\n    \n    return bert_lstm_model","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(42)\nopt = Adam(0.001)\n\nwith tpu_strategy.scope(): \n    bert_lstm_model = build_bert_lstm_model()\n    bert_lstm_model.compile(loss = 'CategoricalCrossentropy', optimizer=opt, metrics=[f1, recall, precision])","execution_count":36,"outputs":[{"output_type":"stream","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'bert.embeddings.tui_type_embeddings.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","name":"stderr"},{"output_type":"stream","text":"Model: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_7 (InputLayer)            [(None, 178)]        0                                            \n__________________________________________________________________________________________________\ninput_8 (InputLayer)            [(None, 178)]        0                                            \n__________________________________________________________________________________________________\nscibert (TFBertModel)           TFBaseModelOutputWit 108310272   input_7[0][0]                    \n                                                                 input_8[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_3 (Bidirectional) (None, 178, 256)     918528      scibert[0][0]                    \n__________________________________________________________________________________________________\nlstm_7 (LSTM)                   (None, 178, 128)     197120      bidirectional_3[0][0]            \n__________________________________________________________________________________________________\noutput (Dense)                  (None, 178, 4)       516         lstm_7[0][0]                     \n==================================================================================================\nTotal params: 109,426,436\nTrainable params: 1,116,164\nNon-trainable params: 108,310,272\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_lstm_his = bert_lstm_model.fit((train_seq, train_mask), train_labels, \n                                    epochs=10, batch_size=64, \n                                    validation_data=((dev_seq, dev_mask), dev_labels))","execution_count":41,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n436/436 [==============================] - 27s 61ms/step - loss: 0.0451 - f1: 0.8593 - recall: 0.8369 - precision: 0.8831 - val_loss: 0.0465 - val_f1: 0.8540 - val_recall: 0.8366 - val_precision: 0.8724\nEpoch 2/10\n436/436 [==============================] - 27s 62ms/step - loss: 0.0449 - f1: 0.8605 - recall: 0.8381 - precision: 0.8843 - val_loss: 0.0464 - val_f1: 0.8553 - val_recall: 0.8355 - val_precision: 0.8763\nEpoch 3/10\n436/436 [==============================] - 27s 62ms/step - loss: 0.0444 - f1: 0.8623 - recall: 0.8399 - precision: 0.8860 - val_loss: 0.0460 - val_f1: 0.8576 - val_recall: 0.8382 - val_precision: 0.8781\nEpoch 4/10\n436/436 [==============================] - 27s 61ms/step - loss: 0.0439 - f1: 0.8636 - recall: 0.8419 - precision: 0.8867 - val_loss: 0.0461 - val_f1: 0.8561 - val_recall: 0.8369 - val_precision: 0.8764\nEpoch 5/10\n436/436 [==============================] - 27s 62ms/step - loss: 0.0436 - f1: 0.8645 - recall: 0.8432 - precision: 0.8871 - val_loss: 0.0464 - val_f1: 0.8547 - val_recall: 0.8353 - val_precision: 0.8751\nEpoch 6/10\n436/436 [==============================] - 27s 61ms/step - loss: 0.0433 - f1: 0.8655 - recall: 0.8445 - precision: 0.8878 - val_loss: 0.0463 - val_f1: 0.8577 - val_recall: 0.8397 - val_precision: 0.8766\nEpoch 7/10\n436/436 [==============================] - 27s 62ms/step - loss: 0.0430 - f1: 0.8664 - recall: 0.8459 - precision: 0.8881 - val_loss: 0.0463 - val_f1: 0.8571 - val_recall: 0.8399 - val_precision: 0.8752\nEpoch 8/10\n436/436 [==============================] - 27s 61ms/step - loss: 0.0426 - f1: 0.8676 - recall: 0.8472 - precision: 0.8892 - val_loss: 0.0471 - val_f1: 0.8549 - val_recall: 0.8414 - val_precision: 0.8689\nEpoch 9/10\n436/436 [==============================] - 27s 61ms/step - loss: 0.0425 - f1: 0.8680 - recall: 0.8478 - precision: 0.8893 - val_loss: 0.0468 - val_f1: 0.8551 - val_recall: 0.8385 - val_precision: 0.8725\nEpoch 10/10\n436/436 [==============================] - 27s 61ms/step - loss: 0.0423 - f1: 0.8683 - recall: 0.8483 - precision: 0.8894 - val_loss: 0.0465 - val_f1: 0.8573 - val_recall: 0.8421 - val_precision: 0.8732\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Optimization\n\n-- best results\n\nLR : 0.001\n\nepochs : 20\n\nunits: 128\n\n-----------\n\nLR : 0.002\n\nepochs : 10\n\nunits: 64\n\n---------\n\nLR : 0.0005\n\nepochs : 10\n\nunits: 64\n\n---------\n\nLR : 0.005\n\nepochs : 10\n\nunits: 64\n\n---------\n\nLR : 0.001\n\nepochs : 10\n\nunits: 64"},{"metadata":{},"cell_type":"markdown","source":"## Eval"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\"\"\"This ignored warning because precision and recall give warnings\nthat not all the true labels are represented in the predictions\"\"\"\n\ndef exclude_from_f1(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(np.array(y_true).flatten(), np.array(y_pred).flatten()):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    f1 = f1_score(ytrue, yhat, average='micro')\n    return f1\n\ndef exclude_from_precision(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    precision = precision_score(ytrue, yhat, average='micro')\n    return precision\n\ndef exclude_from_recall(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    recall = recall_score(ytrue, yhat, average='macro')\n    return recall","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = np.argmax(bert_lstm_model.predict((test_seq, test_mask)), axis=-1)\ntest_labels_argmax = np.argmax(test_labels, axis=-1)\n# This is the f1 for non other entities\nexclude_from_f1(test_labels_argmax, test_pred, [0, 2])","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"0.7248263888888887"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Example"},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_label_map = {v: k for k, v in label_dict.items()}\n\ny_pred = np.argmax(bert_lstm_model.predict((test_seq[1030], test_mask[1030])), axis=-1)\nprint(\"{0:35} {1:40} {2:40}\".format('Extracted Entity', 'Actual Label', 'Predicted Label'))\nprint(\"{0:35} {1:40} {2:40}\".format('________________', '____________', '_______________'))\nfor x, y, yhat in zip(test_sentences[1030], test_labels[1030], y_pred):\n    if x != 0:\n        print(\"{0:35} {1:40} {2:40}\".format(np.array(x), inv_label_map[np.argmax(np.array(y))], inv_label_map[yhat[0]]))","execution_count":77,"outputs":[{"output_type":"stream","text":"Extracted Entity                    Actual Label                             Predicted Label                         \n________________                    ____________                             _______________                         \nMoreover                            O                                        O                                       \n,                                   O                                        O                                       \nstratification                      B-Entity                                 B-Entity                                \nanalyses                            I-Entity                                 B-Entity                                \nindicated                           O                                        O                                       \nthat                                O                                        O                                       \nthe                                 O                                        O                                       \nR1628P                              B-Entity                                 B-Entity                                \npolymorphism                        I-Entity                                 B-Entity                                \nwas                                 O                                        O                                       \nsignificantly                       O                                        O                                       \nassociated                          O                                        O                                       \nwith                                O                                        O                                       \nan                                  O                                        O                                       \nincreased                           O                                        O                                       \nrisk                                O                                        B-Entity                                \nof                                  O                                        O                                       \nPD                                  B-Entity                                 B-Entity                                \namong                               O                                        O                                       \nChinese                             B-Entity                                 B-Entity                                \nas                                  O                                        O                                       \nwell                                O                                        O                                       \nas                                  O                                        O                                       \nnon-Chinese                         B-Entity                                 B-Entity                                \nAsian                               I-Entity                                 B-Entity                                \npopulations                         I-Entity                                 B-Entity                                \nand                                 O                                        O                                       \nan                                  O                                        O                                       \nincreased                           O                                        O                                       \nrisk                                O                                        B-Entity                                \nof                                  O                                        O                                       \nPD                                  O                                        B-Entity                                \nin                                  O                                        O                                       \nChinese                             O                                        B-Entity                                \npatients                            O                                        O                                       \nfrom                                O                                        O                                       \nChina                               B-Entity                                 B-Entity                                \n,                                   O                                        O                                       \nTaiwan                              B-Entity                                 B-Entity                                \n,                                   O                                        O                                       \nand                                 O                                        O                                       \nSingapore                           B-Entity                                 B-Entity                                \n.                                   O                                        O                                       \n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}