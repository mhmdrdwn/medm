{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seqeval","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb0c5777-c7dc-49cc-8ba7-4b564087d331","_cell_guid":"55a376bc-981e-41d5-a628-ce0c0c5a7e3a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport json\nfrom datetime import datetime\nimport shutil\nimport subprocess\nimport pandas as pd\nimport seqeval","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def refomat_data(data_file):\n    with open(data_file, 'r') as file:\n        output_lines = []\n        for line in file.readlines():\n            if \"-DOCSTART-\" in line:\n                continue\n            if line != \"\\n\":\n                token = line.split(\"\\t\")[0]\n                label = line.split(\"\\t\")[3][:-1]\n                output_lines.append(token)\n                output_lines.append(\"\\t\")\n                output_lines.append(label)\n                output_lines.append(\"\\n\")\n            elif line == \"\\n\":\n                output_lines.append(\"\\n\")\n    \n    return output_lines[1:]\n    \ntrain_detect_lines = refomat_data(\"../input/medlinker-data/mm_ner_ent.train.conll\")\ntest_detect_lines = refomat_data(\"../input/medlinker-data/mm_ner_ent.test.conll\")\ndev_detect_lines = refomat_data(\"../input/medlinker-data/mm_ner_ent.dev.conll\")\ntrain_recog_lines = refomat_data(\"../input/medlinker-data/mm_ner_sts.train.conll\")\ntest_recog_lines = refomat_data(\"../input/medlinker-data/mm_ner_sts.test.conll\")\ndev_recog_lines = refomat_data(\"../input/medlinker-data/mm_ner_sts.dev.conll\")","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir ./train_data/","execution_count":67,"outputs":[{"output_type":"stream","text":"mkdir: cannot create directory ‘./train_data/’: File exists\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./train_data/train.txt', 'w') as f:\n    for line in train_detect_lines:\n        f.write(\"%s\" % line)\nwith open('./train_data/dev.txt', 'w') as f:\n    for line in dev_detect_lines:\n        f.write(\"%s\" % line)\nwith open('./train_data/test.txt', 'w') as f:\n    for line in test_detect_lines:\n        f.write(\"%s\" % line)","execution_count":136,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./train_data/","execution_count":69,"outputs":[{"output_type":"stream","text":"dev.txt  test.txt  train.txt\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./train_data/train.txt', 'r') as f:\n    train_lines = f.readlines()\nwith open('./train_data/dev.txt', 'r') as f:\n    dev_lines = f.readlines()","execution_count":137,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lines[1:20]","execution_count":138,"outputs":[{"output_type":"execute_result","execution_count":138,"data":{"text/plain":"['as\\tO\\n',\n 'a\\tO\\n',\n 'modifier\\tO\\n',\n 'of\\tO\\n',\n 'chronic\\tB-Entity\\n',\n 'Pseudomonas\\tI-Entity\\n',\n 'aeruginosa\\tI-Entity\\n',\n 'infection\\tI-Entity\\n',\n 'in\\tO\\n',\n 'cystic\\tO\\n',\n 'fibrosis\\tO\\n',\n '\\n',\n 'Pseudomonas\\tB-Entity\\n',\n 'aeruginosa\\tI-Entity\\n',\n '(\\tI-Entity\\n',\n 'Pa\\tI-Entity\\n',\n ')\\tI-Entity\\n',\n 'infection\\tI-Entity\\n',\n 'in\\tO\\n']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = {}\n\nfor line in train_lines:\n    if line == '\\n': \n        continue\n    label = line.split('\\t')[-1][:-1]\n    assert label == 'O' or label.startswith('B-') or label.startswith('I-'), \"label wrong! %s\" % label\n    if label not in labels: labels[label] = 1\n    else: labels[label] += 1","execution_count":166,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":165,"outputs":[{"output_type":"execute_result","execution_count":165,"data":{"text/plain":"{'B-Entity': 115076, 'O': 523890, 'I-Entity': 57373}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"./train_data/label.txt\", \"w\") as f:\n    for label in reversed(sorted(labels)):\n        f.write(label+\"\\n\")","execution_count":167,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./train_data","execution_count":168,"outputs":[{"output_type":"stream","text":"dev.txt  label.txt  test.txt  train.txt\r\n","name":"stdout"}]},{"metadata":{"_uuid":"b430f3cd-4160-430b-a60a-27e085fc451f","_cell_guid":"adfafa15-232f-4bbb-bd9e-638b3d5e4b88","trusted":true},"cell_type":"code","source":"def unique_words():\n    dict_ = {}\n    lengths = []\n    for txt in [train_sentences, test_sentences, dev_sentences]:\n        for article in txt:\n            lengths.append(max([len(sent) for sent in article]))\n            for sentence in article:\n                for word in np.unique(sentence):\n                    if word.lower() not in dict_.keys():\n                        dict_[word.lower()] = 1\n                    else:\n                        dict_[word.lower()] += 1\n                    \n    return len(dict_), max(lengths)\n            \nnum_tokens, maxlen = unique_words()\nnum_tokens, maxlen","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82c25d76-f5fa-47f0-adb7-4e302605d631","_cell_guid":"3bb9ca42-9c13-425c-9abb-6c3bcfe90394","trusted":true},"cell_type":"markdown","source":"## Download and extract the pre-trained UmlsBERT model"},{"metadata":{"_uuid":"dab85c02-8875-4d96-94bc-1eee3d76885a","_cell_guid":"50e82f54-4684-4bae-a9e6-31a50b0ce951","trusted":true,"collapsed":true},"cell_type":"code","source":"!wget -O umlsbert.tar.xz https://www.dropbox.com/s/qaoq5gfen69xdcc/umlsbert.tar.xz?dl=0\n!tar -xvf umlsbert.tar.xz\n!rm umlsbert.tar.xz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_command(config):\n    command = \"python3\"\n    command += \" \" + config[\"run_file\"] + \" \"\n    command += \"--output_dir \" + config[\"output_dir\"] + \" \"\n    command += \"--model_name_or_path \" + config[\"model_name_or_path\"] + \" \"\n    command += \"--data_dir \" + config[\"data_dir\"] + \" \"\n    command += \"--num_train_epochs \" + str(config[\"num_train_epochs\"]) + \" \"\n    command += \"--per_device_train_batch_size \" + str(config[\"per_device_train_batch_size\"]) + \" \"\n    command += \"--learning_rate \" + str(config[\"learning_rate\"]) + \" \"\n    command += \"--max_seq_length \" + str(config[\"max_seq_length\"]) + \" \"\n\n\n    if \"do_train\" in config:\n        command += \"--do_train \"\n    if \"do_eval\" in config:\n        command += \"--do_eval \"\n    if \"do_predict\" in config:\n        command += \"--do_predict \"\n\n    command += \"--seed \" + str(config[\"seed\"]) + \" \"\n    if \"umls\" in config:\n        command += \"--umls \"\n        command += \"--med_document \" + str(config[\"med_document\"]) + \" \"\n\n    command += \"--labels \" + config[\"labels\"]\n    command += \" --save_steps 50000\"\n\n    return command","execution_count":146,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir ./results","execution_count":175,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {\n    \"run_file\"                    :     \"../input/umlsbert/run_ner.py\",\n    \"labels\"                      :     \"./train_data/label.txt\",\n    \"output_dir\"                  :     \"./results\",\n    \"model_name_or_path\"          :     \"./umlsbert\",\n    \"data_dir\"                    :     \"./train_data\",\n    \"num_train_epochs\"            :     5,\n    \"per_device_train_batch_size\" :     32,\n    \"learning_rate\"               :     1e-4,\n    \"max_seq_length\"              :     80,\n    \"seed\"                        :     42,\n    \"do_train\"                    :     True,\n    \"do_eval\"                     :     True,\n    \"umls\"                        :     True,\n    \"med_document\"                :     \"../input/umlsbert/vocab_updated.txt\",\n    \"do_predict\"                  :     True\n    }\n\n# Run Downstream tasks with given config\ncommand = generate_command(config)\nsubprocess.run(command, shell=True)","execution_count":176,"outputs":[{"output_type":"execute_result","execution_count":176,"data":{"text/plain":"CompletedProcess(args='python3 ../input/umlsbert/run_ner.py --output_dir ./results --model_name_or_path ./umlsbert --data_dir ./train_data --num_train_epochs 5 --per_device_train_batch_size 32 --learning_rate 0.0001 --max_seq_length 80 --do_train --do_eval --do_predict --seed 42 --umls --med_document ../input/umlsbert/vocab_updated.txt --labels ./train_data/label.txt --save_steps 50000', returncode=1)"},"metadata":{}}]},{"metadata":{"_uuid":"7da4b1f3-2051-4373-ad91-c3b49c99f3c4","_cell_guid":"681d2852-6586-4704-bd38-6d6860597fec","trusted":true},"cell_type":"code","source":"!python3 ../input/umlsbert/run_ner.py --output_dir ./results --model_name_or_path ./umlsbert --data_dir ./train_data --num_train_epochs 5 --per_device_train_batch_size 32 --learning_rate 0.0001 --max_seq_length 80 --do_train --do_eval --do_predict --seed 42 --umls --med_document ../input/umlsbert/vocab_updated.txt --labels ./train_data/label.txt --save_steps 50000","execution_count":179,"outputs":[]},{"metadata":{"_uuid":"1542f0d2-c89d-4a37-8b75-da9d4882caa3","_cell_guid":"4d2fdf87-9b60-467b-9c87-0dbb1e4ef008","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b516d3f8-8c19-41b9-bf41-448961f0988d","_cell_guid":"f47ce784-9356-435d-a45c-095427970978","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"649c9e79-a202-41d2-9eda-51e03bec7c3a","_cell_guid":"c3a89054-3463-4963-87ff-90255adec176","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bb781a3-5c6a-420b-bc84-626be0399724","_cell_guid":"e62bcd75-2069-4ae9-b314-f4cd31c22f35","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom keras.preprocessing import text, sequence\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import backend as K\n\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, Dense, LSTM, TimeDistributed, Lambda, SpatialDropout1D\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n\nfrom tensorflow.keras.models import Model\nfrom keras.optimizers import SGD, Adam, RMSprop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8a0961b-5620-4942-8098-7c41e650dc63","_cell_guid":"6c175b59-68ba-40b0-b6fe-1ca1032fd6eb","trusted":true},"cell_type":"code","source":"\ndef encode_pad_data():\n    text_tokenizer = text.Tokenizer(num_tokens+1, lower=True)\n    label_tokenizer = text.Tokenizer(4)\n    \n    label_tokenizer.fit_on_texts(train_detection_labels)\n    \n    text_tokenizer.fit_on_texts(train_sentences)\n    \"\"\"Train the tokenizer on the test and valdiation sequences, \n    otherwise, not all tokens will be tokized and will cause clashes\"\"\"\n    text_tokenizer.fit_on_texts(test_sentences)\n    text_tokenizer.fit_on_texts(dev_sentences)\n    \n    encoded_train_sequences = text_tokenizer.texts_to_sequences(train_sentences)\n    encoded_train_labels = label_tokenizer.texts_to_sequences(train_detection_labels)     \n    encoded_dev_sequences = text_tokenizer.texts_to_sequences(dev_sentences)\n    encoded_dev_labels = label_tokenizer.texts_to_sequences(dev_detection_labels)\n        \n    train_sentences_ = sequence.pad_sequences(encoded_train_sequences, dtype='int32', maxlen=maxlen, padding='post')\n    train_labels = sequence.pad_sequences(encoded_train_labels, maxlen=maxlen, dtype='int32', padding='post')\n    dev_sentences_ = sequence.pad_sequences(encoded_dev_sequences, dtype='int32', maxlen=maxlen, padding='post') \n    dev_labels = sequence.pad_sequences(encoded_dev_labels, maxlen=maxlen, dtype='int32', padding='post')\n    \n    return (train_sentences_, \n            train_labels, \n            dev_sentences_, \n            dev_labels,\n            text_tokenizer ,\n            label_tokenizer)\n\n(train_sentences, \n train_labels, \n dev_sentences, \n dev_labels,\n text_tokenizer,\n label_tokenizer) = encode_pad_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f81c280-808d-4157-bd8a-31569e3f6c54","_cell_guid":"205e3320-05fa-4490-b374-3c6bed435b40","trusted":true},"cell_type":"code","source":"output_dim = 50\ntf.random.set_seed(42)\nopt = Adam(0.005)\n\nsequence_input = Input(shape=(maxlen,), dtype=tf.int32, name='sequence_input')\noutputs = Embedding(input_dim=num_tokens+1, output_dim=output_dim, trainable=True, mask_zero=True)(sequence_input)\noutputs = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat')(outputs)\noutputs = LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(outputs)\noutputs = (TimeDistributed(Dense(64, activation=\"relu\")))(outputs)\n\noutputs = Dense(4, activation=\"softmax\")(outputs)\n\nlstm_model = Model(inputs=sequence_input, outputs=outputs)\nlstm_model.compile(loss = 'SparseCategoricalCrossentropy', optimizer=opt)\nlstm_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c02c8402-ab92-4b96-a236-d9e240cff734","_cell_guid":"dd5a1e50-0210-41f5-8f68-3d4b535a5165","trusted":true},"cell_type":"code","source":"import warnings\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\nwarnings.filterwarnings('ignore')\n\"\"\"This ignored warning because precision and recall give warnings\nthat not all the true labels are represented in the predictions\"\"\"\n\n\ndef exclude_from_f1(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    f1 = f1_score(ytrue, yhat, average='weighted')\n    return f1\n\ndef exclude_from_precision(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    precision = precision_score(ytrue, yhat, average='weighted')\n    return precision\n\ndef exclude_from_recall(y_true, y_pred, excluded_tags=[0]):\n    ytrue, yhat = [], []\n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t not in excluded_tags:\n            ytrue.append(y_t)\n            yhat.append(y_p)\n    recall = recall_score(ytrue, yhat, average='weighted')\n    return recall","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87261ae7-e4e5-48c2-9913-0954f562a8c0","_cell_guid":"02d70554-2e9e-4fec-9369-0dba8988062c","trusted":true},"cell_type":"code","source":"train_f1_epochs, train_precision_epochs, train_recall_epochs = [], [], []\nval_f1_epochs, val_precision_epochs, val_recall_epochs = [], [], []\ntrain_f1_no_other_epochs, train_precision_no_other_epochs, train_recall_no_other_epochs = [], [], []\nval_f1_no_other_epochs, val_precision_no_other_epochs, val_recall_no_other_epochs = [], [], []\n\nfor epoch in range(1, 11):\n    print('epoch ', epoch)\n    for x, y in zip(train_sentences, train_labels):\n        #weights = get_weights(y)\n        lstm_model.train_on_batch(x, y)#, class_weight=weights)\n    \n    #train_f1, train_precision, train_recall = [], [], []\n    #train_f1_no_other, train_precision_no_other, train_recall_no_other = [], [], []\n    #for x, y in zip(train_umls_text, train_labels):\n    #    y_pred = np.argmax(lstm_model.predict(x), axis=-1)\n    #    train_f1.append(exclude_from_f1(y, y_pred, [0]))\n    #    train_precision.append(exclude_from_precision(y, y_pred, [0]))\n    #    train_recall.append(exclude_from_recall(y, y_pred, [0]))\n    #    train_f1_no_other.append(exclude_from_f1(y, y_pred, [0, 1]))\n    #    train_precision_no_other.append(exclude_from_precision(y, y_pred, [0, 1]))\n    #    train_recall_no_other.append(exclude_from_recall(y, y_pred, [0, 1]))\n        \n    #train_f1_epochs.append(np.mean(train_f1))\n    #train_precision_epochs.append(np.mean(train_precision))\n    #train_recall_epochs.append(np.mean(train_recall))\n    #train_f1_no_other_epochs.append(np.mean(train_f1_no_other))\n    #train_precision_no_other_epochs.append(np.mean(train_precision_no_other))\n    #train_recall_no_other_epochs.append(np.mean(train_recall_no_other))\n    \n    val_f1_no_other, val_precision_no_other, val_recall_no_other = [], [], []\n    val_f1, val_precision, val_recall = [], [], []\n    for x, y in zip(dev_sentences[:100], dev_labels[:100]):\n        y_pred = np.argmax(lstm_model.predict(x), axis=-1)\n        val_f1.append(exclude_from_f1(y, y_pred, [0]))\n        val_precision.append(exclude_from_precision(y, y_pred, [0]))\n        val_recall.append(exclude_from_recall(y, y_pred, [0]))\n        #val_f1_no_other.append(exclude_from_f1(y, y_pred, [0, 1]))\n        #val_precision_no_other.append(exclude_from_precision(y, y_pred, [0, 1]))\n        #val_recall_no_other.append(exclude_from_recall(y, y_pred, [0, 1]))\n    \n    val_f1_epochs.append(np.mean(val_f1))\n    val_precision_epochs.append(np.mean(val_precision))\n    val_recall_epochs.append(np.mean(val_recall))\n    #val_f1_no_other_epochs.append(np.mean(val_f1_no_other))\n    #val_precision_no_other_epochs.append(np.mean(val_precision_no_other))\n    #val_recall_no_other_epochs.append(np.mean(val_recall_no_other))\n    \n    print(np.mean(val_f1), np.mean(val_precision), np.mean(val_recall))\n    #print(np.mean(val_f1_no_other), np.mean(val_precision_no_other), np.mean(val_recall_no_other))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}