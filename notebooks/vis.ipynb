{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7YyLBXio-kM",
        "outputId": "a341d592-e40b-4829-ba84-b0ac6cb1114c"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b32knCS-5afG"
      },
      "source": [
        "### Number of articles in this file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0mzxNx1p5Km",
        "outputId": "75b69735-23f5-4077-b477-8328f600b1b9"
      },
      "source": [
        "import json\n",
        "\n",
        "# Opening a file \n",
        "file = open(\"/content/drive/My Drive/thesis/pdf_parses_0_filtered.jsonl\",\"r\")\n",
        "Counter = 0\n",
        "\n",
        "# Reading from file \n",
        "Content = file.read()\n",
        "CoList = Content.split(\"\\n\")\n",
        "\n",
        "for i in CoList:\n",
        "    if i:\n",
        "        Counter += 1\n",
        "\n",
        "print(Counter)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDdgCfjkqZwk",
        "outputId": "af4895da-c8c7-4d41-b0b5-0f48123c2f16"
      },
      "source": [
        "len(CoList)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01XFzWpo5oui"
      },
      "source": [
        "### json structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCaKZnr9rqcD",
        "outputId": "9a92bc57-9f1e-4ee7-8f8e-5a1147f61841"
      },
      "source": [
        "json.loads(CoList[0]).keys()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['paper_id', '_pdf_hash', 'abstract', 'body_text', 'bib_entries', 'ref_entries'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "qpMnl1WmsIHW",
        "outputId": "91d4e5ba-3dcd-477d-9512-550e238d43b5"
      },
      "source": [
        "json.loads(CoList[0])['abstract'][0]['text']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This technical note studies Markov decision processes under parameter uncertainty. We adapt the distributionally robust optimization framework, assume that the uncertain parameters are random variables following an unknown distribution, and seek the strategy which maximizes the expected performance under the most adversarial distribution. In particular, we generalize a previous study [1] which concentrates on distribution sets with very special structure to a considerably more generic class of distribution sets, and show that the optimal strategy can be obtained efficiently under mild technical conditions. This significantly extends the applicability of distributionally robust MDPs by incorporating probabilistic information of uncertainty in a more flexible way.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkU0ysHi5yyp"
      },
      "source": [
        "### body text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEVq0Z5832p7",
        "outputId": "bbab7a18-fd33-4d03-a35c-43475a7ce4c8"
      },
      "source": [
        "json.loads(CoList[0])['body_text']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': '',\n",
              "  'text': '. Illustration of the confidence sets.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': '',\n",
              "  'text': 'optimizing variable and ξ is the unknown parameter, distributionally robust optimization solves max x∈X [inf μ∈C E ξ∼μ u(x, ξ)], where C is an a priori known set of distributions.'},\n",
              " {'cite_spans': [{'end': 52, 'ref_id': 'BIBREF0', 'start': 49, 'text': '[1]'},\n",
              "   {'end': 61, 'ref_id': 'BIBREF0', 'start': 58, 'text': '[1]'},\n",
              "   {'end': 849, 'ref_id': 'BIBREF17', 'start': 845, 'text': '[18]'}],\n",
              "  'ref_spans': [{'end': 346,\n",
              "    'ref_id': 'FIGREF2',\n",
              "    'start': 337,\n",
              "    'text': 'Fig. 1(a)'},\n",
              "   {'end': 1175, 'ref_id': 'FIGREF2', 'start': 1166, 'text': 'Fig. 1(b)'}],\n",
              "  'section': '',\n",
              "  'text': 'We highlight our contributions by comparing with [1] . In [1] the state-wise ambiguity set is restricted to the following form:C s = {μ s |μ s (O i s ) ≥ α i s ∀ i = 1, . . . , n s }, where α i s ≤ α j s and O i s is a proper set of uncertain parameters with a \"nested-set\" structure, i.e., satisfying O i s ⊆ O j s , for all i < j [see Fig. 1(a) ]. This setup can effectively model distributions with a single mode (such as a Gaussian distribution), but less so when modeling multi-mode distributions such as a mixture Gaussian distribution. Moreover, other probabilistic information such as mean, variance etc. cannot be incorporated. Thus, in this technical note, we extend the distributionally robust MDP approach to handle ambiguity sets with more general structures. In particular, we consider a class of ambiguity sets, first proposed in [18] as a unifying framework for modeling and solving distributionally robust single-stage optimization problems, and embed them into the distributionally robust MDPs setup. These ambiguity sets are considerably more general: they are characterized by a class of O i s which can either be nested or disjoint [as shown in Fig. 1(b) ], and moreover, additional linear constraints are allowed to define the ambiguity set, which can be used to incorporate probabilistic information such as mean, covariance or other variation measures. We show that, under this more general class of ambiguity sets, the resulting distributionally robust MDPs remain tractable under mild technical conditions, and often outperform previous methods thanks to the fact that it can model uncertainty in a more flexible way.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'Throughout the technical note, we use capital letters to denote matrices, and bold face letters to denote column vectors. We use e i (m) to denote the ith elementary vector of length m, and use R n + to denote the nonnegative orthant of R n . If C is the set of joint probability distributions of three random vectors a, b, and c, then (a,b) C denotes the set of marginal distributions of (a, b). We use ⊕ to represent mixture distribution: given two probability distributions F 1 , F 2 and a Bernoulli random variable x which takes value 1 w.p. p, xF 1 ⊕ (1 − x)F 2 is a random variable such that it follows distribution F 1 w.p. p, and follows F 2 w.p. 1 − p. We use N (m, σ 2 ) to represent a Gaussian distribution with mean m and variance σ 2 .'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'A (finite) Markov Decision Process (MDP) is defined as a 6-tuple T, γ, S, A, p, r . Here, T is the (possibly infinite) decision horizon; 0018-9286 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.'},\n",
              " {'cite_spans': [{'end': 353,\n",
              "    'ref_id': 'BIBREF1',\n",
              "    'start': 350,\n",
              "    'text': '[2]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'γ ∈ (0, 1] is the discount factor; S is the state set and A s is the action set of state s ∈ S, both assumed to be finite. The parameter p and r are the transition probability and the expected reward, respectively. That is, for s ∈ S and a ∈ A s , r(s, a) is the expected reward and p(s |s, a) is the probability that the next state is s . Following [2] , we denote the set of all history-dependent randomized strategies by Π HR . We use subscript s to denote the value associated with the state s: e.g., r s denotes the vector form of the rewards associated with the state s, and π s is the (randomized) action chosen at state s for strategy π.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'The elements in the vector p s are listed in the following way: the transition probabilities of the same action are arranged in the same block, and inside each block they are listed according to the order of the next state. We use s to denote the (random) state following s, and Δ(s) to denote the probability simplex on A s . We use to represent Cartesian product, e.g., p = s∈S p s . For a given strategy π ∈ Π HR , we denote the expected (discounted) total-reward under parameters pair (p, r) as u(π, p, r)'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'A Distributionally Ambiguous MDP (DAMDP) is defined as a tuple T, γ, S, A,C S , where the transition probability p and the expected reward r are unknown. Instead, they are assumed to obey a joint distribution μ 0 (also unknown) that belongs to a known ambiguity set'},\n",
              " {'cite_spans': [{'end': 125,\n",
              "    'ref_id': 'BIBREF0',\n",
              "    'start': 122,\n",
              "    'text': '[1]'},\n",
              "   {'end': 132, 'ref_id': 'BIBREF18', 'start': 128, 'text': '[19]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'While the DAMDP framework can be very general, mostC S result in formulations that are computationally intractable (e.g., [1] , [19] ). Hence, we make the following requirement ofC S such that the parameters among different states are independent.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'Assumption 1: The ambiguity setC S has the following property:'},\n",
              " {'cite_spans': [{'end': 232,\n",
              "    'ref_id': 'BIBREF15',\n",
              "    'start': 228,\n",
              "    'text': '[16]'},\n",
              "   {'end': 314, 'ref_id': 'BIBREF19', 'start': 310, 'text': '[20]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'where \"state-wise ambiguity set\"C s is a set of distributions of parameters of state s. By the definition ofC S , the state-wise property applies to C S as well. This property is the same as the concept of \"s-rectangularity\" in [16] , and is essential for reducing DAMDP to robust MDP in Lemma 1. In addition, [20] showed that the robust MDP with coupled uncertainty sets is computationally challenging, which implies solving DAMDP with nonrectangular ambiguity sets is even harder.'},\n",
              " {'cite_spans': [{'end': 142,\n",
              "    'ref_id': 'BIBREF17',\n",
              "    'start': 138,\n",
              "    'text': '[18]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'We now discuss the admissible state-wise ambiguity set. Our formulation of the state-wise ambiguity set follows the unifying framework of [18] . In specific, given s ∈ S, the state-wise ambiguity set is representable with the following standard form:'},\n",
              " {'cite_spans': [{'end': 472,\n",
              "    'ref_id': 'BIBREF0',\n",
              "    'start': 469,\n",
              "    'text': '[1]'},\n",
              "   {'end': 479, 'ref_id': 'BIBREF18', 'start': 475, 'text': '[19]'},\n",
              "   {'end': 486, 'ref_id': 'BIBREF21', 'start': 482, 'text': '[21]'},\n",
              "   {'end': 827, 'ref_id': 'BIBREF22', 'start': 823, 'text': '[22,'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'are the lower and upper bounds of the probability that parameters belong to a confidence set. Thus, each confidence set O i s provides an estimation of the uncertain parameters pair (p s , r s ,ũ s ) subject to a different confidence level. Ambiguity setsC s contain prescribed conic representable confidence sets and mean values residing on an affine manifold, which is rich enough to encompass and extend several ambiguity sets considered in recent literature (e.g., [1] , [19] , [21] ). The set of joint distribution of (p s , r s ) is hence C s Δ = (ps ,rs)C s . Notice that a classical technique called \"lifting\" is used here: We introduce an auxiliary random vectorũ, so that some non-linear relationship can be modeled linearly. For example, a constraint on the variance can be modeled using this standard form (see [22, Example 2] ), which is otherwise impossible without the auxiliary variable. This lifting technique thus allows us to model a rich variety of structural information about the marginal distribution of (p, r) in a unified manner. Note when the ambiguity set only contains the support of random variables, i.e.,'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'where the a-priori information of unknown parameters is that they belong to an uncertainty set.'},\n",
              " {'cite_spans': [{'end': 86,\n",
              "    'ref_id': 'BIBREF17',\n",
              "    'start': 82,\n",
              "    'text': '[18]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'Assumptions 2 to 4 are standard requirements for the confidence sets, proposed in [18] . The first one asserts the relationship between different confidence sets.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [{'end': 49,\n",
              "    'ref_id': 'FIGREF2',\n",
              "    'start': 40,\n",
              "    'text': 'Fig. 1(b)'}],\n",
              "  'section': 'II. PRELIMINARIES',\n",
              "  'text': 'The nesting condition is illustrated in Fig. 1(b) . Next, for any s ∈ S we require thatC s satisfies the following regularity condition.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'Assumption 3 (Regularity Conditions forC s ):',\n",
              "  'text': '1) The confidence set O ns s is bounded and has probability one, that is, '},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'Assumption 3 (Regularity Conditions forC s ):',\n",
              "  'text': 's are proper cones (i.e., a closed, convex and pointed cone with nonempty interior).'},\n",
              " {'cite_spans': [{'end': 319,\n",
              "    'ref_id': 'BIBREF0',\n",
              "    'start': 316,\n",
              "    'text': '[1]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'This section focuses on DAMDP with a finite number of decision stages. We show that a strategy defined through backward induction, which we call S-robust strategy, is distributionally robust. We further show such a strategy is solvable in polynomial time under mild technical conditions. This generalizes results in [1] to a significantly more general class of ambiguity sets.'},\n",
              " {'cite_spans': [{'end': 15,\n",
              "    'ref_id': 'BIBREF9',\n",
              "    'start': 11,\n",
              "    'text': '[10]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Similar to [10] , we assume that when a state is visited multiple times, each time it can take a different parameter realization (nonstationary model). This assumption is justified mainly because the stationary model is generally intractable and a lower-bound of it is given by the non-stationary model. Therefore, multiple visits to a state can be treated as visiting different states. By introducing dummy states as in [1, Assumption 2.2], for finite horizon DAMDP we make the following assumption without loss of generality. This will simplify our exposition.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Assumption 5: 1) Each state belongs to only one stage.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': '2) The terminal reward equals zero.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': '3) The first stage only contains one state s ini .'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Using the condition 1 of Assumption 5, we partition S according to the stage each state belongs to. That is, we let S t be the set of states belong to tth stage.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'For π ∈ Π HR and μ ∈ C S , we denote the expected performance of a DAMDP as w π, μ, (s ini ) Δ = E (p,r)∼μ {u(π, p, r)} = u(π, p, r)dμ(p, r).'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'In words, each strategy is evaluated by its expected performance under the (respective) most adversarial distribution of the uncertain parameters, and a distributionally robust strategy is the optimal strategy according to this metric. The main focus of this section is deriving approaches to solve the distributionally robust strategy. To this end, we need the following definition.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Definition 2: Given a DAMDP T, γ, S, A,C S , we define the Srobust strategy as follows'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': '3) A strategyπ * is a Srobust strategy if ∀ s ∈ S, and every history h that ends at s, we haveπ * s , conditioned on history h, is a S-robust action.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'The definition requires that the strategy must be robust w.r.t. each sub-problem, and hence the name \"S-robust.\" The following theorem shows any S-robust strategy π * is distributionally robust, and is the main result of this technical note.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Theorem 1: Let T < ∞. Under Assumptions 1, 2, 4, and 5, if π * is a S-robust strategy, then 1) π * is a distributionally robust strategy with respect to C S . 2) There exists μ * ∈ C s such that (π * , μ * ) is a saddle point. That is'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Proof: We first state a Lemma from [1, Lemma 3.2] without proof.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Lemma 1: Under Assumption 1, fix π ∈ Π HR and μ ∈ C S , denote p = E μ (p) and r = E μ (r). We have w(π, μ, (s ini )) = u(π, p, r).'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Lemma 1 means for any strategy, the expected performance under an admissible distribution μ only depends on the expected value of parameters under μ. Thus, the distributionally robust MDPs reduce to robust MDPs. Next we characterize the set of expected value of the parameters.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Lemma 2: For s ∈ S and π s ∈ Δ(s), we define the set Z s = {E μs (p s , r s )|μ s ∈ C s }. Then set Z s is convex and compact.'},\n",
              " {'cite_spans': [{'end': 222,\n",
              "    'ref_id': 'BIBREF22',\n",
              "    'start': 218,\n",
              "    'text': '[22]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Proof: First, we show that, for s ∈ S and π s ∈ Δ(s), the set defined asZ s = {E μs (p s , r s ,ũ s )|μ s ∈C s } is convex and compact. The convexity can be easily shown, which is omitted due to space constraints (see [22] for details). To show the compactness, notice thatC s is weakly closed (i.e., closed w.r.t. to the weak topology) since the feasible set of each of constraint is weakly closed which implies their intersection is also weakly closed. Thus,Z s is closed since it is the image ofC s under expectation (which is a continuous function). This impliesZ s is compact since O ns s is bounded and henceZ s is bounded. Finally, since Z s is the projection onto the first two coordinates of set Z s , its convexity and compactness thus follow.'},\n",
              " {'cite_spans': [{'end': 239,\n",
              "    'ref_id': 'BIBREF9',\n",
              "    'start': 235,\n",
              "    'text': '[10]'},\n",
              "   {'end': 246, 'ref_id': 'BIBREF10', 'start': 242, 'text': '[11]'},\n",
              "   {'end': 421, 'ref_id': 'BIBREF22', 'start': 417, 'text': '[22]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Lemma 2 implies that, for s ∈ S and π s ∈ Δ(s), there exists (p * s , r * s ) ∈ Z s that satisfies inf (ps,rs)∈Zs u(π s , p s , r s ) = u(π s , p * s , r * s ). Since saddle point of the minimax objective exists for robust MDPs (e.g., [10] , [11] ), we can complete the proof of part 2) following a similar procedure as the last portion of proof for [1, Theorem 3.1]. We omit the details due to space constraint (see [22] for details). Part 1) then follows part 2) immediately.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'We now investigate the computational aspect of finding the S-robust action.'},\n",
              " {'cite_spans': [{'end': 234,\n",
              "    'ref_id': 'BIBREF17',\n",
              "    'start': 230,\n",
              "    'text': '[18]'},\n",
              "   {'end': 274, 'ref_id': 'BIBREF23', 'start': 270, 'text': '[23]'},\n",
              "   {'end': 320, 'ref_id': 'BIBREF22', 'start': 316, 'text': '[22]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Theorem 2: Under Assumption 2, 3, 4, and 5, for s ∈ S t where t < T , the S-robust action is the optimal solution of the following optimization problem (termed Srobust problem hereafter): Proof: The proof essentially follows from [18] and duality of convex optimization [23] , and can be found in the longer version [22] of this technical note.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'Thus, since for s ∈ S t , Δ(s) is compact, we can solve the S-robust action in polynomial time if all K i s are \"easy\" cones such as linear, conic quadratic or semidefinite cones. Moreover, using Theorem 1, by backward induction, we can obtain the S-robust strategy efficiently.'},\n",
              " {'cite_spans': [{'end': 146,\n",
              "    'ref_id': 'BIBREF0',\n",
              "    'start': 143,\n",
              "    'text': '(1)'},\n",
              "   {'end': 214, 'ref_id': 'BIBREF22', 'start': 210, 'text': '[22]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'By virtue of the lifting technique [18, Theorem 5], we show below several widely used ambiguity sets are indeed special cases ofC s defined in (1) . We further derive their corresponding S-robust problems. See [22] for additional examples (variance and expected Huber loss function). '},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'This example can also be treated via \"classical\" robust optimization by virtue of Lemma 1.'},\n",
              " {'cite_spans': [{'end': 263,\n",
              "    'ref_id': 'BIBREF0',\n",
              "    'start': 260,\n",
              "    'text': '[1]'},\n",
              "   {'end': 288, 'ref_id': 'BIBREF22', 'start': 284, 'text': '[22]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'III. FINITE HORIZON DISTRIBUTIONALLY ROBUST MDPS',\n",
              "  'text': 'The finite horizon DAMDP can be easily extended to discountedreward infinite horizon setup. We can generalize the notion of S-robust strategy, which turns to be distributionally robust in both stationary and non-stationary models. This extension is similar to [1] and can be found in [22] .'},\n",
              " {'cite_spans': [{'end': 447,\n",
              "    'ref_id': 'BIBREF24',\n",
              "    'start': 443,\n",
              "    'text': '[24]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'IV. SIMULATION',\n",
              "  'text': 'In this section, we study two synthetic numerical examples: a machine replacement problem and a path planning problem. In the machine replacement problem, the reward parameters are uncertain; whereas in the path planning problem, the transition probabilities are uncertain. All results were generated on desktop with Intel Core i5-3570 CPU of 3.40 GHz clock speed and 8 GB RAM. The S-robust problems are solved in Matlab using the CVX package [24] .'},\n",
              " {'cite_spans': [{'end': 68,\n",
              "    'ref_id': 'BIBREF11',\n",
              "    'start': 64,\n",
              "    'text': '[12]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'A. Reward Uncertainty in the Machine Replacement Problem',\n",
              "  'text': 'We consider a machine replacement problem similar to the one in [12] . Consider the repair cost incurred by a factory that holds a large number of machines, given that each of these machines is modeled with a same underlying MDP for which rewards are subject to uncertainty.'},\n",
              " {'cite_spans': [{'end': 903,\n",
              "    'ref_id': 'BIBREF22',\n",
              "    'start': 899,\n",
              "    'text': '[22]'}],\n",
              "  'ref_spans': [{'end': 254,\n",
              "    'ref_id': 'FIGREF3',\n",
              "    'start': 245,\n",
              "    'text': 'Fig. 2(a)'},\n",
              "   {'end': 324, 'ref_id': 'FIGREF1', 'start': 309, 'text': 'cost N (130, 1)'},\n",
              "   {'end': 569, 'ref_id': 'FIGREF1', 'start': 553, 'text': 'cost N (130, 10)'},\n",
              "   {'end': 1210, 'ref_id': 'FIGREF3', 'start': 1201, 'text': 'Fig. 2(a)'}],\n",
              "  'section': '1) Machine Replacement as a MDP With Gaussian Rewards:',\n",
              "  'text': 'We first consider a machine replacement problem with 50 states, 2 actions (\"repair\" and \"not repair\") for each state, deterministic transitions, a discount factor of 0.8, and uncertain rewards following Gaussian distributions independently [see Fig. 2(a) ]: For the first 48 states, the \"repair\" action has a cost N (130, 1) . The 49th and 50th states of the machine\\'s life are designed to be risky: not repairing at state 50 incurs a highly uncertain cost N (100, 800), while repairing at both states is a more secure but still uncertain option with a cost N (130, 10) . The detailed implementation is as follows: We use the mean value of uncertain rewards to compute the nominal strategy. For both robust and distributionally robust strategy, we construct confidence sets usinĝ m ± 3σ for the first 49 states, andm ± 4σ for state 50 wherem andσ 2 are mean and variance estimated from samples (see [22] for details), as it is more risky and thus hard to estimate. In addition, we construct an extra confidence set (centered at the mean) with 60%-70% confidence level (i.e., α 1 50 = 0.6, α 1 50 = 0.7) for distributionally robust strategy. The optimal paths followed by three strategies are shown in Fig. 2(a) .'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [{'end': 141,\n",
              "    'ref_id': 'FIGREF1',\n",
              "    'start': 135,\n",
              "    'text': 'Fig. 3'},\n",
              "   {'end': 239, 'ref_id': 'TABREF0', 'start': 232, 'text': 'Table I'},\n",
              "   {'end': 414, 'ref_id': 'FIGREF3', 'start': 408, 'text': 'Fig. 2'},\n",
              "   {'end': 474, 'ref_id': 'FIGREF3', 'start': 465, 'text': 'Fig. 2(a)'},\n",
              "   {'end': 533, 'ref_id': 'FIGREF3', 'start': 524, 'text': 'Fig. 2(b)'},\n",
              "   {'end': 1406, 'ref_id': 'FIGREF0', 'start': 1400, 'text': 'Fig. 4'}],\n",
              "  'section': '1) Machine Replacement as a MDP With Gaussian Rewards:',\n",
              "  'text': 'The performance of the strategies obtained by using the nominal, the robust and the distributionally robust approaches is presented in Fig. 3 . The corresponding average total discounted rewards and computational times are shown in Table I . The nominal strategy results in the highest average total discounted rewards. This is well expected as we are using the exact mean value of the reward as the nominal Fig. 2 . Two instances of a machine replacement problem. Fig. 2(a) shows Gaussian uncertainty in the rewards, while Fig. 2(b) shows mixed Gaussian uncertainty in the rewards. parameter. However, the nominal strategy is highly risky: it cannot prevent bad performance (e.g., −0.025) from happening, which is undesirable. While the nominal strategy, blind to any form of risk, finds no advantage in ever repairing, the robust strategy ends up following a highly conservative policy (repairing the machine at state 49 to avoid state 50). In contrast, the distributionally robust optimal strategy makes use of more distributional information and handles the risk efficiently by waiting until state 50 and then repair the machine. Therefore, this strategy beats the nominal and robust strategies in that it strikes a good tradeoff between high mean reward and low variance over 10,000 different trials. These results coincide with what one would typically expect from the three solution concepts. Fig. 4 . Illustration of the confidence sets for two distributionally robust strategies.'},\n",
              " {'cite_spans': [{'end': 373,\n",
              "    'ref_id': 'BIBREF0',\n",
              "    'start': 370,\n",
              "    'text': '[1]'},\n",
              "   {'end': 755, 'ref_id': 'BIBREF22', 'start': 751, 'text': '[22]'},\n",
              "   {'end': 832, 'ref_id': 'BIBREF0', 'start': 829, 'text': '[1]'}],\n",
              "  'ref_spans': [{'end': 178,\n",
              "    'ref_id': 'FIGREF3',\n",
              "    'start': 169,\n",
              "    'text': 'Fig. 2(b)'},\n",
              "   {'end': 279, 'ref_id': 'FIGREF2', 'start': 273, 'text': 'Fig. 1'},\n",
              "   {'end': 918, 'ref_id': 'FIGREF0', 'start': 909, 'text': 'Fig. 4(a)'},\n",
              "   {'end': 1164, 'ref_id': 'FIGREF0', 'start': 1158, 'text': 'Fig. 4'},\n",
              "   {'end': 1466, 'ref_id': 'FIGREF3', 'start': 1457, 'text': 'Fig. 2(b)'}],\n",
              "  'section': '2) Machine Replacement as a MDP With Mixed Gaussian Rewards:',\n",
              "  'text': 'The second experiment has a similar setup as the previous one, except that not repairing at the 50th state has a reward which follows a mixed Gaussian distribution [see Fig. 2(b) ]. This experiment illustrates the effect of the two different nested-set structures shown in Fig. 1 . In specific, we apply the two different distributionally robust approaches (proposed in [1] and this technical note respectively), and show that our method outperforms. The detailed implementation is as follows: For the robust and two distributionally robust strategies, we construct uncertainty set corresponding to 99% probability support of the rewards for the first 49 states, and 99.9% for the 50th state that is more risky, using estimated mean and variance (see [22] for details). For the first distributionally robust strategy proposed in [1] , we construct two additional nested confidence sets O 1 50 and O 2 50 [see Fig. 4(a) ], which w.p. 40%-50% and 60%-70% respectively the uncertain rewards belong to. In contrast, for the second distributionally robust strategy proposed in this technical note, we construct two disjoint confidence sets O 1 50 and O 2 50 [see Fig. 4 (b)] with 70%-80% and 0%-10% confidence level, respectively. Specifically, we select these two intervals around the peaks of the two Gaussian elements [i.e., N (100, 10) and N (140, 2)] to better model this mixed distribution. The optimal paths followed for the three strategies are shown in Fig. 2(b) .'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [{'end': 71,\n",
              "    'ref_id': 'FIGREF5',\n",
              "    'start': 65,\n",
              "    'text': 'Fig. 5'},\n",
              "   {'end': 170, 'ref_id': 'TABREF0', 'start': 162, 'text': 'Table II'}],\n",
              "  'section': '2) Machine Replacement as a MDP With Mixed Gaussian Rewards:',\n",
              "  'text': 'The performance of the three strategies obtained is presented in Fig. 5 . The corresponding average total discounted rewards and computational times are shown in Table II . As expected, the robust strategy ends up following a highly conservative policy repairing the machine at state 49 to avoid state 50. The first distributionally robust strategy, not modeling the mixture Gaussian distribution well, finds it advantageous to repair at the 50th state. In contrast, capable of capturing the distribution information in a more flexible way, the second distributionally robust strategy better models the uncertainty and finds not repairing the machine at state 50 is optimal. The performance comparison clearly shows the second distributionally robust strategy is more desirable, which highlights the distributionally robust approach with general structure of confidence sets can be beneficial in practice.'},\n",
              " {'cite_spans': [{'end': 546,\n",
              "    'ref_id': 'BIBREF25',\n",
              "    'start': 542,\n",
              "    'text': '[25]'},\n",
              "   {'end': 555, 'ref_id': 'BIBREF26', 'start': 551, 'text': '[26]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': '2) Machine Replacement as a MDP With Mixed Gaussian Rewards:',\n",
              "  'text': 'We remark that, in practice, one can obtain the modality structure of uncertain parameters in a data-driven way by applying clustering algorithms to an initial primitive data set. For example, one may check the histogram of historical observations. If the data concentrates on several distinct and disjoint bins, our multi-model DAMDP approach can be applied. Moreover, we note that networked control systems (NCSs) have recently emerged as a topic of significant interest in the control community. A typical application of NCSs is in modern [25] and [26] proposed a novel two-layer structure to solve the setpoints compensation problem for industrial processes under network-based environment.'},\n",
              " {'cite_spans': [{'end': 76, 'ref_id': 'BIBREF0', 'start': 73, 'text': '[1]'}],\n",
              "  'ref_spans': [{'end': 135,\n",
              "    'ref_id': 'FIGREF6',\n",
              "    'start': 126,\n",
              "    'text': 'Fig. 6(a)'}],\n",
              "  'section': 'B. Transition Uncertainty in the Path Planning Problem',\n",
              "  'text': 'We now consider a path planning problem, similar to the one presented in [1] : an agent wants to exit a 4 × 21 maze [shown in Fig. 6(a) ] using the least possible time. Starting from the upper-left corner, the agent can move up, down, left and right, but can only exit the grid at the lower-right corner. Here, a white box stands for a normal place where the agent needs one time unit to pass through. A shaded box represents a \"shaky\" place: if an agent reaches a \"shaky\" place, then he may risk jumping to the starting point (\"reboot\"). The true transition probability of the jump follows a distribution'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'B. Transition Uncertainty in the Path Planning Problem',\n",
              "  'text': 'The four approaches are implemented as follows: The nominal approach neglects this random jump. The robust approach takes a worst-case analysis, i.e., it assumes that with 30%, the whole probability support of transition, the agent will jump to the spot with the highest costto-go. The first distributionally robust approach takes into account an additional information by using two nested confidence sets: the jump probability parameter belonging to 9%-11% is of a confidence 1 − λ. The second distributionally robust approach, which is proposed in this technical note, incorporates more information. In specific, we construct an extra confidence interval disjoint with the above 9%-11% interval. It states that the chance of jumping with probability 20% is λ.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [{'end': 125,\n",
              "    'ref_id': 'FIGREF6',\n",
              "    'start': 116,\n",
              "    'text': 'Fig. 6(b)'},\n",
              "   {'end': 496, 'ref_id': 'FIGREF6', 'start': 487, 'text': 'Fig. 6(a)'},\n",
              "   {'end': 559, 'ref_id': 'FIGREF6', 'start': 550, 'text': 'Fig. 6(b)'}],\n",
              "  'section': 'B. Transition Uncertainty in the Path Planning Problem',\n",
              "  'text': 'The performance of strategies of the nominal, the robust and the two distributionally robust approaches is shown in Fig. 6(b) , where the error bars show the standard error of the expected time to exit. The CPU times of computing optimal policies for four strategies are 0.461, 549, 642, and 654 seconds, respectively. The second distributionally robust approach achieves the best performance over virtually the whole spectrum of λ. This is well expected, since additional probabilistic Fig. 6(a) illustrates the maze for the path plawnning problem. Fig. 6(b) shows the performance comparisons between nominal, robust and two distributionally robust strategies over 3,000 runs of the path planning problem.'},\n",
              " {'cite_spans': [],\n",
              "  'ref_spans': [],\n",
              "  'section': 'B. Transition Uncertainty in the Path Planning Problem',\n",
              "  'text': 'information is available to and incorporated by the second distributionally robust approach which considers ambiguity sets with more general structures.'},\n",
              " {'cite_spans': [{'end': 162,\n",
              "    'ref_id': 'BIBREF0',\n",
              "    'start': 159,\n",
              "    'text': '[1]'},\n",
              "   {'end': 222, 'ref_id': 'BIBREF17', 'start': 218, 'text': '[18]'}],\n",
              "  'ref_spans': [],\n",
              "  'section': 'V. CONCLUSION',\n",
              "  'text': 'In this technical note, we considered Markov decision problems with uncertainty. Specifically, we generalized the distributionally robust approach proposed in [1] to incorporate more general ambiguity sets proposed in [18] to model a-priori probabilistic information of the uncertain parameters. We proposed a way to compute the distributionally robust strategy through a Bellman type backward induction. We showed that the strategy, which achieves maximum expected utility under the worst admissible distributions of uncertain parameters, can be solved in polynomial time under some mild technical conditions. We believe that many important problems that are usually addressed using standard MDP models could be revisited and better resolved using the proposed models when parameter uncertainty exists, as this formulation naturally enables the decision maker to account for more general parameter uncertainty.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIfe77-152lw",
        "outputId": "dde811e3-7238-42b8-8acc-bc157818cd9c"
      },
      "source": [
        "len(json.loads(CoList[0])['body_text'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "MJ3Gm4Kp6STO",
        "outputId": "1d139cd8-35da-4374-adea-7882cd7f0849"
      },
      "source": [
        "json.loads(CoList[0])['body_text'][6]['text']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'γ ∈ (0, 1] is the discount factor; S is the state set and A s is the action set of state s ∈ S, both assumed to be finite. The parameter p and r are the transition probability and the expected reward, respectively. That is, for s ∈ S and a ∈ A s , r(s, a) is the expected reward and p(s |s, a) is the probability that the next state is s . Following [2] , we denote the set of all history-dependent randomized strategies by Π HR . We use subscript s to denote the value associated with the state s: e.g., r s denotes the vector form of the rewards associated with the state s, and π s is the (randomized) action chosen at state s for strategy π.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJU5ieA10dBG"
      },
      "source": [
        "### Convert to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BApzi8X0gB8"
      },
      "source": [
        "import pandas as pd\n",
        "data = []\n",
        "for line in open('/content/drive/My Drive/thesis/pdf_parses_0_filtered.jsonl', 'r'):\n",
        "    data.append(json.loads(line))\n",
        "df = pd.DataFrame(data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "fN9cQG0I0nki",
        "outputId": "86ed3031-a147-44c0-ed9f-c620588bf7bf"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>_pdf_hash</th>\n",
              "      <th>abstract</th>\n",
              "      <th>body_text</th>\n",
              "      <th>bib_entries</th>\n",
              "      <th>ref_entries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18980380</td>\n",
              "      <td>ffe93b67a395cc51d6dc4c5f438a6bbc08a3f31a</td>\n",
              "      <td>[{'section': 'Abstract', 'text': 'This technic...</td>\n",
              "      <td>[{'section': '', 'text': '. Illustration of th...</td>\n",
              "      <td>{'BIBREF0': {'title': 'Distributionally robust...</td>\n",
              "      <td>{'FIGREF0': {'text': 'The condition 1 of Assum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18981111</td>\n",
              "      <td>f3d9a81079300b70808121b2a16afdc5e08773f0</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'section': '', 'text': 'The ability to explo...</td>\n",
              "      <td>{'BIBREF0': {'title': 'Haptic virtual reality ...</td>\n",
              "      <td>{'FIGREF0': {'text': 'Figure 1. Multisensory e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18981625</td>\n",
              "      <td>fbdb4a4e1f62a31c9ee17c0bab27c429712e795c</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'BIBREF0': {'title': 'An algorithm for the tr...</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18982496</td>\n",
              "      <td>90c21e95578699827c1dc8d85c468cd24e6946e0</td>\n",
              "      <td>[{'section': 'Abstract', 'text': 'In this pape...</td>\n",
              "      <td>[{'section': '', 'text': 'name and sent by a c...</td>\n",
              "      <td>{'BIBREF0': {'title': 'Interviewing one's peer...</td>\n",
              "      <td>{'TABREF0': {'text': 'A summary of five survey...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18983082</td>\n",
              "      <td>9ba4fa0a3a19f222c39301aa98b15be560fc371b</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'BIBREF0': {'title': 'High technology of vehi...</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   paper_id  ...                                        ref_entries\n",
              "0  18980380  ...  {'FIGREF0': {'text': 'The condition 1 of Assum...\n",
              "1  18981111  ...  {'FIGREF0': {'text': 'Figure 1. Multisensory e...\n",
              "2  18981625  ...                                                 {}\n",
              "3  18982496  ...  {'TABREF0': {'text': 'A summary of five survey...\n",
              "4  18983082  ...                                                 {}\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "-vZ5K4Dk1DNb",
        "outputId": "f7466de3-5c67-4cdb-99e5-d984104ef3c5"
      },
      "source": [
        "#drop all rows that have empty ref_enteries\n",
        "clean_df = df[df.ref_entries != {}]\n",
        "clean_df.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>_pdf_hash</th>\n",
              "      <th>abstract</th>\n",
              "      <th>body_text</th>\n",
              "      <th>bib_entries</th>\n",
              "      <th>ref_entries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18980380</td>\n",
              "      <td>ffe93b67a395cc51d6dc4c5f438a6bbc08a3f31a</td>\n",
              "      <td>[{'section': 'Abstract', 'text': 'This technic...</td>\n",
              "      <td>[{'section': '', 'text': '. Illustration of th...</td>\n",
              "      <td>{'BIBREF0': {'title': 'Distributionally robust...</td>\n",
              "      <td>{'FIGREF0': {'text': 'The condition 1 of Assum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18981111</td>\n",
              "      <td>f3d9a81079300b70808121b2a16afdc5e08773f0</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'section': '', 'text': 'The ability to explo...</td>\n",
              "      <td>{'BIBREF0': {'title': 'Haptic virtual reality ...</td>\n",
              "      <td>{'FIGREF0': {'text': 'Figure 1. Multisensory e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18982496</td>\n",
              "      <td>90c21e95578699827c1dc8d85c468cd24e6946e0</td>\n",
              "      <td>[{'section': 'Abstract', 'text': 'In this pape...</td>\n",
              "      <td>[{'section': '', 'text': 'name and sent by a c...</td>\n",
              "      <td>{'BIBREF0': {'title': 'Interviewing one's peer...</td>\n",
              "      <td>{'TABREF0': {'text': 'A summary of five survey...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>18983391</td>\n",
              "      <td>9e87b9776d871dcd14c0659b2366bd2211909b3c</td>\n",
              "      <td>[{'section': 'Abstract', 'text': 'The conventi...</td>\n",
              "      <td>[{'section': 'Introduction', 'text': 'Traffic ...</td>\n",
              "      <td>{'BIBREF0': {'title': 'AntNet: Distributed sti...</td>\n",
              "      <td>{'FIGREF1': {'text': 'instant delay or trip ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>199668887</td>\n",
              "      <td>aadef1b1e13de9dc836b6b9e9f40b538a40464c4</td>\n",
              "      <td>[{'section': 'Abstract', 'text': 'Pervasive co...</td>\n",
              "      <td>[{'section': 'INTRODUCTION', 'text': 'Nowadays...</td>\n",
              "      <td>{'BIBREF0': {'title': 'Incorporating Contextua...</td>\n",
              "      <td>{'FIGREF0': {'text': 'Four types of data propo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     paper_id  ...                                        ref_entries\n",
              "0    18980380  ...  {'FIGREF0': {'text': 'The condition 1 of Assum...\n",
              "1    18981111  ...  {'FIGREF0': {'text': 'Figure 1. Multisensory e...\n",
              "3    18982496  ...  {'TABREF0': {'text': 'A summary of five survey...\n",
              "5    18983391  ...  {'FIGREF1': {'text': 'instant delay or trip ti...\n",
              "15  199668887  ...  {'FIGREF0': {'text': 'Four types of data propo...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4IayVsi4pfn",
        "outputId": "ad5579b3-bb02-4cb1-a922-ba3e20c5373d"
      },
      "source": [
        "clean_df.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14422, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csl0fYlA4vf0",
        "outputId": "0c6ed528-cc0f-4e3d-f949-4f95e2fcbc96"
      },
      "source": [
        "# number of sections in each article body text\n",
        "sections = []\n",
        "for row in clean_df.body_text:\n",
        "    sections.append(len(row))\n",
        "print(sections)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[55, 57, 41, 38, 25, 45, 58, 1, 41, 67, 57, 58, 78, 19, 38, 12, 79, 31, 56, 60, 44, 58, 46, 30, 111, 113, 58, 122, 36, 34, 65, 19, 51, 60, 100, 38, 65, 29, 102, 76, 10, 183, 58, 50, 82, 76, 67, 26, 31, 71, 26, 22, 29, 26, 9, 71, 39, 121, 67, 105, 55, 38, 35, 71, 116, 35, 52, 56, 29, 29, 14, 26, 36, 16, 60, 43, 139, 24, 95, 97, 83, 67, 14, 42, 32, 15, 43, 35, 57, 64, 38, 61, 51, 60, 51, 62, 32, 32, 18, 36, 39, 39, 32, 43, 64, 138, 88, 25, 65, 86, 31, 95, 10, 36, 24, 21, 41, 27, 39, 69, 36, 32, 33, 12, 48, 236, 60, 15, 39, 56, 40, 48, 69, 55, 34, 22, 35, 204, 22, 76, 56, 83, 45, 106, 77, 84, 20, 47, 62, 70, 173, 29, 63, 87, 46, 44, 74, 22, 20, 42, 28, 95, 78, 171, 19, 206, 56, 31, 2, 105, 24, 37, 44, 27, 58, 57, 20, 45, 127, 7, 39, 17, 45, 105, 50, 68, 6, 31, 92, 53, 65, 20, 22, 9, 49, 84, 136, 5, 122, 93, 7, 42, 11, 88, 51, 98, 58, 26, 35, 139, 62, 49, 42, 34, 51, 24, 99, 47, 49, 32, 45, 23, 43, 76, 98, 67, 56, 46, 17, 118, 116, 37, 75, 77, 17, 71, 50, 34, 43, 132, 30, 34, 25, 6, 30, 77, 27, 48, 32, 53, 180, 62, 87, 97, 118, 68, 68, 11, 57, 80, 11, 36, 53, 73, 12, 28, 75, 47, 72, 61, 138, 26, 32, 64, 42, 50, 48, 49, 95, 16, 41, 145, 59, 88, 75, 41, 25, 48, 35, 100, 65, 119, 110, 54, 69, 56, 29, 81, 46, 78, 132, 34, 14, 31, 29, 66, 6, 95, 45, 73, 41, 47, 50, 49, 38, 137, 112, 47, 87, 71, 13, 48, 101, 29, 137, 21, 142, 10, 67, 29, 18, 20, 91, 20, 37, 21, 16, 45, 79, 61, 47, 51, 96, 36, 118, 60, 39, 24, 58, 31, 93, 41, 72, 102, 187, 18, 57, 32, 32, 6, 15, 48, 23, 65, 25, 76, 5, 67, 35, 85, 34, 56, 3, 1, 79, 7, 57, 24, 93, 21, 127, 58, 61, 86, 33, 57, 187, 32, 64, 22, 29, 1, 15, 41, 73, 43, 35, 9, 11, 62, 147, 146, 168, 45, 24, 120, 28, 51, 52, 47, 79, 76, 110, 71, 24, 27, 91, 40, 77, 54, 42, 3, 34, 55, 23, 33, 59, 12, 21, 28, 22, 20, 85, 81, 77, 35, 90, 29, 70, 70, 13, 63, 25, 36, 62, 59, 25, 44, 76, 27, 63, 22, 13, 4, 50, 51, 32, 55, 13, 69, 44, 49, 31, 42, 31, 52, 56, 80, 32, 48, 5, 78, 46, 49, 27, 128, 48, 119, 50, 54, 21, 48, 66, 73, 131, 51, 60, 28, 27, 17, 10, 149, 44, 13, 5, 22, 97, 33, 17, 40, 86, 31, 62, 116, 41, 72, 43, 68, 30, 63, 31, 39, 120, 34, 35, 1, 7, 28, 28, 145, 81, 42, 5, 35, 53, 54, 37, 73, 22, 93, 193, 156, 230, 69, 30, 29, 26, 15, 95, 22, 81, 59, 21, 112, 93, 98, 124, 41, 5, 57, 33, 45, 85, 39, 30, 30, 30, 23, 55, 26, 123, 36, 1, 38, 53, 36, 58, 69, 32, 75, 18, 60, 105, 94, 50, 23, 54, 50, 51, 7, 12, 38, 137, 98, 18, 46, 249, 42, 45, 10, 13, 34, 59, 36, 156, 2, 34, 42, 40, 24, 47, 194, 194, 30, 39, 29, 56, 62, 136, 36, 89, 36, 52, 33, 50, 85, 35, 55, 92, 29, 55, 88, 23, 50, 82, 224, 82, 12, 25, 35, 52, 18, 59, 56, 122, 238, 62, 40, 13, 58, 108, 56, 27, 92, 93, 44, 11, 64, 25, 36, 15, 36, 47, 11, 16, 90, 81, 105, 60, 40, 21, 52, 154, 43, 30, 19, 111, 30, 58, 32, 45, 30, 21, 41, 23, 29, 10, 68, 56, 22, 6, 23, 21, 41, 35, 41, 62, 136, 42, 79, 69, 25, 117, 15, 53, 127, 99, 112, 23, 64, 31, 52, 44, 74, 75, 31, 36, 21, 32, 56, 167, 40, 73, 78, 18, 71, 31, 55, 40, 62, 51, 19, 38, 15, 41, 45, 40, 40, 74, 43, 38, 77, 33, 40, 38, 62, 79, 51, 12, 20, 31, 81, 31, 45, 57, 85, 29, 80, 79, 52, 10, 49, 49, 35, 28, 101, 31, 74, 34, 67, 44, 50, 36, 58, 37, 107, 19, 101, 43, 21, 66, 86, 42, 27, 33, 95, 36, 2, 69, 23, 35, 85, 131, 13, 19, 38, 18, 24, 23, 68, 44, 43, 33, 42, 255, 46, 22, 43, 164, 32, 61, 81, 8, 17, 82, 36, 148, 134, 29, 48, 38, 37, 30, 131, 57, 66, 41, 37, 43, 50, 31, 46, 33, 83, 44, 46, 28, 18, 7, 16, 28, 61, 25, 71, 13, 74, 88, 52, 35, 49, 76, 119, 14, 18, 44, 35, 25, 97, 30, 83, 25, 36, 92, 129, 53, 60, 37, 23, 60, 20, 44, 5, 18, 65, 36, 141, 25, 30, 162, 67, 70, 83, 70, 17, 60, 76, 10, 47, 53, 26, 16, 23, 25, 26, 66, 83, 106, 20, 11, 13, 4, 43, 42, 17, 17, 20, 23, 133, 45, 8, 34, 54, 41, 47, 52, 59, 33, 17, 27, 37, 12, 52, 76, 56, 161, 14, 31, 66, 20, 17, 8, 51, 6, 59, 46, 122, 27, 28, 52, 15, 128, 53, 42, 96, 84, 72, 74, 34, 343, 114, 126, 145, 53, 5, 6, 17, 18, 14, 35, 41, 7, 177, 58, 80, 44, 4, 56, 23, 144, 57, 48, 59, 147, 13, 80, 40, 37, 56, 60, 48, 56, 86, 60, 54, 21, 41, 52, 60, 81, 34, 31, 38, 97, 115, 45, 96, 85, 128, 51, 87, 62, 26, 34, 38, 27, 39, 14, 86, 161, 127, 70, 84, 66, 71, 55, 35, 25, 93, 45, 46, 173, 25, 48, 80, 56, 45, 54, 25, 52, 46, 57, 27, 19, 49, 188, 51, 56, 10, 20, 31, 69, 37, 79, 131, 83, 35, 106, 38, 79, 47, 100, 75, 57, 30, 54, 100, 93, 148, 154, 18, 72, 40, 74, 34, 73, 31, 6, 77, 174, 53, 28, 38, 25, 64, 36, 16, 28, 67, 7, 70, 29, 35, 191, 67, 14, 58, 31, 114, 34, 78, 86, 74, 16, 37, 39, 39, 22, 80, 38, 102, 43, 19, 25, 53, 58, 5, 1, 48, 2, 75, 16, 150, 29, 18, 79, 18, 37, 27, 29, 48, 117, 59, 146, 41, 35, 52, 42, 19, 102, 12, 61, 79, 137, 34, 46, 139, 57, 30, 52, 108, 44, 107, 62, 30, 72, 39, 77, 66, 47, 36, 77, 134, 69, 119, 58, 63, 26, 50, 83, 40, 65, 15, 24, 100, 37, 110, 25, 91, 101, 69, 199, 88, 39, 53, 26, 54, 129, 54, 25, 12, 15, 96, 31, 95, 12, 62, 20, 14, 22, 27, 88, 110, 14, 37, 137, 58, 42, 78, 90, 105, 48, 74, 61, 38, 39, 62, 49, 57, 42, 70, 31, 3, 91, 97, 33, 4, 26, 85, 25, 25, 108, 25, 29, 130, 13, 41, 90, 109, 42, 17, 133, 70, 62, 45, 21, 59, 39, 135, 70, 24, 51, 36, 20, 17, 194, 70, 63, 92, 59, 163, 66, 37, 9, 42, 59, 63, 33, 69, 51, 117, 261, 32, 66, 162, 66, 69, 28, 39, 39, 29, 24, 44, 75, 77, 63, 92, 33, 132, 20, 46, 6, 63, 24, 37, 67, 19, 51, 39, 23, 40, 177, 45, 40, 46, 21, 75, 56, 32, 16, 78, 29, 43, 8, 42, 73, 6, 72, 81, 43, 48, 49, 174, 39, 10, 51, 74, 199, 76, 53, 9, 24, 56, 60, 12, 87, 23, 76, 17, 72, 95, 89, 142, 35, 26, 10, 62, 105, 39, 13, 60, 229, 43, 67, 40, 69, 35, 25, 19, 19, 82, 73, 1, 34, 94, 91, 55, 159, 41, 68, 60, 100, 77, 10, 14, 20, 20, 45, 11, 22, 53, 13, 54, 19, 35, 49, 44, 56, 42, 31, 36, 76, 15, 167, 90, 19, 37, 26, 13, 46, 38, 52, 110, 49, 21, 22, 70, 60, 28, 48, 67, 66, 28, 34, 36, 25, 18, 35, 55, 60, 60, 6, 23, 17, 42, 59, 90, 78, 19, 21, 45, 94, 19, 62, 44, 50, 50, 37, 32, 56, 28, 34, 214, 21, 16, 58, 69, 37, 41, 9, 55, 54, 6, 1, 17, 29, 48, 39, 62, 52, 49, 56, 17, 89, 179, 8, 34, 28, 16, 62, 42, 6, 6, 34, 49, 21, 32, 40, 55, 28, 21, 141, 55, 83, 122, 46, 15, 94, 71, 16, 55, 8, 18, 24, 1, 1, 15, 58, 70, 70, 52, 76, 52, 27, 40, 38, 32, 38, 95, 40, 46, 37, 19, 46, 31, 122, 59, 65, 41, 33, 163, 76, 29, 132, 44, 25, 26, 25, 113, 95, 51, 54, 18, 15, 53, 34, 14, 11, 1, 49, 79, 59, 112, 14, 41, 83, 46, 64, 145, 43, 48, 28, 25, 61, 140, 113, 15, 23, 56, 199, 67, 42, 29, 53, 17, 26, 21, 30, 31, 32, 79, 27, 85, 60, 37, 26, 83, 39, 16, 25, 53, 46, 47, 61, 80, 31, 52, 48, 22, 47, 141, 17, 53, 18, 53, 59, 72, 98, 45, 31, 34, 79, 47, 41, 57, 31, 31, 29, 146, 30, 62, 36, 10, 51, 2, 48, 29, 54, 61, 29, 193, 90, 11, 154, 73, 116, 98, 40, 37, 35, 45, 35, 52, 40, 201, 42, 78, 54, 12, 230, 43, 49, 165, 79, 71, 53, 91, 89, 27, 29, 68, 28, 80, 88, 54, 123, 39, 59, 41, 3, 45, 10, 18, 166, 86, 112, 26, 99, 63, 20, 225, 18, 43, 32, 65, 104, 32, 41, 71, 64, 9, 5, 13, 30, 77, 54, 32, 49, 16, 65, 77, 22, 13, 39, 49, 44, 142, 43, 66, 117, 15, 25, 29, 59, 61, 192, 145, 42, 78, 229, 1, 59, 49, 28, 23, 53, 61, 56, 24, 32, 4, 29, 65, 37, 71, 59, 24, 31, 39, 107, 77, 44, 62, 22, 61, 29, 195, 1, 52, 25, 39, 22, 37, 22, 46, 21, 21, 34, 38, 62, 30, 36, 65, 23, 46, 26, 33, 66, 50, 81, 54, 14, 42, 37, 63, 142, 46, 28, 23, 54, 53, 51, 32, 15, 25, 16, 75, 30, 17, 41, 73, 58, 62, 30, 66, 52, 46, 50, 14, 21, 14, 23, 13, 48, 30, 48, 36, 32, 10, 7, 7, 10, 8, 7, 12, 9, 8, 9, 7, 11, 105, 33, 219, 99, 16, 18, 68, 17, 84, 93, 18, 24, 47, 47, 81, 29, 91, 142, 31, 64, 40, 69, 27, 73, 81, 65, 63, 10, 50, 106, 103, 11, 20, 47, 86, 21, 106, 41, 25, 45, 28, 107, 60, 57, 58, 8, 77, 73, 8, 11, 84, 54, 1, 76, 48, 95, 65, 50, 10, 11, 46, 56, 27, 26, 14, 112, 72, 153, 151, 126, 62, 30, 102, 44, 128, 127, 236, 68, 115, 73, 111, 71, 63, 31, 65, 122, 57, 46, 2, 38, 89, 47, 51, 35, 36, 156, 112, 105, 10, 34, 36, 25, 28, 25, 42, 14, 48, 36, 32, 58, 32, 65, 76, 227, 18, 64, 144, 32, 50, 150, 23, 119, 12, 157, 27, 204, 172, 51, 61, 116, 27, 41, 14, 95, 75, 56, 83, 17, 73, 45, 34, 65, 50, 50, 116, 16, 34, 45, 41, 61, 99, 5, 8, 3, 13, 9, 5, 10, 97, 40, 66, 53, 39, 109, 139, 111, 61, 85, 64, 51, 26, 167, 164, 83, 44, 179, 99, 60, 6, 56, 41, 57, 54, 40, 12, 35, 84, 19, 79, 158, 16, 54, 44, 15, 53, 34, 52, 44, 102, 28, 31, 40, 75, 218, 36, 23, 30, 91, 10, 37, 52, 45, 32, 33, 38, 53, 20, 96, 48, 32, 31, 49, 85, 23, 61, 49, 40, 137, 82, 66, 127, 11, 33, 117, 46, 44, 100, 50, 28, 27, 52, 33, 156, 47, 62, 19, 40, 83, 66, 32, 28, 87, 49, 39, 77, 44, 34, 51, 114, 56, 71, 300, 113, 36, 20, 96, 164, 69, 248, 54, 170, 83, 48, 36, 9, 36, 48, 14, 4, 62, 48, 182, 113, 19, 46, 35, 23, 75, 25, 77, 165, 76, 32, 13, 12, 55, 9, 14, 22, 32, 92, 87, 13, 33, 102, 63, 26, 14, 25, 64, 42, 76, 87, 3, 55, 43, 18, 60, 22, 29, 67, 116, 24, 141, 29, 98, 2, 74, 128, 114, 129, 42, 23, 28, 59, 11, 23, 36, 33, 28, 73, 126, 60, 56, 8, 25, 175, 96, 26, 75, 81, 41, 68, 24, 45, 12, 49, 40, 11, 14, 11, 121, 99, 169, 19, 45, 36, 34, 141, 26, 26, 65, 19, 45, 30, 49, 60, 93, 31, 38, 266, 52, 10, 46, 18, 43, 81, 26, 23, 14, 129, 98, 26, 66, 51, 21, 31, 14, 405, 173, 43, 3, 125, 17, 57, 43, 57, 37, 12, 5, 15, 37, 63, 16, 58, 26, 42, 85, 74, 42, 36, 49, 42, 135, 12, 44, 24, 83, 43, 81, 59, 50, 97, 42, 103, 97, 77, 29, 90, 14, 38, 60, 43, 8, 64, 12, 26, 29, 69, 13, 50, 11, 103, 77, 40, 15, 88, 46, 11, 32, 15, 38, 72, 42, 90, 10, 41, 48, 64, 12, 39, 25, 29, 31, 45, 33, 13, 51, 53, 33, 22, 18, 25, 97, 45, 44, 44, 41, 219, 48, 35, 21, 13, 48, 52, 27, 43, 49, 11, 123, 101, 118, 76, 46, 57, 34, 15, 77, 114, 29, 39, 68, 72, 24, 39, 61, 58, 51, 202, 27, 47, 66, 66, 23, 43, 121, 38, 16, 64, 23, 42, 27, 16, 60, 37, 24, 37, 35, 7, 53, 98, 41, 107, 79, 194, 75, 156, 17, 36, 19, 68, 199, 17, 41, 102, 78, 45, 33, 30, 61, 35, 18, 138, 87, 126, 31, 72, 97, 11, 19, 121, 95, 97, 48, 46, 44, 49, 27, 18, 56, 70, 52, 9, 93, 21, 58, 17, 39, 60, 12, 36, 58, 32, 6, 64, 53, 101, 74, 122, 44, 94, 8, 32, 28, 16, 18, 35, 53, 36, 53, 103, 53, 38, 29, 39, 54, 34, 158, 76, 63, 195, 79, 79, 34, 57, 1, 41, 53, 30, 69, 48, 82, 30, 101, 35, 81, 63, 42, 64, 35, 49, 13, 11, 45, 45, 160, 56, 27, 60, 72, 26, 21, 58, 32, 1, 31, 45, 12, 57, 36, 100, 36, 45, 76, 27, 31, 84, 80, 118, 6, 51, 54, 243, 81, 162, 33, 273, 6, 45, 53, 8, 116, 54, 82, 40, 80, 49, 32, 178, 53, 16, 29, 10, 38, 95, 24, 141, 59, 23, 65, 59, 116, 32, 71, 107, 57, 65, 47, 66, 259, 59, 172, 13, 26, 67, 18, 57, 13, 17, 82, 12, 25, 50, 99, 58, 48, 93, 43, 80, 29, 23, 61, 22, 25, 31, 30, 16, 51, 33, 29, 23, 72, 20, 139, 65, 21, 108, 54, 45, 49, 75, 1, 53, 13, 33, 21, 52, 22, 9, 24, 143, 196, 41, 53, 35, 32, 49, 127, 5, 41, 37, 49, 38, 40, 31, 49, 78, 31, 39, 103, 92, 80, 34, 78, 133, 41, 61, 91, 224, 79, 86, 60, 161, 120, 43, 22, 103, 13, 17, 92, 17, 1, 101, 3, 135, 28, 82, 101, 30, 96, 34, 67, 31, 36, 42, 114, 82, 47, 20, 20, 44, 12, 136, 52, 18, 33, 43, 73, 63, 91, 32, 77, 9, 91, 19, 12, 11, 93, 136, 41, 69, 12, 61, 10, 47, 57, 53, 35, 31, 135, 32, 42, 14, 36, 46, 56, 9, 10, 47, 79, 53, 73, 45, 39, 24, 2, 114, 97, 79, 128, 29, 83, 80, 71, 42, 43, 250, 61, 41, 17, 107, 14, 35, 11, 52, 34, 36, 42, 49, 20, 9, 108, 21, 123, 12, 77, 69, 74, 12, 6, 80, 25, 133, 58, 37, 39, 111, 27, 79, 5, 101, 75, 47, 54, 17, 63, 39, 126, 48, 46, 31, 48, 71, 41, 81, 47, 294, 41, 37, 61, 12, 44, 28, 51, 125, 58, 110, 27, 67, 89, 24, 40, 30, 47, 16, 33, 23, 76, 48, 7, 51, 40, 95, 125, 19, 89, 168, 226, 18, 57, 109, 24, 45, 71, 14, 55, 20, 87, 36, 32, 24, 50, 81, 34, 66, 66, 61, 130, 17, 63, 104, 33, 150, 55, 62, 52, 39, 52, 40, 34, 30, 31, 234, 31, 21, 42, 11, 46, 32, 70, 31, 57, 60, 46, 54, 260, 26, 57, 63, 126, 62, 24, 38, 125, 18, 69, 55, 68, 55, 56, 19, 53, 24, 22, 109, 85, 46, 61, 82, 20, 28, 40, 52, 64, 31, 48, 26, 14, 25, 30, 84, 123, 21, 16, 53, 77, 65, 44, 35, 3, 67, 31, 11, 66, 27, 37, 32, 85, 75, 45, 50, 65, 23, 12, 36, 72, 20, 27, 228, 8, 50, 12, 106, 22, 54, 7, 7, 41, 25, 135, 73, 44, 88, 34, 34, 102, 38, 21, 16, 72, 34, 58, 33, 158, 48, 97, 44, 83, 33, 47, 26, 99, 27, 59, 76, 27, 86, 68, 73, 65, 95, 157, 8, 47, 55, 165, 91, 69, 70, 56, 66, 32, 80, 10, 116, 44, 35, 18, 35, 42, 82, 66, 75, 51, 26, 73, 25, 1, 15, 45, 61, 61, 116, 75, 79, 73, 12, 31, 27, 11, 16, 93, 30, 31, 146, 80, 35, 57, 15, 87, 62, 61, 32, 11, 96, 40, 101, 105, 32, 66, 29, 25, 66, 30, 29, 56, 30, 1, 50, 273, 97, 36, 37, 170, 53, 82, 32, 33, 62, 22, 26, 16, 160, 29, 41, 18, 60, 94, 33, 42, 15, 41, 48, 59, 196, 49, 134, 29, 34, 62, 138, 147, 95, 38, 82, 32, 13, 23, 39, 47, 72, 37, 25, 69, 17, 68, 63, 63, 115, 51, 32, 21, 45, 24, 58, 61, 36, 63, 7, 36, 88, 43, 37, 61, 52, 56, 131, 50, 16, 47, 136, 29, 9, 58, 50, 56, 54, 30, 35, 47, 51, 42, 43, 2, 132, 30, 75, 70, 32, 53, 111, 38, 39, 17, 54, 54, 9, 39, 78, 64, 46, 50, 8, 7, 40, 36, 44, 19, 51, 108, 13, 45, 45, 5, 240, 33, 30, 23, 27, 71, 44, 68, 44, 19, 115, 76, 44, 50, 67, 72, 28, 47, 104, 30, 33, 15, 116, 32, 56, 81, 108, 33, 43, 162, 25, 129, 67, 33, 106, 45, 66, 191, 39, 48, 7, 41, 39, 124, 62, 77, 6, 2, 51, 58, 96, 44, 227, 81, 48, 95, 39, 30, 1, 19, 112, 50, 56, 144, 20, 106, 210, 43, 82, 42, 58, 55, 122, 23, 50, 21, 1, 14, 69, 116, 67, 52, 56, 34, 52, 41, 53, 110, 11, 50, 1, 38, 2, 28, 45, 4, 63, 4, 29, 34, 56, 56, 25, 49, 31, 133, 91, 121, 26, 26, 102, 192, 49, 61, 51, 30, 36, 169, 101, 24, 11, 32, 32, 57, 42, 37, 135, 34, 76, 38, 14, 24, 10, 34, 33, 69, 17, 19, 47, 104, 72, 10, 49, 12, 63, 12, 21, 6, 29, 178, 95, 20, 63, 107, 53, 32, 50, 29, 117, 55, 34, 8, 130, 83, 11, 29, 43, 37, 26, 45, 77, 40, 39, 17, 13, 27, 104, 29, 39, 13, 32, 47, 75, 18, 127, 57, 32, 76, 33, 85, 33, 53, 62, 45, 16, 68, 67, 27, 32, 39, 111, 14, 108, 177, 20, 50, 76, 9, 74, 29, 26, 5, 117, 49, 21, 40, 35, 44, 72, 60, 25, 70, 46, 28, 17, 37, 30, 86, 44, 24, 58, 18, 67, 74, 90, 224, 46, 35, 42, 193, 18, 20, 171, 43, 66, 74, 59, 46, 16, 203, 36, 19, 82, 17, 25, 108, 33, 109, 10, 31, 22, 49, 39, 28, 6, 31, 61, 119, 42, 32, 18, 31, 23, 18, 57, 21, 38, 80, 22, 95, 29, 26, 24, 70, 64, 40, 32, 77, 92, 37, 18, 47, 10, 47, 47, 19, 23, 16, 5, 13, 63, 36, 81, 72, 52, 19, 56, 44, 15, 144, 25, 79, 27, 15, 43, 95, 35, 52, 100, 73, 96, 31, 79, 28, 50, 32, 56, 34, 37, 74, 27, 104, 51, 61, 59, 76, 47, 43, 30, 39, 45, 28, 42, 35, 25, 13, 31, 30, 85, 125, 24, 83, 34, 10, 34, 39, 56, 89, 31, 70, 32, 22, 15, 53, 5, 87, 36, 46, 27, 106, 77, 15, 9, 22, 41, 47, 48, 19, 49, 35, 38, 63, 51, 50, 46, 58, 1, 61, 103, 26, 81, 58, 83, 22, 26, 27, 54, 16, 32, 33, 122, 24, 18, 39, 43, 95, 26, 52, 39, 142, 25, 19, 35, 31, 19, 30, 137, 8, 58, 114, 72, 42, 58, 112, 21, 54, 75, 39, 57, 53, 79, 28, 12, 36, 37, 38, 63, 47, 16, 4, 137, 9, 31, 57, 95, 69, 57, 30, 14, 11, 48, 50, 69, 40, 169, 25, 22, 154, 57, 91, 41, 112, 27, 26, 75, 37, 20, 12, 19, 36, 69, 15, 45, 51, 57, 17, 40, 97, 65, 66, 64, 22, 79, 165, 84, 102, 172, 57, 186, 39, 51, 99, 55, 47, 27, 117, 97, 3, 26, 77, 21, 116, 15, 21, 168, 56, 87, 32, 18, 260, 119, 34, 36, 129, 39, 9, 5, 90, 32, 25, 25, 84, 25, 70, 31, 72, 4, 33, 13, 21, 35, 53, 36, 132, 80, 52, 80, 39, 31, 48, 120, 118, 66, 29, 23, 99, 10, 129, 5, 34, 35, 62, 65, 30, 15, 20, 16, 15, 37, 3, 86, 30, 75, 54, 13, 16, 52, 47, 10, 121, 11, 95, 25, 53, 52, 29, 282, 50, 9, 77, 61, 11, 92, 24, 64, 22, 35, 39, 25, 6, 26, 78, 35, 17, 105, 105, 26, 17, 33, 24, 10, 18, 2, 19, 185, 46, 112, 80, 19, 26, 37, 31, 27, 50, 78, 20, 84, 30, 50, 178, 40, 45, 73, 21, 24, 52, 28, 29, 73, 5, 41, 121, 65, 62, 61, 113, 18, 71, 22, 16, 1, 19, 61, 42, 42, 54, 52, 32, 76, 50, 45, 40, 61, 32, 23, 11, 63, 34, 2, 87, 30, 47, 10, 91, 1, 80, 41, 81, 64, 37, 398, 25, 24, 28, 49, 136, 122, 32, 61, 41, 30, 74, 142, 79, 21, 54, 49, 58, 1, 42, 44, 120, 48, 21, 36, 20, 6, 42, 39, 23, 21, 73, 63, 91, 56, 48, 11, 61, 21, 60, 41, 40, 23, 39, 42, 103, 49, 41, 59, 57, 21, 71, 95, 129, 24, 61, 26, 99, 30, 138, 42, 43, 45, 79, 60, 85, 70, 143, 79, 74, 47, 24, 72, 109, 14, 130, 100, 44, 24, 63, 25, 15, 33, 8, 47, 69, 71, 112, 41, 5, 129, 31, 54, 39, 55, 58, 36, 55, 48, 9, 45, 41, 66, 32, 32, 39, 34, 87, 18, 80, 45, 74, 50, 38, 21, 45, 10, 64, 24, 61, 41, 69, 34, 111, 41, 70, 32, 41, 105, 169, 21, 201, 42, 36, 7, 84, 37, 60, 59, 7, 55, 53, 59, 7, 55, 8, 102, 48, 43, 157, 4, 49, 98, 122, 34, 72, 90, 55, 54, 55, 67, 47, 11, 29, 42, 78, 35, 111, 97, 27, 76, 32, 147, 28, 48, 55, 33, 111, 70, 46, 36, 51, 109, 47, 50, 163, 59, 87, 33, 39, 1, 74, 8, 96, 97, 92, 17, 89, 146, 44, 58, 19, 61, 45, 8, 64, 93, 55, 55, 359, 60, 259, 63, 40, 27, 63, 149, 22, 113, 34, 20, 41, 71, 46, 99, 24, 8, 146, 79, 68, 44, 11, 24, 41, 53, 27, 15, 77, 154, 57, 30, 21, 16, 74, 18, 26, 24, 40, 21, 28, 89, 84, 6, 24, 134, 40, 35, 32, 67, 28, 139, 113, 54, 82, 76, 76, 31, 56, 98, 46, 20, 68, 19, 65, 45, 15, 193, 8, 56, 83, 85, 9, 59, 29, 43, 65, 28, 100, 39, 48, 45, 33, 57, 108, 31, 34, 30, 10, 11, 82, 41, 29, 107, 32, 21, 28, 36, 285, 171, 24, 34, 28, 25, 44, 49, 65, 56, 17, 6, 54, 45, 65, 104, 32, 38, 75, 37, 11, 59, 10, 112, 38, 40, 24, 57, 199, 33, 40, 37, 93, 124, 8, 46, 79, 46, 23, 93, 47, 77, 34, 54, 26, 28, 38, 23, 45, 30, 12, 71, 21, 38, 118, 54, 53, 53, 33, 61, 19, 35, 8, 41, 35, 225, 95, 59, 22, 41, 32, 44, 67, 66, 27, 36, 142, 42, 108, 49, 51, 24, 147, 64, 77, 39, 91, 102, 61, 32, 57, 104, 31, 244, 29, 77, 31, 19, 13, 19, 56, 54, 41, 45, 77, 63, 92, 52, 41, 7, 48, 38, 70, 37, 351, 38, 41, 32, 37, 89, 16, 2, 41, 26, 28, 29, 53, 44, 34, 27, 114, 186, 57, 118, 56, 48, 27, 48, 26, 73, 153, 73, 62, 63, 43, 64, 26, 73, 81, 14, 63, 34, 53, 32, 35, 20, 27, 10, 63, 51, 30, 48, 96, 42, 34, 43, 14, 60, 49, 42, 36, 51, 58, 61, 145, 13, 101, 21, 23, 124, 17, 22, 123, 25, 37, 47, 203, 34, 20, 43, 67, 82, 40, 55, 233, 99, 185, 21, 91, 84, 85, 36, 69, 75, 7, 74, 60, 22, 5, 25, 102, 12, 69, 62, 38, 26, 68, 68, 74, 210, 52, 61, 41, 31, 17, 30, 144, 84, 40, 52, 59, 180, 68, 104, 26, 15, 60, 51, 120, 41, 60, 48, 146, 97, 73, 151, 12, 109, 29, 55, 41, 99, 50, 36, 55, 31, 64, 110, 35, 70, 28, 110, 40, 93, 34, 35, 66, 83, 12, 87, 50, 50, 62, 55, 22, 22, 105, 27, 50, 7, 142, 3, 17, 3, 13, 59, 23, 84, 53, 29, 50, 32, 9, 36, 87, 44, 66, 89, 59, 58, 95, 38, 182, 35, 105, 85, 74, 47, 46, 48, 87, 80, 38, 67, 126, 23, 21, 47, 84, 27, 50, 31, 72, 139, 13, 162, 31, 35, 19, 48, 9, 47, 68, 36, 85, 92, 38, 27, 58, 25, 41, 86, 21, 50, 136, 13, 28, 52, 22, 114, 52, 30, 45, 90, 25, 59, 39, 67, 120, 140, 34, 40, 60, 14, 80, 22, 39, 70, 96, 13, 21, 30, 75, 14, 38, 35, 4, 55, 19, 88, 54, 20, 76, 155, 54, 48, 19, 47, 17, 43, 28, 66, 23, 62, 40, 7, 62, 35, 109, 19, 19, 104, 45, 31, 43, 42, 36, 55, 66, 46, 53, 83, 30, 47, 108, 26, 23, 15, 60, 103, 147, 35, 11, 56, 48, 24, 36, 83, 30, 24, 183, 41, 43, 51, 85, 31, 77, 39, 237, 43, 28, 30, 29, 2, 103, 36, 60, 26, 38, 45, 47, 48, 67, 24, 16, 77, 142, 13, 71, 38, 46, 43, 35, 46, 63, 47, 88, 81, 141, 26, 119, 12, 26, 6, 90, 49, 52, 30, 96, 66, 67, 43, 37, 17, 81, 32, 68, 65, 37, 34, 27, 14, 40, 45, 35, 56, 33, 61, 13, 50, 74, 106, 18, 59, 50, 43, 15, 152, 28, 32, 147, 55, 38, 64, 24, 292, 91, 54, 6, 36, 232, 15, 157, 32, 23, 75, 63, 29, 54, 13, 85, 18, 31, 110, 46, 67, 40, 68, 99, 69, 100, 5, 81, 39, 27, 32, 84, 110, 65, 65, 66, 72, 25, 147, 50, 32, 39, 42, 45, 21, 18, 10, 34, 45, 35, 83, 47, 51, 75, 138, 49, 44, 47, 56, 37, 19, 43, 118, 7, 25, 27, 49, 166, 79, 96, 50, 118, 81, 64, 71, 14, 62, 190, 31, 197, 128, 64, 47, 26, 25, 9, 80, 34, 39, 41, 36, 42, 31, 35, 156, 57, 33, 84, 77, 40, 41, 56, 14, 12, 8, 9, 4, 3, 14, 9, 12, 10, 14, 13, 5, 13, 16, 10, 13, 9, 12, 12, 16, 5, 12, 9, 7, 37, 53, 33, 49, 57, 61, 37, 60, 83, 40, 25, 84, 25, 1, 80, 21, 100, 27, 55, 61, 26, 28, 40, 41, 45, 69, 25, 81, 46, 77, 62, 116, 51, 52, 51, 10, 47, 33, 24, 3, 44, 44, 11, 29, 19, 26, 88, 25, 63, 88, 55, 111, 50, 40, 117, 58, 41, 87, 33, 39, 12, 38, 35, 14, 36, 103, 74, 55, 32, 19, 100, 33, 39, 9, 38, 23, 40, 112, 39, 41, 14, 58, 53, 18, 29, 46, 101, 154, 25, 12, 26, 69, 60, 26, 66, 43, 79, 11, 42, 56, 31, 58, 61, 34, 74, 2, 75, 18, 26, 43, 25, 24, 83, 78, 67, 34, 14, 12, 35, 69, 51, 9, 21, 60, 42, 38, 62, 20, 89, 10, 34, 84, 117, 27, 73, 30, 58, 50, 62, 98, 156, 11, 35, 36, 17, 82, 93, 50, 25, 118, 157, 205, 13, 30, 32, 50, 17, 68, 45, 150, 36, 173, 172, 120, 25, 30, 41, 63, 108, 7, 50, 31, 47, 1, 1, 84, 150, 22, 79, 37, 104, 40, 13, 20, 31, 34, 28, 6, 135, 129, 92, 3, 52, 91, 77, 39, 69, 103, 53, 76, 67, 11, 17, 50, 25, 34, 153, 48, 49, 70, 37, 50, 68, 65, 85, 54, 51, 42, 50, 46, 68, 4, 82, 78, 109, 42, 57, 17, 46, 40, 41, 71, 73, 32, 60, 109, 8, 28, 81, 24, 50, 10, 62, 40, 37, 28, 21, 40, 56, 29, 90, 1, 45, 78, 46, 15, 65, 86, 81, 71, 72, 62, 25, 60, 70, 43, 48, 23, 38, 17, 37, 89, 21, 55, 48, 74, 75, 29, 22, 10, 18, 11, 37, 65, 37, 26, 81, 42, 39, 30, 41, 50, 19, 114, 98, 82, 149, 35, 37, 59, 34, 110, 96, 32, 98, 17, 79, 46, 191, 72, 56, 34, 64, 20, 83, 27, 139, 39, 58, 40, 47, 50, 29, 32, 34, 58, 41, 65, 82, 24, 116, 80, 40, 221, 55, 84, 106, 62, 43, 52, 129, 19, 22, 11, 48, 86, 117, 122, 39, 102, 122, 41, 172, 41, 69, 60, 23, 22, 177, 43, 48, 38, 48, 54, 37, 49, 92, 40, 68, 45, 16, 69, 44, 117, 169, 43, 11, 18, 46, 63, 74, 75, 75, 67, 41, 111, 20, 71, 130, 178, 67, 1, 25, 151, 34, 12, 224, 37, 23, 4, 53, 70, 52, 49, 99, 30, 42, 8, 62, 25, 40, 111, 2, 22, 5, 29, 32, 48, 9, 79, 102, 17, 66, 48, 18, 21, 50, 115, 43, 79, 44, 43, 24, 59, 37, 189, 25, 39, 65, 18, 80, 67, 74, 25, 41, 43, 48, 53, 49, 32, 135, 42, 76, 22, 96, 8, 45, 51, 64, 36, 43, 53, 63, 45, 23, 216, 39, 45, 38, 42, 47, 41, 47, 29, 45, 72, 129, 48, 24, 65, 50, 52, 43, 53, 47, 38, 6, 65, 85, 61, 25, 38, 50, 18, 113, 91, 12, 73, 12, 52, 9, 39, 15, 1, 66, 34, 36, 38, 49, 31, 133, 86, 66, 69, 56, 59, 11, 46, 41, 107, 17, 113, 72, 103, 23, 102, 24, 32, 17, 26, 13, 102, 68, 24, 86, 11, 70, 22, 69, 70, 23, 41, 66, 84, 47, 38, 44, 91, 7, 79, 82, 39, 31, 83, 31, 25, 40, 61, 74, 17, 28, 58, 33, 30, 103, 48, 42, 95, 66, 7, 80, 153, 8, 80, 67, 131, 182, 90, 75, 253, 66, 94, 15, 23, 44, 37, 274, 71, 25, 24, 89, 113, 106, 21, 117, 48, 33, 34, 39, 18, 27, 20, 19, 3, 16, 31, 31, 50, 69, 22, 48, 59, 38, 64, 7, 32, 39, 13, 53, 52, 109, 14, 156, 74, 41, 75, 44, 23, 51, 53, 155, 177, 37, 42, 67, 28, 41, 31, 44, 36, 36, 51, 80, 50, 93, 59, 100, 41, 20, 137, 35, 36, 70, 133, 31, 46, 39, 67, 114, 92, 37, 55, 166, 148, 51, 44, 38, 65, 21, 47, 52, 50, 94, 36, 37, 166, 33, 12, 23, 38, 39, 45, 23, 98, 38, 25, 222, 70, 38, 20, 16, 36, 91, 43, 21, 67, 174, 72, 17, 44, 28, 22, 34, 64, 10, 22, 80, 33, 86, 10, 42, 54, 51, 28, 75, 6, 53, 54, 1, 72, 105, 28, 61, 5, 13, 48, 101, 84, 59, 154, 29, 51, 76, 30, 112, 32, 40, 82, 26, 32, 20, 38, 32, 77, 27, 25, 68, 17, 35, 32, 39, 119, 93, 158, 54, 23, 71, 22, 26, 27, 50, 88, 22, 77, 34, 112, 5, 59, 78, 12, 114, 7, 22, 34, 46, 48, 13, 26, 25, 86, 36, 97, 97, 58, 59, 13, 50, 117, 76, 70, 122, 75, 4, 4, 62, 19, 41, 242, 78, 40, 40, 47, 55, 18, 24, 23, 71, 22, 18, 20, 16, 52, 20, 37, 39, 198, 45, 33, 40, 92, 73, 41, 68, 13, 69, 57, 36, 24, 37, 9, 25, 49, 73, 94, 19, 8, 18, 67, 38, 62, 24, 80, 85, 54, 68, 29, 30, 34, 69, 28, 78, 48, 101, 90, 26, 94, 85, 53, 13, 5, 49, 58, 68, 14, 22, 24, 161, 39, 220, 100, 162, 21, 23, 63, 35, 127, 7, 33, 57, 39, 90, 34, 8, 37, 122, 158, 20, 48, 172, 221, 34, 22, 54, 33, 103, 108, 9, 52, 18, 30, 32, 34, 35, 66, 48, 190, 141, 81, 49, 18, 5, 85, 28, 57, 43, 178, 49, 42, 47, 6, 33, 19, 84, 21, 42, 20, 57, 77, 50, 22, 43, 53, 135, 2, 45, 57, 13, 66, 118, 73, 17, 77, 96, 44, 12, 17, 159, 30, 22, 65, 71, 42, 65, 27, 12, 83, 115, 92, 59, 37, 28, 159, 62, 40, 18, 36, 61, 36, 94, 16, 27, 6, 45, 9, 36, 20, 36, 54, 53, 49, 61, 58, 28, 64, 5, 63, 95, 59, 50, 19, 42, 42, 21, 24, 108, 21, 34, 93, 57, 9, 77, 24, 13, 90, 90, 73, 169, 49, 18, 24, 51, 54, 44, 16, 34, 46, 39, 36, 28, 98, 11, 23, 68, 22, 35, 87, 44, 23, 45, 47, 1, 45, 2, 93, 55, 55, 83, 45, 30, 19, 35, 96, 10, 82, 43, 42, 26, 67, 1, 18, 112, 27, 31, 50, 102, 27, 33, 63, 73, 64, 46, 31, 46, 295, 20, 32, 47, 38, 13, 52, 50, 49, 109, 63, 34, 40, 18, 27, 85, 54, 26, 4, 35, 32, 23, 38, 42, 56, 34, 24, 10, 64, 97, 37, 53, 37, 28, 41, 191, 38, 34, 14, 25, 24, 12, 50, 109, 40, 14, 33, 95, 13, 73, 83, 56, 35, 209, 25, 44, 38, 22, 45, 142, 31, 42, 153, 58, 28, 45, 21, 21, 22, 62, 85, 46, 38, 135, 214, 89, 99, 57, 4, 64, 74, 13, 82, 51, 39, 1, 120, 60, 61, 40, 27, 44, 102, 93, 137, 84, 36, 20, 53, 35, 10, 18, 112, 49, 54, 43, 5, 35, 61, 93, 28, 58, 47, 53, 8, 41, 46, 69, 49, 33, 27, 108, 61, 53, 17, 59, 10, 85, 27, 40, 80, 67, 99, 12, 45, 39, 20, 29, 33, 13, 51, 43, 202, 23, 31, 50, 82, 46, 109, 123, 70, 87, 113, 66, 101, 30, 30, 31, 39, 36, 35, 64, 33, 56, 30, 57, 46, 43, 20, 57, 30, 3, 28, 5, 43, 11, 24, 21, 66, 37, 113, 37, 52, 56, 137, 140, 23, 36, 19, 29, 35, 7, 7, 45, 96, 44, 67, 68, 32, 51, 78, 21, 46, 25, 39, 69, 40, 43, 83, 56, 147, 71, 85, 216, 61, 168, 22, 78, 17, 67, 49, 134, 150, 86, 46, 62, 47, 84, 32, 108, 61, 144, 25, 135, 37, 89, 13, 97, 58, 53, 47, 47, 37, 24, 40, 49, 80, 28, 36, 133, 134, 59, 66, 47, 44, 5, 38, 31, 36, 46, 33, 147, 82, 74, 28, 109, 37, 132, 24, 106, 87, 58, 40, 70, 144, 22, 39, 58, 70, 27, 76, 106, 39, 47, 36, 89, 27, 42, 13, 44, 33, 1, 136, 15, 61, 81, 26, 78, 35, 202, 33, 54, 33, 48, 87, 89, 26, 37, 86, 1, 47, 16, 52, 46, 66, 109, 40, 83, 26, 2, 49, 49, 25, 35, 44, 94, 68, 89, 16, 78, 9, 42, 18, 48, 23, 11, 79, 31, 35, 45, 30, 24, 11, 99, 33, 117, 64, 28, 21, 52, 51, 63, 90, 17, 35, 36, 13, 31, 66, 24, 101, 82, 63, 51, 52, 24, 69, 22, 88, 42, 24, 73, 13, 45, 102, 9, 30, 66, 35, 62, 66, 22, 155, 4, 16, 20, 10, 26, 281, 57, 46, 52, 20, 33, 11, 41, 54, 59, 94, 31, 132, 112, 46, 46, 168, 23, 81, 50, 153, 44, 31, 50, 28, 91, 53, 48, 163, 14, 6, 25, 24, 84, 25, 8, 40, 19, 12, 63, 82, 21, 58, 8, 153, 70, 35, 53, 112, 20, 22, 75, 58, 111, 59, 72, 86, 81, 20, 75, 46, 67, 3, 73, 52, 48, 44, 90, 75, 73, 91, 48, 70, 53, 102, 195, 52, 19, 5, 48, 114, 82, 48, 59, 19, 18, 57, 39, 13, 52, 17, 27, 14, 55, 80, 62, 77, 23, 27, 29, 48, 29, 61, 81, 67, 29, 79, 35, 60, 41, 43, 64, 50, 43, 46, 49, 22, 72, 68, 23, 22, 26, 122, 40, 138, 49, 83, 41, 117, 58, 34, 135, 38, 26, 58, 76, 19, 119, 36, 19, 83, 76, 73, 37, 23, 31, 18, 54, 65, 62, 27, 68, 85, 126, 26, 16, 36, 18, 32, 225, 48, 22, 58, 70, 3, 16, 4, 39, 119, 31, 14, 72, 39, 1, 33, 111, 71, 21, 56, 12, 58, 31, 33, 81, 67, 49, 63, 22, 35, 46, 54, 23, 120, 36, 40, 28, 128, 40, 38, 136, 29, 178, 85, 42, 28, 81, 98, 35, 120, 43, 67, 128, 48, 39, 64, 62, 65, 56, 191, 55, 137, 71, 27, 41, 49, 30, 69, 18, 101, 57, 30, 40, 74, 101, 22, 80, 194, 47, 44, 68, 130, 22, 21, 133, 15, 45, 25, 42, 33, 114, 19, 30, 46, 61, 31, 63, 114, 20, 7, 73, 71, 39, 80, 15, 60, 118, 42, 57, 45, 63, 205, 25, 37, 92, 56, 63, 142, 82, 15, 127, 343, 29, 59, 19, 241, 16, 14, 38, 22, 84, 22, 55, 29, 102, 26, 5, 51, 76, 131, 77, 76, 28, 56, 37, 28, 93, 122, 58, 41, 76, 26, 44, 64, 50, 32, 98, 37, 124, 49, 133, 30, 57, 52, 49, 129, 88, 63, 23, 17, 12, 14, 43, 62, 30, 67, 125, 70, 101, 50, 51, 20, 30, 36, 20, 18, 128, 280, 44, 46, 64, 102, 8, 70, 32, 23, 20, 33, 91, 2, 28, 1, 78, 49, 26, 19, 137, 14, 45, 25, 23, 16, 41, 80, 63, 138, 16, 14, 48, 46, 62, 170, 42, 26, 50, 71, 78, 40, 73, 45, 197, 36, 70, 272, 46, 32, 35, 72, 53, 79, 29, 39, 34, 44, 42, 49, 46, 119, 75, 63, 36, 33, 30, 149, 32, 34, 21, 85, 60, 94, 22, 32, 76, 18, 39, 130, 42, 70, 48, 21, 38, 43, 53, 67, 11, 20, 32, 5, 70, 24, 8, 1, 1, 75, 34, 70, 23, 43, 86, 9, 58, 28, 18, 53, 54, 181, 131, 45, 49, 72, 43, 31, 80, 57, 25, 85, 42, 1, 81, 50, 53, 13, 10, 30, 59, 23, 19, 87, 33, 21, 55, 124, 55, 30, 31, 66, 69, 40, 37, 6, 25, 65, 60, 76, 62, 62, 106, 47, 34, 108, 58, 37, 48, 51, 43, 82, 17, 129, 1, 15, 23, 48, 18, 50, 60, 27, 91, 11, 21, 124, 56, 30, 19, 80, 26, 43, 6, 126, 7, 51, 35, 31, 46, 73, 55, 63, 58, 35, 58, 42, 34, 37, 68, 136, 27, 18, 18, 25, 20, 34, 45, 103, 46, 10, 1, 8, 6, 97, 47, 35, 65, 33, 27, 31, 60, 54, 2, 30, 17, 19, 40, 60, 84, 66, 18, 32, 136, 56, 58, 49, 84, 60, 59, 77, 160, 89, 20, 30, 73, 79, 175, 89, 51, 33, 13, 54, 21, 79, 26, 78, 69, 63, 16, 23, 26, 27, 232, 9, 67, 40, 138, 88, 44, 90, 36, 26, 63, 27, 28, 86, 45, 8, 28, 73, 80, 38, 59, 45, 27, 42, 178, 96, 36, 98, 45, 41, 147, 76, 16, 27, 130, 11, 24, 36, 9, 28, 149, 1, 93, 28, 2, 89, 52, 69, 48, 110, 44, 19, 75, 37, 78, 57, 36, 26, 29, 62, 31, 38, 81, 8, 37, 54, 134, 52, 38, 36, 22, 1, 105, 42, 66, 25, 80, 60, 98, 43, 153, 98, 95, 93, 17, 30, 56, 48, 46, 57, 67, 86, 147, 23, 44, 69, 10, 41, 44, 21, 102, 34, 23, 38, 76, 63, 48, 73, 66, 32, 43, 39, 8, 52, 17, 77, 74, 45, 35, 8, 123, 109, 74, 18, 155, 60, 26, 19, 33, 24, 83, 29, 89, 52, 68, 121, 119, 54, 40, 63, 50, 22, 84, 33, 101, 16, 117, 30, 33, 14, 19, 31, 46, 52, 41, 63, 49, 23, 65, 65, 117, 58, 113, 109, 43, 66, 65, 32, 29, 57, 58, 23, 43, 57, 71, 44, 17, 51, 32, 72, 79, 47, 53, 31, 79, 33, 35, 62, 26, 56, 82, 41, 30, 22, 63, 140, 20, 4, 31, 43, 24, 108, 26, 30, 67, 47, 49, 36, 25, 55, 34, 33, 36, 30, 19, 56, 58, 19, 53, 27, 85, 63, 11, 7, 191, 43, 56, 19, 55, 91, 44, 64, 57, 40, 18, 30, 89, 41, 34, 35, 36, 31, 19, 60, 75, 27, 82, 7, 51, 61, 263, 8, 84, 20, 65, 34, 109, 81, 171, 20, 39, 81, 86, 33, 63, 107, 15, 6, 19, 54, 73, 30, 99, 22, 40, 28, 32, 40, 69, 105, 74, 103, 118, 34, 27, 11, 53, 93, 46, 50, 70, 30, 73, 23, 60, 30, 45, 72, 24, 36, 72, 23, 100, 80, 66, 103, 28, 33, 33, 33, 72, 42, 29, 26, 2, 92, 55, 101, 26, 22, 25, 19, 59, 53, 13, 35, 47, 31, 78, 54, 156, 55, 44, 27, 70, 48, 96, 114, 78, 40, 49, 54, 16, 1, 35, 1, 1, 19, 14, 53, 9, 9, 39, 47, 39, 76, 34, 40, 75, 53, 24, 60, 17, 128, 65, 105, 4, 45, 28, 53, 50, 99, 18, 49, 40, 76, 58, 86, 17, 57, 13, 14, 5, 64, 27, 51, 52, 25, 33, 41, 58, 37, 84, 159, 41, 51, 55, 16, 189, 32, 46, 172, 24, 43, 6, 75, 66, 21, 39, 37, 65, 53, 162, 14, 40, 28, 39, 46, 139, 27, 42, 58, 41, 18, 33, 55, 139, 78, 3, 36, 37, 43, 51, 102, 74, 98, 28, 65, 54, 75, 52, 35, 19, 48, 29, 51, 86, 49, 32, 113, 65, 34, 13, 29, 30, 52, 35, 31, 103, 42, 10, 81, 69, 136, 139, 44, 294, 30, 58, 27, 9, 19, 16, 57, 21, 48, 42, 72, 49, 39, 58, 18, 28, 26, 38, 82, 44, 45, 44, 50, 30, 28, 41, 45, 23, 164, 43, 79, 55, 62, 18, 8, 27, 36, 20, 63, 9, 35, 82, 12, 19, 19, 237, 60, 25, 45, 98, 41, 20, 47, 33, 82, 30, 74, 17, 33, 46, 18, 31, 57, 11, 70, 50, 44, 72, 178, 15, 131, 60, 51, 26, 38, 112, 41, 28, 18, 28, 18, 41, 65, 173, 85, 48, 59, 19, 24, 21, 26, 13, 120, 114, 36, 57, 16, 73, 115, 43, 6, 68, 23, 74, 39, 35, 59, 81, 23, 100, 27, 93, 13, 109, 131, 28, 56, 25, 17, 56, 60, 48, 67, 16, 7, 79, 47, 15, 49, 51, 216, 25, 53, 31, 48, 128, 243, 49, 38, 21, 6, 13, 46, 117, 58, 82, 6, 172, 12, 28, 12, 240, 66, 84, 10, 21, 56, 76, 51, 13, 98, 57, 289, 110, 87, 20, 80, 37, 102, 63, 15, 7, 8, 10, 10, 18, 14, 9, 53, 45, 14, 39, 13, 34, 32, 25, 113, 17, 43, 85, 46, 51, 81, 46, 25, 80, 25, 113, 30, 59, 53, 40, 74, 72, 20, 51, 42, 43, 62, 63, 50, 53, 105, 45, 21, 191, 131, 17, 44, 31, 24, 102, 13, 34, 90, 36, 37, 10, 40, 58, 107, 24, 28, 53, 96, 64, 10, 51, 60, 60, 44, 197, 96, 78, 24, 12, 121, 37, 1, 22, 49, 16, 9, 58, 23, 28, 8, 82, 17, 80, 37, 39, 10, 52, 75, 26, 94, 32, 69, 65, 85, 74, 23, 70, 13, 102, 15, 98, 122, 56, 39, 18, 56, 36, 65, 60, 39, 59, 43, 62, 24, 10, 27, 13, 60, 13, 25, 42, 95, 30, 116, 41, 61, 66, 118, 80, 132, 59, 29, 42, 59, 97, 12, 9, 33, 101, 51, 29, 10, 53, 56, 46, 49, 69, 43, 21, 178, 23, 67, 43, 35, 83, 81, 21, 29, 31, 32, 91, 33, 164, 25, 25, 18, 37, 38, 104, 90, 86, 62, 49, 56, 57, 75, 26, 59, 95, 67, 34, 43, 80, 50, 42, 51, 84, 42, 92, 120, 51, 11, 98, 236, 72, 65, 89, 122, 47, 89, 36, 18, 64, 20, 74, 46, 43, 40, 13, 76, 72, 31, 16, 59, 32, 36, 50, 15, 21, 30, 103, 168, 43, 132, 79, 84, 14, 174, 12, 24, 35, 27, 92, 19, 75, 47, 96, 47, 57, 65, 22, 78, 17, 80, 135, 99, 37, 139, 62, 49, 50, 67, 37, 100, 57, 83, 25, 173, 27, 36, 61, 19, 79, 41, 63, 63, 54, 67, 79, 71, 38, 58, 11, 36, 35, 93, 194, 109, 39, 72, 29, 91, 130, 87, 32, 109, 50, 137, 12, 18, 32, 63, 68, 66, 85, 30, 55, 66, 82, 82, 59, 70, 56, 147, 15, 24, 39, 19, 31, 22, 100, 132, 45, 30, 38, 38, 61, 49, 27, 45, 34, 32, 79, 67, 34, 27, 30, 22, 29, 43, 81, 42, 75, 27, 133, 54, 48, 33, 109, 20, 73, 5, 30, 70, 106, 25, 22, 61, 59, 31, 176, 218, 43, 51, 133, 37, 44, 30, 39, 19, 27, 60, 36, 8, 30, 108, 27, 21, 100, 19, 34, 49, 77, 8, 46, 146, 126, 102, 55, 58, 19, 43, 30, 120, 47, 15, 47, 68, 21, 31, 35, 130, 47, 29, 10, 15, 1, 15, 2, 22, 33, 68, 19, 52, 113, 12, 80, 31, 20, 53, 25, 21, 80, 14, 38, 30, 90, 80, 140, 76, 30, 56, 62, 25, 52, 63, 112, 97, 49, 66, 7, 99, 25, 27, 68, 115, 55, 38, 11, 27, 7, 46, 47, 111, 67, 43, 19, 33, 27, 52, 13, 11, 26, 50, 33, 57, 38, 21, 99, 1, 22, 107, 90, 1, 78, 26, 40, 48, 46, 52, 67, 166, 51, 52, 33, 69, 13, 129, 23, 38, 101, 38, 75, 49, 21, 34, 37, 61, 55, 18, 29, 49, 135, 10, 108, 14, 19, 87, 25, 121, 37, 10, 24, 75, 96, 62, 67, 79, 16, 64, 90, 30, 104, 49, 58, 55, 56, 82, 21, 117, 91, 63, 74, 245, 34, 79, 99, 46, 111, 28, 53, 55, 53, 33, 102, 26, 120, 43, 33, 31, 73, 14, 13, 10, 78, 121, 28, 81, 85, 56, 66, 93, 40, 79, 219, 29, 27, 12, 39, 24, 25, 58, 52, 34, 29, 69, 41, 24, 59, 54, 30, 63, 54, 34, 43, 85, 103, 25, 34, 44, 45, 24, 67, 59, 48, 49, 52, 111, 14, 53, 62, 88, 31, 82, 148, 88, 68, 112, 47, 55, 34, 9, 32, 32, 26, 42, 10, 11, 47, 81, 56, 19, 97, 104, 50, 33, 78, 88, 66, 105, 126, 152, 64, 59, 12, 80, 104, 95, 98, 29, 54, 29, 124, 60, 35, 14, 72, 26, 207, 38, 75, 80, 50, 59, 36, 75, 61, 106, 51, 62, 81, 40, 14, 3, 71, 108, 67, 53, 66, 71, 184, 85, 58, 50, 21, 61, 84, 18, 82, 45, 89, 10, 28, 31, 50, 61, 76, 186, 28, 18, 53, 26, 28, 54, 51, 90, 72, 61, 136, 38, 81, 18, 48, 8, 226, 34, 34, 79, 45, 37, 140, 55, 208, 68, 37, 57, 202, 34, 91, 17, 54, 43, 40, 64, 42, 88, 22, 38, 144, 6, 21, 64, 74, 36, 49, 38, 38, 50, 43, 37, 32, 100, 28, 68, 53, 67, 20, 50, 66, 51, 64, 112, 51, 33, 192, 38, 82, 88, 18, 76, 82, 13, 12, 71, 11, 27, 51, 97, 53, 25, 11, 87, 82, 49, 34, 9, 79, 60, 92, 65, 48, 19, 75, 32, 92, 25, 112, 37, 49, 18, 38, 25, 31, 149, 70, 25, 46, 117, 54, 234, 111, 68, 13, 37, 36, 39, 24, 19, 37, 78, 49, 69, 46, 29, 132, 14, 119, 83, 43, 61, 32, 37, 24, 35, 23, 105, 34, 107, 62, 27, 107, 25, 24, 44, 14, 43, 70, 64, 59, 56, 71, 46, 75, 7, 41, 52, 97, 7, 60, 21, 16, 71, 42, 101, 29, 230, 96, 93, 109, 36, 31, 36, 30, 40, 66, 14, 85, 62, 133, 76, 75, 17, 39, 25, 34, 26, 10, 6, 74, 24, 69, 19, 1, 52, 51, 3, 90, 46, 43, 40, 30, 32, 136, 44, 18, 68, 12, 85, 194, 78, 38, 26, 68, 41, 28, 22, 90, 143, 110, 282, 238, 125, 20, 58, 67, 26, 25, 21, 27, 56, 31, 53, 67, 76, 49, 84, 19, 38, 63, 51, 74, 28, 174, 11, 55, 59, 78, 31, 78, 20, 37, 33, 37, 23, 54, 40, 28, 134, 29, 65, 59, 138, 30, 87, 26, 36, 86, 49, 33, 28, 54, 85, 19, 44, 41, 19, 59, 63, 110, 103, 22, 31, 183, 115, 12, 95, 135, 12, 69, 21, 62, 31, 10, 35, 99, 98, 53, 42, 66, 59, 37, 49, 31, 10, 84, 28, 51, 28, 163, 27, 82, 33, 101, 15, 43, 167, 12, 25, 68, 21, 33, 55, 21, 36, 46, 54, 50, 59, 10, 94, 57, 49, 31, 33, 52, 29, 28, 31, 41, 22, 63, 20, 40, 31, 147, 57, 31, 35, 27, 31, 84, 111, 208, 13, 36, 154, 77, 39, 39, 4, 15, 22, 55, 16, 32, 40, 120, 99, 70, 21, 9, 132, 28, 68, 41, 70, 18, 11, 28, 23, 20, 48, 15, 72, 109, 43, 49, 26, 35, 54, 36, 48, 51, 106, 44, 22, 18, 76, 64, 57, 83, 305, 69, 41, 101, 83, 110, 38, 29, 66, 68, 33, 123, 24, 62, 60, 56, 15, 5, 41, 110, 36, 73, 85, 46, 19, 61, 12, 41, 20, 19, 16, 45, 35, 27, 57, 11, 56, 1, 23, 12, 46, 61, 24, 57, 26, 119, 8, 12, 47, 58, 26, 55, 41, 15, 11, 35, 87, 8, 40, 144, 63, 35, 40, 119, 52, 138, 55, 33, 67, 52, 15, 16, 62, 21, 10, 120, 4, 15, 32, 67, 73, 59, 71, 2, 27, 171, 26, 68, 63, 51, 55, 21, 4, 64, 9, 59, 64, 52, 21, 12, 48, 48, 63, 22, 36, 47, 11, 28, 55, 59, 12, 55, 43, 123, 99, 59, 193, 34, 47, 44, 29, 82, 1, 92, 158, 20, 22, 28, 44, 18, 98, 53, 7, 10, 70, 55, 120, 20, 10, 32, 44, 44, 38, 32, 99, 38, 52, 12, 54, 34, 82, 26, 11, 18, 20, 49, 16, 21, 38, 40, 38, 59, 74, 78, 139, 22, 32, 21, 4, 1, 1, 38, 7, 75, 41, 38, 38, 58, 34, 104, 13, 33, 81, 23, 49, 6, 20, 39, 29, 85, 19, 51, 46, 121, 18, 35, 161, 62, 32, 33, 30, 83, 121, 77, 54, 30, 110, 60, 42, 43, 57, 22, 71, 28, 46, 46, 24, 25, 74, 83, 11, 99, 36, 52, 130, 31, 15, 61, 145, 22, 76, 184, 4, 43, 34, 130, 31, 50, 34, 31, 43, 25, 36, 30, 17, 63, 31, 47, 41, 42, 57, 14, 9, 14, 115, 25, 112, 60, 141, 40, 52, 11, 27, 52, 40, 16, 3, 37, 30, 52, 86, 16, 12, 26, 42, 85, 52, 52, 25, 59, 29, 119, 41, 17, 12, 69, 18, 4, 61, 39, 111, 16, 30, 25, 57, 73, 95, 70, 40, 48, 5, 44, 50, 153, 47, 57, 64, 62, 54, 148, 16, 113, 53, 45, 61, 57, 36, 59, 124, 43, 1, 40, 66, 72, 81, 61, 18, 24, 34, 22, 51, 59, 34, 83, 22, 48, 13, 12, 23, 31, 56, 23, 42, 55, 31, 108, 45, 74, 99, 54, 100, 63, 82, 66, 23, 11, 34, 28, 30, 46, 30, 9, 42, 6, 4, 39, 59, 71, 68, 6, 127, 17, 25, 57, 40, 17, 77, 96, 387, 26, 149, 57, 44, 6, 55, 30, 22, 91, 30, 146, 29, 50, 22, 142, 90, 35, 47, 32, 16, 57, 21, 1, 79, 142, 130, 78, 22, 58, 44, 57, 60, 20, 36, 59, 101, 52, 95, 51, 43, 37, 232, 36, 63, 52, 94, 88, 3, 34, 9, 42, 40, 25, 48, 40, 45, 64, 86, 66, 50, 75, 24, 57, 47, 33, 151, 76, 34, 57, 73, 71, 61, 93, 62, 27, 19, 238, 51, 40, 25, 56, 57, 137, 24, 32, 39, 38, 367, 47, 77, 17, 46, 79, 75, 42, 36, 30, 60, 43, 34, 105, 30, 43, 64, 60, 167, 50, 13, 36, 44, 38, 30, 79, 33, 67, 16, 36, 61, 22, 54, 29, 44, 59, 22, 233, 125, 59, 48, 32, 27, 14, 52, 18, 19, 39, 80, 56, 157, 39, 67, 16, 183, 9, 59, 42, 122, 14, 41, 61, 24, 63, 66, 21, 63, 47, 118, 143, 32, 35, 67, 44, 103, 4, 63, 113, 15, 292, 26, 20, 89, 51, 41, 21, 12, 60, 32, 41, 109, 48, 41, 71, 9, 33, 77, 52, 78, 21, 36, 53, 125, 88, 10, 12, 44, 37, 70, 60, 36, 23, 43, 46, 1, 40, 81, 43, 138, 78, 18, 65, 70, 53, 99, 104, 29, 114, 21, 25, 42, 66, 22, 57, 36, 1, 138, 58, 48, 13, 52, 67, 10, 36, 125, 40, 120, 91, 26, 138, 56, 56, 3, 32, 55, 16, 29, 17, 16, 47, 37, 186, 53, 181, 55, 57, 20, 18, 46, 57, 104, 107, 26, 31, 46, 23, 43, 48, 34, 27, 117, 60, 45, 46, 43, 14, 25, 95, 44, 45, 102, 36, 81, 62, 25, 35, 182, 43, 47, 60, 87, 18, 23, 52, 94, 45, 28, 61, 27, 41, 39, 28, 45, 199, 21, 56, 135, 67, 72, 115, 27, 37, 80, 13, 194, 20, 120, 63, 49, 35, 71, 27, 77, 49, 42, 10, 100, 67, 80, 76, 50, 43, 59, 55, 12, 67, 76, 54, 14, 81, 58, 28, 87, 168, 74, 10, 55, 68, 56, 28, 51, 48, 37, 162, 74, 37, 35, 47, 2, 23, 76, 91, 39, 68, 54, 20, 28, 49, 17, 112, 163, 24, 109, 40, 93, 84, 43, 30, 111, 11, 13, 25, 54, 21, 50, 28, 44, 36, 40, 3, 12, 21, 2, 34, 39, 41, 94, 42, 32, 22, 164, 54, 77, 62, 28, 37, 59, 55, 51, 86, 49, 13, 50, 41, 56, 42, 56, 33, 67, 39, 32, 20, 72, 37, 71, 18, 69, 149, 68, 74, 144, 15, 26, 147, 6, 92, 34, 64, 36, 60, 160, 16, 57, 56, 1, 30, 112, 126, 28, 105, 63, 35, 52, 32, 120, 106, 50, 86, 286, 15, 42, 104, 60, 84, 103, 60, 82, 37, 89, 42, 68, 25, 35, 150, 13, 105, 27, 50, 6, 46, 33, 36, 56, 28, 39, 122, 31, 75, 48, 39, 157, 36, 44, 63, 3, 15, 53, 54, 74, 44, 26, 30, 102, 15, 165, 45, 100, 42, 86, 161, 26, 27, 14, 36, 32, 64, 64, 35, 52, 36, 10, 35, 14, 26, 25, 53, 50, 66, 26, 13, 90, 23, 31, 28, 56, 43, 44, 80, 36, 38, 142, 43, 32, 76, 10, 149, 43, 41, 65, 79, 39, 43, 67, 15, 33, 16, 55, 45, 35, 40, 50, 80, 94, 48, 16, 19, 30, 21, 24, 19, 20, 47, 78, 27, 20, 68, 80, 51, 164, 81, 105, 157, 83, 27, 35, 70, 42, 71, 60, 23, 73, 22, 27, 37, 26, 158, 103, 243, 67, 31, 29, 30, 123, 51, 30, 33, 65, 33, 39, 110, 69, 77, 45, 16, 70, 55, 29, 39, 54, 19, 19, 26, 46, 43, 69, 63, 149, 77, 90, 33, 59, 82, 138, 40, 15, 15, 7, 101, 10, 39, 50, 23, 92, 11, 109, 35, 30, 58, 70, 173, 54, 25, 65, 43, 58, 21, 35, 50, 68, 22, 28, 23, 88, 24, 20, 99, 20, 1, 17, 34, 39, 70, 43, 50, 52, 4, 84, 10, 10, 104, 26, 46, 17, 71, 53, 27, 50, 66, 36, 45, 19, 77, 18, 78, 62, 21, 61, 65, 19, 61, 4, 64, 47, 44, 52, 30, 30, 18, 96, 39, 51, 90, 156, 81, 21, 44, 90, 81, 45, 95, 27, 93, 26, 13, 62, 88, 55, 12, 48, 145, 69, 10, 32, 89, 42, 112, 44, 105, 69, 36, 3, 106, 18, 19, 16, 42, 111, 80, 175, 51, 49, 59, 32, 85, 80, 41, 1, 21, 49, 35, 105, 21, 5, 61, 68, 48, 2, 94, 155, 124, 25, 42, 44, 13, 90, 20, 42, 67, 39, 69, 25, 49, 48, 101, 37, 42, 55, 128, 42, 71, 55, 72, 25, 137, 39, 18, 49, 6, 37, 58, 64, 76, 49, 180, 42, 176, 197, 45, 157, 41, 53, 69, 15, 42, 47, 88, 143, 52, 23, 93, 62, 19, 15, 11, 64, 145, 42, 79, 64, 74, 53, 62, 222, 19, 61, 60, 35, 46, 64, 16, 52, 26, 78, 47, 34, 48, 96, 42, 44, 94, 60, 243, 73, 35, 14, 26, 25, 62, 66, 76, 64, 53, 29, 28, 24, 33, 14, 26, 76, 49, 135, 34, 54, 47, 29, 31, 19, 69, 12, 54, 33, 39, 59, 4, 32, 41, 43, 49, 89, 168, 8, 101, 17, 36, 1, 34, 120, 16, 120, 56, 62, 17, 261, 40, 60, 200, 91, 29, 62, 20, 96, 32, 37, 47, 1, 45, 95, 42, 17, 42, 27, 43, 123, 94, 78, 22, 37, 17, 20, 94, 43, 26, 63, 79, 39, 44, 66, 156, 32, 12, 135, 40, 47, 72, 12, 77, 34, 40, 31, 105, 12, 104, 33, 63, 74, 54, 64, 30, 28, 48, 205, 19, 59, 38, 94, 19, 19, 63, 75, 54, 112, 19, 13, 53, 102, 34, 62, 20, 31, 42, 81, 4, 29, 39, 11, 45, 16, 144, 159, 29, 25, 23, 61, 28, 25, 29, 26, 44, 18, 63, 105, 26, 20, 52, 26, 33, 121, 55, 85, 48, 13, 43, 69, 24, 62, 81, 40, 63, 13, 157, 122, 66, 53, 30, 17, 66, 80, 25, 57, 56, 63, 30, 47, 42, 67, 36, 70, 62, 44, 17, 13, 13, 173, 39, 26, 64, 52, 25, 14, 20, 47, 26, 98, 116, 36, 78, 32, 24, 36, 131, 50, 29, 93, 6, 1, 48, 107, 77, 38, 15, 53, 255, 26, 58, 41, 192, 27, 68, 51, 124, 10, 33, 77, 25, 111, 55, 115, 29, 16, 82, 52, 53, 36, 72, 24, 95, 12, 16, 16, 50, 61, 59, 85, 117, 153, 116, 22, 31, 69, 39, 27, 31, 1, 56, 47, 16, 61, 99, 11, 46, 40, 173, 59, 64, 127, 56, 93, 51, 97, 53, 192, 48, 59, 64, 71, 8, 80, 33, 22, 66, 63, 21, 78, 9, 56, 44, 38, 62, 135, 64, 39, 77, 68, 43, 73, 23, 21, 45, 110, 110, 60, 29, 178, 19, 18, 87, 161, 88, 34, 38, 102, 19, 25, 106, 45, 22, 37, 29, 94, 20, 67, 36, 52, 70, 80, 6, 53, 70, 39, 122, 41, 108, 47, 14, 36, 56, 78, 37, 57, 139, 22, 61, 23, 20, 38, 48, 37, 26, 20, 92, 35, 39, 34, 31, 70, 76, 41, 78, 56, 58, 61, 33, 49, 179, 71, 72, 72, 65, 56, 50, 61, 27, 18, 24, 151, 52, 46, 135, 39, 45, 38, 13, 36, 98, 59, 16, 140, 34, 41, 92, 16, 72, 25, 21, 54, 146, 88, 9, 52, 43, 175, 51, 33, 21, 91, 199, 162, 4, 27, 41, 34, 17, 43, 66, 52, 29, 58, 48, 65, 44, 33, 92, 53, 87, 70, 62, 62, 18, 55, 71, 54, 41, 87, 67, 35, 68, 57, 56, 180, 63, 28, 99, 73, 29, 29, 58, 19, 56, 35, 22, 98, 82, 41, 29, 57, 60, 19, 54, 20, 26, 14, 41, 111, 35, 24, 80, 18, 40, 71, 36, 77, 50, 40, 71, 55, 4, 82, 177, 42, 78, 61, 31, 48, 43, 68, 9, 36, 70, 76, 44, 62, 34, 59, 40, 224, 53, 36, 102, 55, 68, 12, 21, 15, 43, 49, 18, 49, 60, 47, 30, 31, 4, 268, 16, 48, 11, 57, 94, 83, 23, 54, 33, 163, 42, 57, 16, 27, 112, 23, 76, 13, 5, 42, 28, 13, 42, 98, 71, 69, 111, 77, 45, 16, 65, 39, 77, 145, 42, 98, 50, 44, 87, 28, 58, 25, 43, 31, 24, 32, 74, 1, 43, 40, 139, 24, 25, 59, 69, 105, 82, 84, 39, 147, 46, 16, 49, 6, 48, 37, 264, 33, 101, 11, 31, 52, 54, 60, 54, 10, 123, 89, 9, 46, 52, 69, 31, 11, 57, 4, 22, 42, 75, 38, 55, 9, 44, 32, 24, 47, 11, 14, 63, 8, 70, 47, 45, 50, 69, 27, 50, 46, 26, 50, 48, 33, 2, 76, 47, 29, 32, 75, 49, 81, 76, 43, 58, 35, 27, 7, 64, 53, 98, 74, 76, 34, 41, 24, 67, 69, 94, 35, 139, 35, 120, 23, 42, 13, 32, 40, 25, 15, 61, 42, 59, 76, 40, 67, 117, 27, 25, 94, 95, 25, 8, 40, 107, 36, 29, 79, 157, 16, 39, 21, 9, 53, 48, 86, 26, 40, 28, 47, 124, 74, 101, 42, 84, 38, 88, 31, 48, 34, 41, 28, 28, 88, 60, 45, 36, 74, 39, 27, 37, 74, 88, 39, 48, 31, 149, 57, 8, 67, 61, 67, 44, 57, 22, 96, 22, 14, 63, 43, 68, 87, 51, 19, 18, 30, 104, 46, 122, 61, 136, 153, 93, 132, 97, 30, 34, 38, 28, 37, 20, 21, 61, 39, 16, 12, 40, 100, 53, 49, 45, 51, 47, 63, 57, 52, 77, 71, 66, 22, 76, 26, 117, 85, 80, 65, 77, 30, 71, 38, 170, 55, 27, 78, 171, 71, 21, 33, 56, 3, 35, 59, 18, 24, 43, 81, 26, 110, 61, 57, 49, 70, 23, 38, 18, 107, 58, 76, 37, 61, 70, 17, 58, 43, 18, 72, 64, 22, 21, 23, 1, 36, 45, 27, 150, 124, 146, 25, 121, 66, 33, 98, 51, 30, 38, 80, 57, 14, 20, 69, 24, 35, 215, 44, 90, 65, 47, 75, 33, 118, 52, 44, 151, 109, 11, 47, 31, 74, 31, 53, 17, 68, 75, 41, 24, 44, 91, 88, 47, 66, 46, 20, 54, 173, 86, 58, 54, 20, 17, 41, 31, 51, 9, 74, 39, 20, 55, 166, 47, 7, 45, 14, 19, 40, 133, 23, 55, 33, 39, 36, 33, 34, 26, 76, 39, 40, 48, 27, 37, 34, 2, 194, 37, 15, 32, 59, 32, 56, 83, 57, 27, 112, 37, 70, 40, 61, 33, 81, 18, 26, 153, 56, 34, 27, 18, 82, 82, 68, 50, 15, 37, 30, 118, 1, 133, 48, 47, 30, 37, 39, 61, 21, 60, 32, 36, 47, 38, 42, 50, 15, 59, 28, 30, 91, 11, 32, 14, 24, 4, 20, 105, 138, 154, 21, 1, 40, 41, 96, 96, 40, 6, 21, 27, 26, 59, 116, 31, 29, 32, 69, 38, 69, 4, 8, 109, 52, 28, 47, 33, 49, 34, 44, 3, 23, 42, 35, 33, 101, 3, 46, 36, 35, 30, 77, 55, 5, 51, 24, 56, 42, 56, 50, 12, 92, 45, 20, 53, 39, 30, 58, 56, 67, 22, 69, 67, 34, 32, 30, 32, 84, 10, 4, 40, 37, 88, 34, 80, 31, 67, 41, 12, 23, 47, 12, 165, 48, 102, 64, 112, 79, 22, 36, 66, 67, 36, 16, 25, 19, 63, 40, 47, 46, 31, 82, 69, 11, 49, 48, 9, 27, 57, 100, 61, 66, 129, 81, 56, 99, 68, 10, 43, 47, 43, 23, 12, 62, 150, 1, 18, 114, 94, 31, 2, 18, 75, 78, 51, 176, 33, 80, 26, 53, 76, 59, 35, 50, 54, 50, 65, 7, 34, 58, 51, 61, 30, 110, 44, 17, 11, 85, 25, 57, 6, 59, 33, 22, 70, 11, 39, 40, 67, 61, 116, 45, 90, 11, 18, 56, 74, 128, 63, 22, 50, 24, 36, 73, 43, 38, 45, 37, 45, 14, 56, 66, 64, 9, 146, 21, 88, 38, 29, 82, 11, 72, 141, 50, 116, 50, 36, 101, 53, 18, 69, 27, 11, 44, 28, 90, 52, 47, 60, 45, 37, 72, 39, 50, 28, 46, 75, 99, 63, 69, 46, 30, 20, 111, 39, 68, 32, 69, 238, 47, 70, 193, 11, 44, 62, 100, 24, 20, 44, 95, 87, 56, 38, 31, 52, 33, 42, 14, 107, 136, 39, 14, 65, 71, 39, 37, 74, 2, 62, 144, 72, 30, 167, 56, 72, 87, 2, 40, 126, 75, 20, 22, 112, 56, 41, 22, 87, 3, 112, 62, 41, 14, 32, 9, 7, 41, 154, 61, 39, 45, 51, 48, 17, 100, 90, 49, 24, 103, 10, 59, 18, 168, 76, 111, 243, 29, 10, 38, 153, 49, 9, 118, 73, 17, 39, 65, 12, 114, 82, 21, 2, 41, 21, 38, 20, 33, 86, 21, 63, 70, 55, 33, 24, 57, 62, 30, 102, 38, 71, 88, 151, 34, 48, 55, 62, 82, 45, 56, 34, 39, 34, 147, 59, 95, 45, 90, 33, 14, 11, 32, 18, 19, 83, 51, 48, 53, 33, 32, 42, 46, 247, 44, 11, 40, 59, 47, 42, 68, 24, 75, 49, 10, 1, 33, 25, 5, 88, 91, 38, 39, 135, 64, 51, 12, 5, 31, 45, 6, 52, 43, 62, 58, 23, 26, 231, 80, 39, 44, 30, 66, 63, 56, 41, 52, 30, 58, 27, 12, 10, 17, 20, 20, 65, 56, 60, 140, 22, 47, 35, 50, 55, 52, 55, 68, 104, 24, 46, 177, 8, 36, 45, 20, 13, 13, 75, 104, 51, 48, 81, 84, 25, 44, 29, 95, 4, 79, 48, 19, 28, 72, 36, 81, 56, 23, 13, 42, 9, 37, 12, 15, 24, 14, 49, 45, 22, 23, 28, 304, 39, 41, 4, 12, 41, 99, 18, 127, 54, 24, 41, 88, 24, 48, 54, 11, 10, 12, 12, 131, 111, 16, 8, 21, 30, 56, 25, 20, 32, 51, 49, 54, 96, 61, 43, 13, 55, 44, 19, 34, 24, 97, 53, 66, 20, 34, 58, 3, 20, 24, 1, 17, 18, 93, 74, 15, 71, 72, 53, 40, 46, 66, 16, 44, 122, 28, 156, 59, 64, 200, 42, 52, 31, 42, 27, 49, 64, 22, 77, 67, 29, 72, 41, 31, 28, 60, 37, 64, 48, 124, 35, 31, 21, 54, 28, 4, 31, 40, 37, 28, 108, 38, 64, 49, 40, 87, 56, 22, 49, 55, 131, 18, 31, 85, 44, 189, 43, 72, 13, 29, 128, 46, 45, 28, 42, 47, 34, 58, 52, 37, 5, 68, 120, 33, 27, 70, 19, 144, 4, 7, 39, 30, 15, 18, 53, 11, 65, 74, 19, 100, 41, 64, 24, 53, 42, 38, 9, 50, 52, 19, 18, 70, 62, 40, 30, 48, 138, 113, 99, 52, 4, 27, 27, 30, 165, 61, 5, 26, 36, 39, 59, 7, 16, 8, 127, 1, 9, 14, 24, 98, 44, 84, 186, 106, 73, 85, 40, 54, 57, 72, 177, 53, 240, 11, 30, 56, 17, 8, 23, 50, 54, 25, 60, 56, 19, 15, 47, 42, 69, 49, 25, 18, 72, 53, 82, 36, 19, 64, 92, 148, 101, 64, 27, 108, 21, 54, 31, 47, 39, 41, 59, 87, 40, 69, 40, 27, 55, 47, 39, 36, 13, 26, 37, 40, 44, 72, 53, 89, 41, 76, 50, 29, 19, 24, 16, 131, 24, 28, 32, 123, 175, 26, 59, 67, 47, 162, 221, 45, 46, 36, 121, 21, 68, 32, 39, 40, 54, 61, 86, 93, 68, 194, 41, 264, 98, 34, 33, 40, 23, 38, 87, 30, 64, 3, 26, 132, 13, 21, 33, 63, 30, 47, 105, 85, 44, 79, 20, 42, 153, 18, 166, 41, 105, 8, 50, 188, 42, 56, 38, 45, 11, 31, 1, 12, 77, 48, 79, 73, 61, 40, 65, 46, 66, 13, 10, 6, 53, 116, 38, 40, 16, 3, 129, 19, 138, 46, 13, 57, 85, 35, 37, 62, 32, 38, 92, 40, 74, 43, 25, 103, 107, 12, 22, 32, 50, 90, 87, 10, 23, 9, 35, 55, 52, 48, 91, 46, 48, 70, 59, 32, 40, 54, 103, 65, 25, 70, 72, 37, 28, 72, 21, 52, 36, 53, 45, 53, 8, 115, 43, 76, 25, 10, 55, 63, 29, 93, 28, 78, 20, 46, 50, 108, 73, 36, 60, 75, 24, 22, 70, 72, 30, 97, 73, 21, 58, 78, 36, 19, 24, 36, 36, 47, 50, 46, 33, 106, 68, 70, 67, 15, 14, 37, 63, 49, 45, 6, 15, 23, 80, 71, 165, 30, 81, 23, 103, 93, 39, 111, 25, 101, 81, 40, 52, 44, 9, 22, 78, 168, 89, 17, 14, 39, 29, 43, 40, 38, 13, 41, 10, 41, 26, 42, 66, 12, 46, 38, 53, 38, 34, 51, 20, 23, 112, 19, 42, 15, 33, 124, 20, 41, 47, 41, 31, 46, 16, 53, 28, 46, 23, 35, 80, 30, 119, 19, 56, 85, 40, 77, 53, 25, 75, 51, 91, 35, 55, 83, 35, 224, 176, 45, 182, 84, 28, 23, 39, 44, 27, 56, 53, 99, 76, 50, 24, 50, 37, 83, 51, 57, 148, 12, 71, 45, 56, 169, 47, 59, 39, 20, 26, 94, 12, 44, 42, 113, 40, 18, 15, 22, 119, 63, 28, 56, 135, 66, 103, 20, 22, 31, 22, 7, 64, 59, 57, 50, 131, 73, 34, 19, 29, 75, 23, 82, 61, 193, 71, 67, 127, 97, 29, 58, 14, 41, 57, 26, 3, 32, 41, 81, 39, 32, 44, 51, 13, 89, 28, 27, 125, 95, 12, 51, 32, 138, 4, 39, 33, 70, 55, 63, 104, 97, 36, 64, 37, 78, 39, 65, 45, 40, 79, 67, 74, 85, 108, 66, 16, 34, 172, 10, 43, 69, 95, 42, 10, 39, 69, 79, 24, 32, 38, 43, 22, 29, 39, 130, 79, 83, 40, 46, 46, 112, 26, 63, 46, 35, 59, 132, 52, 35, 92, 41, 79, 15, 35, 23, 64, 117, 27, 23, 109, 56, 8, 37, 84, 114, 33, 37, 53, 44, 65, 18, 77, 82, 33, 16, 74, 31, 19, 58, 58, 33, 7, 79, 42, 11, 55, 16, 41, 39, 29, 1, 36, 89, 71, 69, 50, 57, 41, 85, 174, 30, 241, 156, 41, 16, 25, 60, 47, 67, 39, 63, 62, 72, 51, 80, 55, 60, 58, 87, 9, 67, 58, 57, 133, 52, 44, 68, 43, 59, 10, 32, 28, 13, 23, 23, 9, 13, 33, 40, 56, 36, 161, 15, 23, 37, 13, 74, 35, 28, 43, 33, 75, 120, 17, 58, 78, 43, 36, 115, 55, 29, 90, 13, 32, 60, 10, 38, 54, 15, 39, 93, 25, 100, 113, 153, 62, 43, 18, 30, 56, 21, 29, 136, 46, 9, 42, 82, 66, 49, 81, 18, 144, 23, 24, 56, 57, 69, 75, 4, 68, 11, 38, 12, 66, 19, 34, 35, 44, 31, 30, 31, 23, 258, 40, 40, 125, 80, 36, 33, 89, 27, 49, 51, 51, 53, 137, 22, 41, 110, 88, 51, 92, 28, 43, 27, 33, 40, 37, 18, 36, 26, 42, 125, 85, 100, 77, 87, 81, 71, 116, 21, 80, 117, 74, 39, 67, 12, 154, 37, 61, 1, 70, 45, 21, 26, 63, 89, 148, 54, 36, 101, 68, 66, 75, 85, 28, 55, 86, 14, 59, 8, 14, 65, 12, 90, 41, 40, 3, 344, 3, 42, 19, 60, 49, 18, 35, 103, 39, 101, 77, 95, 65, 68, 62, 36, 54, 162, 41, 36, 61, 34, 46, 109, 48, 17, 37, 56, 35, 46, 93, 33, 27, 22, 8, 41, 82, 30, 82, 38, 117, 40, 71, 46, 26, 96, 49, 14, 18, 29, 27, 41, 133, 99, 24, 15, 20, 25, 22, 52, 49, 44, 12, 49, 24, 39, 45, 39, 42, 43, 46, 24, 64, 58, 49, 64, 83, 110, 9, 10, 11, 7, 151, 39, 56, 61, 104, 48, 58, 129, 30, 76, 133, 139, 42, 44, 44, 113, 46, 53, 18, 28, 9, 74, 33, 88, 42, 44, 118, 82, 37, 24, 13, 16, 48, 30, 11, 35, 59, 57, 75, 14, 14, 111, 34, 47, 47, 13, 70, 27, 1, 22, 26, 47, 37, 2, 58, 49, 133, 26, 27, 50, 62, 13, 37, 74, 23, 118, 43, 65, 71, 14, 78, 43, 55, 15, 253, 42, 15, 74, 88, 42, 44, 32, 35, 6, 8, 9, 14, 8, 7, 67, 39, 21, 21, 26, 9, 8, 74, 188, 299, 76, 11, 39, 109, 13, 3, 17, 64, 66, 30, 94, 75, 49, 37, 119, 12, 11, 33, 56, 74, 53, 39, 13, 68, 28, 28, 32, 52, 27, 22, 9, 14, 115, 20, 100, 78, 61, 35, 67, 41, 27, 52, 60, 16, 115, 70, 48, 24, 15, 19, 55, 53, 76, 52, 16, 50, 41, 42, 76, 51, 25, 30, 47, 19, 32, 72, 24, 47, 49, 212, 23, 12, 240, 56, 17, 46, 53, 70, 19, 28, 81, 75, 41, 50, 17, 32, 38, 99, 1, 60, 27, 54, 48, 62, 25, 70, 13, 16, 43, 35, 41, 12, 73, 205, 38, 38, 97, 44, 36, 19, 58, 46, 79, 73, 37, 59, 1, 11, 36, 76, 129, 85, 90, 33, 53, 87, 28, 33, 20, 102, 14, 66, 11, 55, 22, 43, 80, 252, 24, 165, 24, 67, 51, 30, 92, 25, 118, 73, 12, 31, 31, 97, 17, 9, 57, 28, 76, 81, 12, 29, 25, 43, 200, 28, 69, 40, 74, 27, 2, 41, 6, 72, 76, 42, 45, 115, 74, 41, 84, 60, 25, 246, 39, 80, 60, 142, 67, 71, 50, 39, 181, 47, 27, 29, 59, 58, 75, 26, 80, 13, 24, 88, 32, 63, 38, 11, 2, 50, 25, 31, 16, 71, 47, 75, 52, 218, 8, 53, 97, 136, 180, 98, 45, 91, 80, 55, 149, 25, 112, 37, 6, 30, 53, 46, 55, 46, 1, 35, 42, 61, 84, 8, 78, 76, 51, 1, 46, 42, 56, 78, 24, 37, 46, 46, 108, 47, 42, 42, 30, 39, 73, 43, 203, 104, 130, 93, 28, 38, 65, 148, 61, 35, 94, 16, 32, 37, 49, 20, 33, 36, 71, 14, 121, 18, 20, 140, 57, 2, 52, 45, 81, 55, 204, 55, 58, 68, 52, 16, 13, 72, 31, 42, 61, 51, 46, 40, 14, 41, 11, 46, 39, 62, 23, 32, 135, 127, 118, 63, 43, 7, 49, 28, 33, 26, 37, 44, 202, 43, 11, 24, 55, 50, 52, 7, 16, 40, 114, 32, 57, 42, 31, 6, 24, 221, 84, 34, 24, 23, 16, 15, 12, 29, 76, 16, 27, 44, 29, 34, 24, 87, 43, 2, 80, 76, 56, 42, 8, 46, 82, 37, 94, 118, 52, 12, 24, 58, 27, 25, 54, 60, 43, 46, 82, 57, 49, 23, 25, 47, 58, 12, 21, 188, 45, 42, 40, 22, 56, 46, 35, 84, 30, 90, 73, 66, 6, 69, 21, 37, 1, 43, 42, 29, 13, 55, 26, 157, 61, 51, 57, 60, 37, 21, 22, 39, 92, 53, 23, 38, 19, 40, 148, 65, 11, 65, 98, 44, 61, 110, 34, 60, 43, 86, 74, 74, 9, 57, 43, 59, 36, 27, 79, 121, 32, 130, 92, 139, 39, 42, 30, 18, 67, 18, 50, 12, 25, 71, 19, 13, 58, 101, 18, 33, 112, 4, 29, 45, 38, 143, 24, 40, 55, 3, 51, 49, 40, 60, 43, 62, 10, 30, 85, 88, 49, 79, 42, 157, 82, 55, 69, 170, 43, 55, 68, 24, 45, 111, 12, 21, 57, 37, 23, 81, 27, 90, 30, 24, 132, 86, 53, 28, 47, 31, 90, 16, 69, 31, 95, 37, 44, 62, 30, 42, 30, 27, 43, 133, 55, 64, 20, 41, 44, 17, 76, 99, 60, 41, 51, 271, 180, 139, 48, 9, 63, 23, 51, 81, 65, 33, 63, 77, 125, 94, 171, 23, 138, 98, 45, 16, 18, 28, 35, 56, 151, 13, 156, 30, 74, 55, 74, 116, 100, 59, 47, 134, 25, 63, 63, 90, 44, 62, 58, 1, 17, 2, 30, 30, 49, 44, 97, 16, 54, 30, 20, 20, 69, 46, 10, 113, 32, 77, 12, 49, 57, 63, 1, 17, 60, 33, 25, 53, 68, 14, 47, 45, 23, 41, 28, 156, 75, 38, 58, 30, 39, 21, 76, 86, 114, 77, 10, 33, 24, 30, 14, 143, 83, 135, 14, 14, 45, 47, 27, 55, 46, 48, 182, 214, 93, 36, 68, 20, 20, 59, 41, 20, 38, 53, 18, 12, 204, 76, 34, 22, 204, 97, 102, 58, 21, 21, 55, 48, 27, 176, 23, 13, 40, 69, 92, 32, 88, 160, 33, 45, 80, 41, 53, 125, 61, 64, 155, 36, 17, 13, 18, 34, 18, 147, 2, 35, 45, 155, 43, 35, 95, 30, 37, 228, 48, 42, 49, 22, 57, 52, 34, 48, 6, 21, 11, 99, 38, 44, 152, 26, 24, 26, 132, 26, 20, 76, 46, 77, 55, 31, 66, 66, 72, 10, 58, 30, 51, 139, 59, 35, 187, 20, 119, 31, 79, 51, 46, 52, 17, 67, 34, 18, 21, 73, 50, 54, 29, 34, 53, 18, 207, 87, 46, 98, 21, 36, 95, 35, 42, 79, 46, 5, 56, 62, 7, 16, 61, 17, 71, 40, 80, 47, 92, 18, 29, 46, 21, 125, 72, 50, 75, 37, 1, 37, 29, 16, 39, 37, 20, 32, 153, 26, 14, 19, 29, 63, 45, 29, 23, 99, 60, 40, 79, 39, 46, 35, 21, 58, 122, 72, 18, 54, 19, 66, 28, 110, 12, 31, 13, 40, 43, 48, 96, 35, 55, 51, 25, 140, 59, 65, 41, 49, 64, 14, 53, 68, 76, 38, 71, 39, 16, 22, 28, 50, 133, 25, 71, 20, 93, 46, 47, 47, 66, 129, 51, 26, 64, 40, 46, 104, 56, 43, 30, 39, 42, 29, 40, 18, 50, 134, 34, 98, 11, 51, 25, 29, 31, 49, 141, 35, 23, 60, 5, 14, 35, 19, 68, 35, 36, 44, 41, 54, 27, 62, 27, 23, 32, 32, 175, 41, 10, 77, 18, 37, 36, 72, 37, 94, 49, 154, 29, 60, 53, 45, 40, 79, 27, 99, 22, 84, 59, 28, 48, 32, 40, 91, 39, 24, 59, 32, 104, 27, 15, 36, 32, 75, 25, 51, 52, 16, 59, 10, 85, 52, 40, 38, 11, 21, 1, 36, 19, 32, 73, 5, 62, 59, 15, 103, 25, 62, 121, 73, 29, 49, 116, 172, 67, 44, 30, 34, 28, 37, 150, 41, 30, 8, 12, 74, 11, 51, 99, 51, 7, 9, 56, 83, 91, 54, 43, 13, 52, 96, 99, 42, 18, 24, 43, 23, 36, 51, 41, 26, 29, 26, 47, 25, 28, 64, 63, 52, 85, 66, 78, 28, 42, 39, 63, 23, 55, 85, 67, 10, 43, 50, 17, 28, 30, 65, 38, 113, 10, 30, 34, 82, 40, 61, 192, 25, 76, 84, 120, 78, 60, 1, 11, 17, 77, 149, 27, 145, 190, 41, 36, 29, 119, 54, 15, 88, 56, 93, 42, 28, 26, 26, 30, 31, 49, 41, 67, 12, 39, 25, 20, 80, 18, 52, 7, 136, 14, 45, 163, 97, 53, 39, 147, 143, 62, 53, 47, 42, 1, 60, 99, 92, 102, 137, 43, 22, 54, 19, 24, 71, 21, 34, 34, 41, 166, 77, 37, 25, 44, 84, 26, 64, 109, 180, 48, 70, 76, 91, 54, 6, 48, 61, 61, 11, 52, 80, 63, 30, 41, 44, 53, 76, 53, 51, 135, 73, 9, 60, 115, 30, 98, 49, 44, 20, 24, 42, 59, 144, 70, 40, 5, 18, 58, 54, 82, 51, 18, 10, 24, 51, 138, 68, 44, 67, 68, 67, 2, 11, 84, 34, 74, 44, 96, 28, 40, 41, 14, 17, 83, 49, 33, 25, 47, 36, 33, 22, 26, 1, 57, 17, 62, 137, 35, 16, 61, 10, 7, 56, 34, 39, 47, 29, 80, 157, 28, 32, 35, 74, 220, 26, 38, 57, 35, 25, 28, 85, 54, 15, 74, 44, 39, 46, 24, 28, 36, 43, 20, 53, 132, 50, 14, 26, 101, 40, 58, 22, 32, 42, 63, 33, 65, 79, 99, 46, 106, 36, 12, 54, 31, 30, 59, 60, 39, 47, 64, 16, 110, 26, 45, 130, 67, 129, 80, 56, 29, 12, 108, 31, 89, 84, 28, 44, 54, 42, 55, 26, 28, 51, 9, 26, 34, 29, 33, 30, 33, 49, 71, 39, 23, 93, 29, 23, 28, 80, 8, 52, 36, 39, 73, 60, 23, 23, 17, 86, 39, 44, 45, 29, 26, 11, 10, 17, 95, 34, 70, 27, 20, 42, 44, 44, 36, 9, 24, 75, 25, 88, 64, 53, 54, 53, 68, 122, 31, 81, 22, 66, 43, 155, 44, 95, 33, 65, 103, 49, 28, 48, 21, 65, 73, 90, 33, 33, 23, 25, 30, 25, 22, 35, 66, 55, 73, 64, 35, 77, 39, 55, 53, 37, 56, 31, 52, 28, 30, 31, 58, 117, 14, 58, 36, 60, 43, 36, 61, 46, 61, 69, 25, 58, 36, 65, 59, 19, 48, 10, 206, 113, 25, 80, 81, 32, 102, 17, 11, 10, 77, 73, 59, 57, 101, 14, 53, 72, 18, 102, 41, 14, 52, 41, 51, 18, 35, 13, 2, 100, 84, 99, 90, 53, 57, 60, 13, 178, 110, 137, 35, 66, 72, 1, 50, 51, 75, 42, 60, 142, 20, 70, 21, 25, 40, 18, 41, 126, 17, 39, 41, 37, 63, 36, 121, 77, 28, 36, 104, 51, 22, 91, 57, 47, 40, 21, 74, 68, 47, 21, 56, 55, 21, 35, 72, 70, 193, 4, 16, 27, 15, 87, 94, 5, 105, 93, 155, 78, 22, 86, 56, 20, 28, 72, 137, 77, 63, 40, 114, 122, 60, 30, 119, 12, 46, 230, 29, 57, 14, 7, 4, 91, 28, 104, 54, 22, 67, 39, 59, 40, 1, 38, 67, 46, 52, 75, 32, 67, 31, 64, 75, 35, 35, 30, 47, 39, 167, 77, 122, 23, 53, 6, 27, 84, 47, 52, 20, 13, 65, 58, 18, 59, 35, 85, 55, 45, 52, 51, 36, 20, 239, 29, 6, 28, 14, 24, 69, 26, 30, 56, 54, 43, 33, 231, 29, 32, 10, 25, 14, 116, 62, 50, 47, 45, 71, 40, 48, 23, 194, 117, 74, 63, 19, 30, 44, 81, 163, 30, 63, 53, 80, 46, 148, 19, 22, 63, 37, 17, 42, 14, 18, 24, 97, 77, 46, 62, 90, 56, 11, 54, 39, 152, 88, 13, 25, 58, 110, 24, 68, 69, 59, 26, 91, 36, 72, 15, 65, 119, 101, 81, 55, 57, 69, 95, 45, 46, 59, 32, 56, 95, 122, 31, 51, 33, 31, 45, 49, 24, 38, 167, 14, 18, 36, 26, 240, 60, 83, 35, 34, 14, 15, 59, 30, 55, 34, 74, 100, 174, 38, 53, 97, 8, 100, 34, 8, 24, 79, 143, 25, 134, 22, 17, 57, 38, 37, 31, 20, 105, 55, 37, 48, 325, 62, 32, 65, 14, 50, 96, 26, 26, 44, 27, 23, 33, 59, 72, 127, 108, 6, 149, 36, 55, 11, 27, 17, 38, 34, 70, 102, 28, 14, 38, 48, 63, 62, 43, 76, 22, 29, 33, 55, 44, 51, 38, 10, 63, 93, 128, 29, 51, 50, 39, 54, 53, 52, 31, 79, 54, 81, 121, 82, 91, 36, 32, 61, 24, 31, 20, 38, 81, 19, 22, 125, 51, 21, 61, 108, 89, 32, 164, 44, 1, 8, 33, 60, 53, 62, 39, 48, 171, 74, 39, 83, 31, 9, 68, 82, 39, 19, 69, 18, 116, 16, 3, 28, 28, 50, 115, 99, 13, 92, 56, 37, 61, 8, 108, 37, 130, 61, 51, 52, 66, 34, 33, 97, 121, 66, 62, 73, 8, 11, 3, 45, 27, 71, 25, 40, 15, 36, 34, 27, 34, 38, 116, 27, 94, 105, 47, 86, 16, 12, 43, 30, 64, 41, 19, 51, 42, 24, 40, 151, 41, 9, 32, 43, 45, 86, 81, 56, 169, 57, 19, 66, 272, 30, 34, 17, 77, 13, 5, 53, 19, 114, 11, 21, 64, 112, 19, 7, 63, 14, 34, 12, 21, 46, 27, 46, 25, 53, 53, 53, 89, 138, 129, 70, 17, 48, 11, 120, 73, 3, 29, 40, 27, 103, 121, 53, 47, 6, 59, 42, 268, 62, 29, 24, 96, 41, 43, 14, 51, 25, 31, 64, 122, 61, 142, 12, 40, 33, 59, 46, 40, 143, 52, 34, 154, 83, 64, 81, 60, 37, 51, 143, 35, 37, 15, 8, 25, 31, 46, 29, 29, 20, 7, 57, 92, 23, 50, 45, 66, 96, 25, 16, 28, 1, 32, 44, 31, 2, 92, 89, 69, 96, 242, 17, 60, 122, 36, 34, 26, 30, 60, 14, 58, 4, 69, 47, 50, 38, 108, 98, 92, 42, 25, 16, 94, 65, 48, 68, 27, 1, 72, 136, 69, 56, 14, 40, 66, 85, 65, 109, 25, 41, 42, 39, 12, 60, 45, 50, 34, 64, 79, 94, 63, 1, 54, 1, 34, 64, 54, 27, 23, 52, 10, 43, 123, 74, 81, 24, 64, 16, 77, 85, 83, 56, 38, 9, 102, 58, 90, 53, 62, 96, 38, 45, 59, 53, 34, 79, 2, 27, 74, 50, 58, 130, 33, 224, 53, 108, 53, 16, 77, 143, 56, 30, 37, 57, 50, 41, 93, 49, 63, 34, 14, 56, 140, 51, 360, 117, 69, 80, 2, 12, 15, 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uDG1xBZ46Cl",
        "outputId": "32b89388-b348-4ca2-f682-172b62411b5d"
      },
      "source": [
        "if 0 in sections:\n",
        "  print('yes')\n",
        "else:\n",
        "  print('No zeros')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No zeros\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "AmU7F7BZ5NzG",
        "outputId": "678117f1-062d-46cb-c654-3e65339fc091"
      },
      "source": [
        "clean_df.body_text[3][1]['text']"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"In an ideal world, we, fellow academics, irrespective of the difference in rank, gender, or seniority, would support each other's research (and hence publications) by responding to requests for helppresumably every response, every bit of contribution to their data would count. (It should be noted that such appeals to fellow academics for help are typically made by academics in education, including language education, where they may have research interests that necessitate the participation of fellow academics across disciplines.) The literature, especially that of health research, has discussed the pros and cons of undertaking qualitative interviews with one's peers in the same profession [1, 2] . However, little reflection is found in the literature on the implications of working with fellow academics who are not necessarily in one's own field of profession or discipline. At the same time, while research methodology books give ample advice on accessing participants beyond academia, little discussion can be found on working with fellow academics as research participants. The present paper will address this gap in the literature. The observations shared here represent personal views, which have not been tested by systematic research. However, a discussion of the phenomenon is worthwhile as accessing target research participants is an important issue for many academics in social sciences.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KBkr5Z_5pSr",
        "outputId": "d1900965-aa67-4218-91ce-fee62a8146ef"
      },
      "source": [
        "number_words_of_article = 0\n",
        "for txt in clean_df.body_text[3]:\n",
        "  number_words_of_article+=len(txt['text'].split())\n",
        "number_words_of_article"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6214"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZcsD1Md6djA"
      },
      "source": [
        "# adding all texts into one single entry for each row\n",
        "new_df = clean_df.copy()\n",
        "article_text_column = []\n",
        "for i in range(len(clean_df.body_text)):\n",
        "  article_text = []\n",
        "  for entry in clean_df.iloc[i]['body_text']:\n",
        "    article_text.append(entry['text'])\n",
        "  article_text = delete_regex('. '.join(article_text))\n",
        "  article_text_column.append(article_text)\n",
        "\n",
        "new_df['pure_text'] = article_text_column"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4FkVW5l0ICB"
      },
      "source": [
        "### NER using Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtZuN5Vi68J3"
      },
      "source": [
        "import re\n",
        "\n",
        "def delete_regex(text):\n",
        "  #remove html\n",
        "  output = re.sub('<[^>]*>', '', text)\n",
        "  #remove numbers, all maths operations\n",
        "  output = re.sub('((?:[-\\d)(+/*]+)?(?:(?:cos|sin|tan)[(](?:\\d+?|Pi)[)])?(?:[\\-\\d\\)\\(\\+/*]+)?)', '', output)\n",
        "  output = re.sub('#', '', output)\n",
        "  output = re.sub('/', '', output)\n",
        "  #remove abbreivations multiple consecutive capital letters\n",
        "  output = re.sub('\\b[A-Z]{2,}\\b', '', output)\n",
        "  return output"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ-vHcFOJ6HE"
      },
      "source": [
        "#!pip install transformers \n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import time\n",
        "\n",
        "# Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"fran-martinez/scibert_scivocab_cased_ner_jnlpba\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"fran-martinez/scibert_scivocab_cased_ner_jnlpba\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "document_to_recognise = new_df.iloc[1]['pure_text']\n",
        "\n",
        "document_to_recognise = delete_regex(document_to_recognise)\n",
        "\n",
        "token_list = []\n",
        "entities_dict = {}\n",
        "\n",
        "for i in range(1000, len(document_to_recognise), 1000):\n",
        "  # take a chunk\n",
        "  test_sequence = new_df.iloc[1]['pure_text'][i-1000:i]\n",
        "\n",
        "  # pass chunk to BERT\n",
        "  input_ids = torch.tensor(tokenizer.encode(test_sequence)).unsqueeze(0)\n",
        "\n",
        "  # Predict\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "  # From the output let's take the first element of the tuple.\n",
        "  # Then, let's get rid of [CLS] and [SEP] tokens (first and last)\n",
        "  predictions = outputs[0].argmax(axis=-1)[0][1:-1]\n",
        "\n",
        "  # Map label class indexes to string labels.\n",
        "  for token, pred in zip(tokenizer.tokenize(test_sequence), predictions):\n",
        "    if model.config.id2label[pred.numpy().item()] == 'O':\n",
        "      pass\n",
        "    else:\n",
        "      if model.config.id2label[pred.numpy().item()] in entities_dict.keys():\n",
        "        entities_dict[model.config.id2label[pred.numpy().item()]].append(token)\n",
        "      else:\n",
        "        entities_dict[model.config.id2label[pred.numpy().item()]] = [token]\n",
        "\n",
        "stop = time.time()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I3N5Ta973xO",
        "outputId": "4280e9ad-436e-4365-c5f0-47ba661464fe"
      },
      "source": [
        "print('Time taken to NER for one article is: ', stop-start)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to NER for one article is:  25.799999952316284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTMCw0tZ76Eb",
        "outputId": "6545fa2d-bb06-4d3f-c084-b61e08554706"
      },
      "source": [
        "entities_dict"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-DNA': ['mv', 'mv', 'mv'], 'B-protein': ['ff', 'ff', 'ff', 'ff']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}